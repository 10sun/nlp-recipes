{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nlp_path = os.path.abspath('../../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "\n",
    "from utils_nlp.dataset.squad import load_pandas_df\n",
    "from utils_nlp.models.bert.common import Language, Tokenizer\n",
    "from utils_nlp.models.bert.question_answering import BERTQAExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUAD_VERSION = \"v1.1\" \n",
    "CACHE_DIR = \"./temp\"\n",
    "\n",
    "LANGUAGE = Language.ENGLISH\n",
    "DO_LOWER_CASE = True\n",
    "\n",
    "DOC_TEXT_COL = \"doc_text\"\n",
    "QUESTION_TEXT_COL = \"question_text\"\n",
    "ANSWER_START_COL = \"answer_start\"\n",
    "ANSWER_TEXT_COL = \"answer_text\"\n",
    "QA_ID_COL = \"qa_id\"\n",
    "IS_IMPOSSIBLE_COL = \"is_impossible\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_pandas_df(local_cache_path=\".\", squad_version=\"v1.1\", file_split=\"train\")\n",
    "dev_df = load_pandas_df(local_cache_path=\".\", squad_version=\"v1.1\", file_split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_text</th>\n",
       "      <th>question_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>qa_id</th>\n",
       "      <th>is_impossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>92</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As at most other universities, Notre Dame's st...</td>\n",
       "      <td>In what year did the student paper Common Sens...</td>\n",
       "      <td>908</td>\n",
       "      <td>1987</td>\n",
       "      <td>5733bf84d058e614000b61c1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The university is the major seat of the Congre...</td>\n",
       "      <td>Which prize did Frederick Buechner create?</td>\n",
       "      <td>675</td>\n",
       "      <td>Buechner Prize for Preaching</td>\n",
       "      <td>5733bed24776f4190066118c</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The College of Engineering was established in ...</td>\n",
       "      <td>The College of Science began to offer civil en...</td>\n",
       "      <td>155</td>\n",
       "      <td>the 1870s</td>\n",
       "      <td>5733a6424776f41900660f52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All of Notre Dame's undergraduate students are...</td>\n",
       "      <td>Which organization declared the First Year of ...</td>\n",
       "      <td>647</td>\n",
       "      <td>U.S. News &amp; World Report</td>\n",
       "      <td>5733a70c4776f41900660f65</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_text  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  As at most other universities, Notre Dame's st...   \n",
       "2  The university is the major seat of the Congre...   \n",
       "3  The College of Engineering was established in ...   \n",
       "4  All of Notre Dame's undergraduate students are...   \n",
       "\n",
       "                                       question_text  answer_start  \\\n",
       "0  What sits on top of the Main Building at Notre...            92   \n",
       "1  In what year did the student paper Common Sens...           908   \n",
       "2         Which prize did Frederick Buechner create?           675   \n",
       "3  The College of Science began to offer civil en...           155   \n",
       "4  Which organization declared the First Year of ...           647   \n",
       "\n",
       "                          answer_text                     qa_id  is_impossible  \n",
       "0  a golden statue of the Virgin Mary  5733be284776f4190066117e          False  \n",
       "1                                1987  5733bf84d058e614000b61c1          False  \n",
       "2        Buechner Prize for Preaching  5733bed24776f4190066118c          False  \n",
       "3                           the 1870s  5733a6424776f41900660f52          False  \n",
       "4            U.S. News & World Report  5733a70c4776f41900660f65          False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_text</th>\n",
       "      <th>question_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>qa_id</th>\n",
       "      <th>is_impossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>What 2015 NFL team one the AFC playoff?</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>56d9895ddc89441400fdb510</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Panthers finished the regular season with ...</td>\n",
       "      <td>What year did the Carolina Panthers form?</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>56d98a59dc89441400fdb52e</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Broncos took an early lead in Super Bowl 5...</td>\n",
       "      <td>How many tackles did Von Miller accomlish by h...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>56d98b33dc89441400fdb53e</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBS broadcast Super Bowl 50 in the U.S., and c...</td>\n",
       "      <td>What performer lead the Super Bowl XLVIII half...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>56d98c53dc89441400fdb548</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In early 2012, NFL Commissioner Roger Goodell ...</td>\n",
       "      <td>What year did Roger Goodell announce that Supe...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>56d98d0adc89441400fdb54f</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_text  \\\n",
       "0  Super Bowl 50 was an American football game to...   \n",
       "1  The Panthers finished the regular season with ...   \n",
       "2  The Broncos took an early lead in Super Bowl 5...   \n",
       "3  CBS broadcast Super Bowl 50 in the U.S., and c...   \n",
       "4  In early 2012, NFL Commissioner Roger Goodell ...   \n",
       "\n",
       "                                       question_text answer_start answer_text  \\\n",
       "0            What 2015 NFL team one the AFC playoff?         None        None   \n",
       "1          What year did the Carolina Panthers form?         None        None   \n",
       "2  How many tackles did Von Miller accomlish by h...         None        None   \n",
       "3  What performer lead the Super Bowl XLVIII half...         None        None   \n",
       "4  What year did Roger Goodell announce that Supe...         None        None   \n",
       "\n",
       "                      qa_id  is_impossible  \n",
       "0  56d9895ddc89441400fdb510          False  \n",
       "1  56d98a59dc89441400fdb52e          False  \n",
       "2  56d98b33dc89441400fdb53e          False  \n",
       "3  56d98c53dc89441400fdb548          False  \n",
       "4  56d98d0adc89441400fdb54f          False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(language=LANGUAGE, to_lower=DO_LOWER_CASE,cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, qa_examples = tokenizer.tokenize_qa(\n",
    "    doc_text=train_df[DOC_TEXT_COL], \n",
    "    question_text=train_df[QUESTION_TEXT_COL], \n",
    "    answer_start=train_df[ANSWER_START_COL], \n",
    "    answer_text=train_df[ANSWER_TEXT_COL],\n",
    "    qa_id=train_df[QA_ID_COL],\n",
    "    is_impossible=train_df[IS_IMPOSSIBLE_COL],\n",
    "    is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_id\n",
      "1000000000\n",
      "\n",
      "example_index\n",
      "0\n",
      "\n",
      "tokens\n",
      "['[CLS]', 'what', 'sits', 'on', 'top', 'of', 'the', 'main', 'building', 'at', 'notre', 'dame', '?', '[SEP]', 'architectural', '##ly', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'up', '##rai', '##sed', 'with', 'the', 'legend', '\"', 've', '##ni', '##te', 'ad', 'me', 'om', '##nes', '\"', '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'gr', '##otto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'gr', '##otto', 'at', 'lou', '##rdes', ',', 'france', 'where', 'the', 'virgin', 'mary', 'reputed', '##ly', 'appeared', 'to', 'saint', 'bern', '##ade', '##tte', 'so', '##ub', '##iro', '##us', 'in', '1858', '.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mary', '.', '[SEP]']\n",
      "\n",
      "token_to_orig_map\n",
      "{14: 0, 15: 0, 16: 0, 17: 1, 18: 2, 19: 3, 20: 4, 21: 5, 22: 6, 23: 6, 24: 7, 25: 8, 26: 9, 27: 10, 28: 10, 29: 10, 30: 11, 31: 12, 32: 13, 33: 14, 34: 15, 35: 16, 36: 17, 37: 18, 38: 19, 39: 20, 40: 20, 41: 21, 42: 22, 43: 23, 44: 24, 45: 25, 46: 26, 47: 27, 48: 28, 49: 29, 50: 30, 51: 30, 52: 31, 53: 32, 54: 33, 55: 34, 56: 35, 57: 36, 58: 37, 59: 38, 60: 39, 61: 39, 62: 39, 63: 40, 64: 41, 65: 42, 66: 43, 67: 43, 68: 43, 69: 43, 70: 44, 71: 45, 72: 46, 73: 46, 74: 46, 75: 46, 76: 47, 77: 48, 78: 49, 79: 50, 80: 51, 81: 52, 82: 53, 83: 54, 84: 55, 85: 56, 86: 57, 87: 58, 88: 58, 89: 59, 90: 60, 91: 61, 92: 62, 93: 63, 94: 64, 95: 65, 96: 65, 97: 65, 98: 66, 99: 67, 100: 68, 101: 69, 102: 70, 103: 71, 104: 72, 105: 72, 106: 73, 107: 74, 108: 75, 109: 76, 110: 77, 111: 78, 112: 79, 113: 79, 114: 80, 115: 81, 116: 81, 117: 81, 118: 82, 119: 83, 120: 84, 121: 85, 122: 86, 123: 87, 124: 87, 125: 88, 126: 89, 127: 90, 128: 91, 129: 91, 130: 91, 131: 92, 132: 92, 133: 92, 134: 92, 135: 93, 136: 94, 137: 94, 138: 95, 139: 96, 140: 97, 141: 98, 142: 99, 143: 100, 144: 101, 145: 102, 146: 102, 147: 103, 148: 104, 149: 105, 150: 106, 151: 107, 152: 108, 153: 109, 154: 110, 155: 111, 156: 112, 157: 113, 158: 114, 159: 115, 160: 115, 161: 115, 162: 116, 163: 117, 164: 118, 165: 118, 166: 119, 167: 120, 168: 121, 169: 122, 170: 123, 171: 123}\n",
      "\n",
      "token_is_max_context\n",
      "{14: True, 15: True, 16: True, 17: True, 18: True, 19: True, 20: True, 21: True, 22: True, 23: True, 24: True, 25: True, 26: True, 27: True, 28: True, 29: True, 30: True, 31: True, 32: True, 33: True, 34: True, 35: True, 36: True, 37: True, 38: True, 39: True, 40: True, 41: True, 42: True, 43: True, 44: True, 45: True, 46: True, 47: True, 48: True, 49: True, 50: True, 51: True, 52: True, 53: True, 54: True, 55: True, 56: True, 57: True, 58: True, 59: True, 60: True, 61: True, 62: True, 63: True, 64: True, 65: True, 66: True, 67: True, 68: True, 69: True, 70: True, 71: True, 72: True, 73: True, 74: True, 75: True, 76: True, 77: True, 78: True, 79: True, 80: True, 81: True, 82: True, 83: True, 84: True, 85: True, 86: True, 87: True, 88: True, 89: True, 90: True, 91: True, 92: True, 93: True, 94: True, 95: True, 96: True, 97: True, 98: True, 99: True, 100: True, 101: True, 102: True, 103: True, 104: True, 105: True, 106: True, 107: True, 108: True, 109: True, 110: True, 111: True, 112: True, 113: True, 114: True, 115: True, 116: True, 117: True, 118: True, 119: True, 120: True, 121: True, 122: True, 123: True, 124: True, 125: True, 126: True, 127: True, 128: True, 129: True, 130: True, 131: True, 132: True, 133: True, 134: True, 135: True, 136: True, 137: True, 138: True, 139: True, 140: True, 141: True, 142: True, 143: True, 144: True, 145: True, 146: True, 147: True, 148: True, 149: True, 150: True, 151: True, 152: True, 153: True, 154: True, 155: True, 156: True, 157: True, 158: True, 159: True, 160: True, 161: True, 162: True, 163: True, 164: True, 165: True, 166: True, 167: True, 168: True, 169: True, 170: True, 171: True}\n",
      "\n",
      "input_ids\n",
      "[101, 2054, 7719, 2006, 2327, 1997, 1996, 2364, 2311, 2012, 10289, 8214, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 1997, 2984, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "input_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "segment_ids\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "start_position\n",
      "33\n",
      "\n",
      "end_position\n",
      "39\n",
      "\n",
      "paragraph_len\n",
      "158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_feature = train_features[0]\n",
    "for f in type(sample_feature)._fields:\n",
    "    print(f)\n",
    "    print(getattr(sample_feature, f))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "# This script reuses some code from\n",
    "# https://github.com/huggingface/pytorch-transformers/blob/067923d3267325f525f4e46f357360c191ba562e/examples/run_squad.py\n",
    "# https://github.com/huggingface/pytorch-transformers/blob/067923d3267325f525f4e46f357360c191ba562e/examples/utils_squad.py\n",
    "# https://github.com/huggingface/pytorch-transformers/blob/067923d3267325f525f4e46f357360c191ba562e/examples/utils_squad_evaluate.py\n",
    "\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    "    TensorDataset,\n",
    ")\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "from pytorch_transformers.modeling_bert import BertConfig, BertForQuestionAnswering\n",
    "\n",
    "from utils_nlp.models.bert.common import Language\n",
    "from utils_nlp.common.pytorch_utils import get_device, move_to_device\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BERTQAExtractor:\n",
    "    def __init__(self, language=Language.ENGLISH, cache_dir=\".\", fine_tuned_model=None):\n",
    "                \n",
    "        self.language = language\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "\n",
    "        if fine_tuned_model:\n",
    "            self.model = BertForQuestionAnswering.from_pretrained(fine_tuned_model)\n",
    "        else:\n",
    "            config = BertConfig.from_pretrained(language.value)\n",
    "            self.model = BertForQuestionAnswering.from_pretrained(\n",
    "                language.value, config=config\n",
    "            )\n",
    "\n",
    "    def fit(\n",
    "        self, \n",
    "        features,\n",
    "        num_gpus=None,\n",
    "        num_epochs=1,\n",
    "        batch_size=32,\n",
    "        lr=2e-5,\n",
    "        warmup_proportion=None,\n",
    "        max_grad_norm=1.0,\n",
    "        model_output_dir=None\n",
    "        ):\n",
    "        device = get_device(\"cpu\" if num_gpus == 0 or not torch.cuda.is_available() else \"gpu\")\n",
    "        self.model = move_to_device(self.model, device, num_gpus)\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "\n",
    "        all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n",
    "        all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n",
    "        train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                                      all_start_positions, all_end_positions)\n",
    "\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "        t_total = len(train_dataloader) * num_epochs\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=1e-8)\n",
    "\n",
    "        if warmup_proportion:\n",
    "            warmup_steps = t_total * warmup_proportion\n",
    "        else:\n",
    "            warmup_steps = 0\n",
    "\n",
    "        scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss, logging_loss = 0.0, 0.0\n",
    "        self.model.zero_grad()\n",
    "        train_iterator = trange(int(num_epochs), desc=\"Epoch\")\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.model.train()\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                inputs = {'input_ids':       batch[0],\n",
    "                          'attention_mask':  batch[1], \n",
    "                          'token_type_ids':  batch[2],  \n",
    "                          'start_positions': batch[3], \n",
    "                          'end_positions':   batch[4]}\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                loss = outputs[0]  # model outputs are always tuple in pytorch-transformers\n",
    "\n",
    "                loss = loss.mean() # mean() to average on multi-gpu parallel (not distributed) training\n",
    "\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                optimizer.step()\n",
    "                self.model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if global_step % 50 == 0:\n",
    "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/50, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "            epoch_iterator.close()\n",
    "        train_iterator.close()\n",
    "\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "        print (\"Average training loss: {}\".format(tr_loss / global_step))\n",
    "\n",
    "        if model_output_dir:\n",
    "            if not os.path.exists(model_output_dir):\n",
    "                os.makedirs(model_output_dir)\n",
    "\n",
    "            logger.info(\"Saving model checkpoint to %s\", model_output_dir)\n",
    "            # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "            # They can then be reloaded using `from_pretrained()`\n",
    "            model_to_save = self.model.module if hasattr(self.model, 'module') else model  # Take care of distributed/parallel training\n",
    "            model_to_save.save_pretrained(model_output_dir)\n",
    "\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        features,\n",
    "        num_gpus=None,\n",
    "        batch_size=32,\n",
    "        probabilities=False):\n",
    "        \n",
    "        device = get_device(\"cpu\" if num_gpus == 0 or not torch.cuda.is_available() else \"gpu\")\n",
    "        self.model = move_to_device(self.model, device, num_gpus)\n",
    "\n",
    "        # score\n",
    "        self.model.eval()\n",
    "\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "\n",
    "        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "        test_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                                     all_example_index)\n",
    "\n",
    "        test_sampler = SequentialSampler(test_dataset)\n",
    "        test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "        all_results = []\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {'input_ids':      batch[0],\n",
    "                          'attention_mask': batch[1],\n",
    "                          'token_type_ids': batch[2]\n",
    "                        }\n",
    "                example_indices = batch[3]\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "            for i, example_index in enumerate(example_indices):\n",
    "                test_feature = features[example_index.item()]\n",
    "                unique_id = int(test_feature.unique_id)\n",
    "\n",
    "                result = RawResult(unique_id    = unique_id,\n",
    "                                   start_logits = to_list(outputs[0][i]),\n",
    "                                   end_logits   = to_list(outputs[1][i]))\n",
    "                all_results.append(result)\n",
    "qa_extractor = BERTQAExtractor(language=LANGUAGE, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features[0:63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "qa_extractor.fit(train_features,\n",
    "                 num_gpus=0,\n",
    "                 num_epochs=1,\n",
    "                 batch_size=32,\n",
    "                 lr=2e-5,\n",
    "                 warmup_proportion=0.1,\n",
    "                 max_grad_norm=1.0,\n",
    "                 model_output_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_cpu",
   "language": "python",
   "name": "nlp_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
