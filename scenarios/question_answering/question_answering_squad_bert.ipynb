{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering on the SQuAD Dataset using BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before You Start\n",
    "\n",
    "The running time shown in this notebook is on a Standard_NC24s_v3 Azure Deep Learning Virtual Machine with 4 NVIDIA Tesla V100 GPUs. If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`** to run the notebook on a small subset of the data and a smaller number of epochs. \n",
    "The table below provides some reference running time on different machine configurations.  \n",
    "\n",
    "|QUICK_RUN|Machine Configurations|Running time|\n",
    "|:---------|:----------------------|:------------|\n",
    "|True|4 **CPU**s, 14GB memory| ~ 10 minutes |\n",
    "|True|1 NVIDIA Tesla K80 GPUs, 12GB GPU memory| |\n",
    "|False|1 NVIDIA Tesla K80 GPUs, 12GB GPU memory| |\n",
    "|False|4 NVIDIA Tesla V100 GPUs, 64GB GPU memory| |\n",
    "\n",
    "If you run into CUDA out-of-memory error, try reducing the `BATCH_SIZE` and `MAX_SEQ_LENGTH`, but note that model performance will be compromised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This notebook demonstrates how to fine tune [pretrained BERT model](https://github.com/huggingface/pytorch-transformers) for extractive question answering task. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, result postprocessing, and model evaluation. \n",
    "\n",
    "BERT[\\[1\\]](#References) is a powerful pre-trained lanaguage model that can be used for multiple NLP tasks, including text classification, question answering, named entity recognition, etc. It's able to achieve state of the art performance with only a few epochs of fine tuning on task specific datasets.  \n",
    "The figure below illustrates how BERT can be fine tuned for extractive question answering task. The question and paragraph tokens are concatenated as a single input token sequence with a special token [SEP] between them. For the paragraph tokens, BERT predicts the probabilities of each token being the start and end of the answer span. The tokens with the highest sum of starting probability and ending probability define the span of the predicted answer\n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/bert_qa.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "nlp_path = os.path.abspath('../../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "\n",
    "from utils_nlp.dataset.squad import load_pandas_df\n",
    "from utils_nlp.models.bert.common import Language, Tokenizer\n",
    "from utils_nlp.models.bert.question_answering import BERTQAExtractor\n",
    "from utils_nlp.models.bert.qa_utils import postprocess_answer, evaluate_qa\n",
    "from utils_nlp.common.timer import Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_USED_PERCENT = 1\n",
    "DEV_DATA_USED_PERCENT = 1\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "if QUICK_RUN:\n",
    "    TRAIN_DATA_USED_PERCENT = 0.001\n",
    "    DEV_DATA_USED_PERCENT = 0.01\n",
    "    NUM_EPOCHS = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    MAX_SEQ_LENGTH = 384\n",
    "    DOC_STRIDE = 128\n",
    "    BATCH_SIZE = 8\n",
    "else:\n",
    "    MAX_SEQ_LENGTH = 128\n",
    "    DOC_STRIDE = 64\n",
    "    BATCH_SIZE = 4\n",
    "    \n",
    "SQUAD_VERSION = \"v1.1\" \n",
    "CACHE_DIR = \"./temp\"\n",
    "\n",
    "LANGUAGE = Language.ENGLISHLARGEWWM\n",
    "DO_LOWER_CASE = True\n",
    "\n",
    "MAX_QUESTION_LENGTH = 64\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP = 0.1\n",
    "\n",
    "DOC_TEXT_COL = \"doc_text\"\n",
    "QUESTION_TEXT_COL = \"question_text\"\n",
    "ANSWER_START_COL = \"answer_start\"\n",
    "ANSWER_TEXT_COL = \"answer_text\"\n",
    "QA_ID_COL = \"qa_id\"\n",
    "IS_IMPOSSIBLE_COL = \"is_impossible\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if  torch.cuda.device_count() > 0:\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SQuAD Dataset\n",
    "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. [\\[2, 3\\]](#References)\n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/squad.png\">\n",
    "\n",
    "There has been two versions of SQuAD datasets. SQuAD 1.1 contains 100,000+ question-answer pairs on 500+ articles. SQuAD 2.0 adds 50,000 new, unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. These datasets are available at [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/). Each dataset comes with a training dataset and a development dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility function `load_pandas_df` downloads the dataset specified by `squad_version` and `file_split` to `local_cache_path` if it doesn't exist already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_pandas_df(local_cache_path=\".\", squad_version=\"v1.1\", file_split=\"train\")\n",
    "dev_df = load_pandas_df(local_cache_path=\".\", squad_version=\"v1.1\", file_split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_text</th>\n",
       "      <th>question_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>qa_id</th>\n",
       "      <th>is_impossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>515</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>188</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>279</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>381</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>92</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_text  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                       question_text  answer_start  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...           515   \n",
       "1  What is in front of the Notre Dame Main Building?           188   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...           279   \n",
       "3                  What is the Grotto at Notre Dame?           381   \n",
       "4  What sits on top of the Main Building at Notre...            92   \n",
       "\n",
       "                               answer_text                     qa_id  \\\n",
       "0               Saint Bernadette Soubirous  5733be284776f41900661182   \n",
       "1                a copper statue of Christ  5733be284776f4190066117f   \n",
       "2                        the Main Building  5733be284776f41900661180   \n",
       "3  a Marian place of prayer and reflection  5733be284776f41900661181   \n",
       "4       a golden statue of the Virgin Mary  5733be284776f4190066117e   \n",
       "\n",
       "   is_impossible  \n",
       "0          False  \n",
       "1          False  \n",
       "2          False  \n",
       "3          False  \n",
       "4          False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_text</th>\n",
       "      <th>question_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>qa_id</th>\n",
       "      <th>is_impossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
       "      <td>[177, 177, 177]</td>\n",
       "      <td>[Denver Broncos, Denver Broncos, Denver Broncos]</td>\n",
       "      <td>56be4db0acb8001400a502ec</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
       "      <td>[249, 249, 249]</td>\n",
       "      <td>[Carolina Panthers, Carolina Panthers, Carolin...</td>\n",
       "      <td>56be4db0acb8001400a502ed</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Where did Super Bowl 50 take place?</td>\n",
       "      <td>[403, 355, 355]</td>\n",
       "      <td>[Santa Clara, California, Levi's Stadium, Levi...</td>\n",
       "      <td>56be4db0acb8001400a502ee</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team won Super Bowl 50?</td>\n",
       "      <td>[177, 177, 177]</td>\n",
       "      <td>[Denver Broncos, Denver Broncos, Denver Broncos]</td>\n",
       "      <td>56be4db0acb8001400a502ef</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>What color was used to emphasize the 50th anni...</td>\n",
       "      <td>[488, 488, 521]</td>\n",
       "      <td>[gold, gold, gold]</td>\n",
       "      <td>56be4db0acb8001400a502f0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_text  \\\n",
       "0  Super Bowl 50 was an American football game to...   \n",
       "1  Super Bowl 50 was an American football game to...   \n",
       "2  Super Bowl 50 was an American football game to...   \n",
       "3  Super Bowl 50 was an American football game to...   \n",
       "4  Super Bowl 50 was an American football game to...   \n",
       "\n",
       "                                       question_text     answer_start  \\\n",
       "0  Which NFL team represented the AFC at Super Bo...  [177, 177, 177]   \n",
       "1  Which NFL team represented the NFC at Super Bo...  [249, 249, 249]   \n",
       "2                Where did Super Bowl 50 take place?  [403, 355, 355]   \n",
       "3                  Which NFL team won Super Bowl 50?  [177, 177, 177]   \n",
       "4  What color was used to emphasize the 50th anni...  [488, 488, 521]   \n",
       "\n",
       "                                         answer_text  \\\n",
       "0   [Denver Broncos, Denver Broncos, Denver Broncos]   \n",
       "1  [Carolina Panthers, Carolina Panthers, Carolin...   \n",
       "2  [Santa Clara, California, Levi's Stadium, Levi...   \n",
       "3   [Denver Broncos, Denver Broncos, Denver Broncos]   \n",
       "4                                 [gold, gold, gold]   \n",
       "\n",
       "                      qa_id  is_impossible  \n",
       "0  56be4db0acb8001400a502ec          False  \n",
       "1  56be4db0acb8001400a502ed          False  \n",
       "2  56be4db0acb8001400a502ee          False  \n",
       "3  56be4db0acb8001400a502ef          False  \n",
       "4  56be4db0acb8001400a502f0          False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=TRAIN_DATA_USED_PERCENT).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(frac=DEV_DATA_USED_PERCENT).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(language=LANGUAGE, to_lower=DO_LOWER_CASE, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tokenizer_qa` method of `Tokenizer` tokenizes the input paragraph, question, and answer texts and converts them into the format required by pre-trained BERT model, involving the following steps:\n",
    "* WordPiece tokenization.\n",
    "* Convert character-based answer span indices to token-based indices.\n",
    "* Truncate the question token list if it's longer than `max_question_length`.\n",
    "* Split the paragraph into multiple segments if it's longer than `max_len` - `max_question_length` - 3. (The \"-3\" is for the special [CLS] token and two [SEP] tokens.)\n",
    "* Add the special tokens [CLS] and [SEP].\n",
    "* Pad the concatenated token sequence to `max_len` if it's shorter.\n",
    "* Convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary.\n",
    "\n",
    "In additional to the features required by BERT, `tokenize_qa` outputs a few additional fields needed by postprocessing. See the `QAFeatures` class in [qa_utils.py](../../utils_nlp/models/bert/qa_utils.py) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, qa_examples = tokenizer.tokenize_qa(\n",
    "    doc_text=train_df[DOC_TEXT_COL], \n",
    "    question_text=train_df[QUESTION_TEXT_COL], \n",
    "    answer_start=train_df[ANSWER_START_COL], \n",
    "    answer_text=train_df[ANSWER_TEXT_COL],\n",
    "    qa_id=train_df[QA_ID_COL],\n",
    "    is_impossible=train_df[IS_IMPOSSIBLE_COL],\n",
    "    is_training=True,\n",
    "    max_len=MAX_SEQ_LENGTH,\n",
    "    max_question_length=MAX_QUESTION_LENGTH,\n",
    "    doc_stride=DOC_STRIDE,\n",
    "    cache_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_features, dev_examples = tokenizer.tokenize_qa(\n",
    "    doc_text=dev_df[DOC_TEXT_COL], \n",
    "    question_text=dev_df[QUESTION_TEXT_COL], \n",
    "    answer_start=dev_df[ANSWER_START_COL], \n",
    "    answer_text=dev_df[ANSWER_TEXT_COL],\n",
    "    qa_id=dev_df[QA_ID_COL],\n",
    "    is_impossible=dev_df[IS_IMPOSSIBLE_COL],\n",
    "    is_training=False,\n",
    "    max_len=MAX_SEQ_LENGTH,\n",
    "    max_question_length=MAX_QUESTION_LENGTH,\n",
    "    doc_stride=DOC_STRIDE,\n",
    "    cache_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_id\n",
      "1000000000\n",
      "\n",
      "qa_id\n",
      "56de4d9ecffd8e1900b4b7e2\n",
      "\n",
      "tokens\n",
      "['[CLS]', 'what', 'year', 'was', 'the', 'ban', '##ska', 'aka', '##de', '##mia', 'founded', '?', '[SEP]', 'the', 'world', \"'\", 's', 'first', 'institution', 'of', 'technology', 'or', 'technical', 'university', 'with', 'tertiary', 'technical', 'education', 'is', 'the', 'ban', '##ska', 'aka', '##de', '##mia', 'in', 'ban', '##ska', 'st', '##ia', '##vn', '##ica', ',', 'slovakia', ',', 'founded', 'in', '1735', ',', 'academy', 'since', 'december', '13', ',', '1762', 'established', 'by', 'queen', 'maria', 'theresa', 'in', 'order', 'to', 'train', 'specialists', 'of', 'silver', 'and', 'gold', 'mining', 'and', 'metal', '##lu', '##rgy', 'in', 'neighbourhood', '.', 'teaching', 'started', 'in', '1764', '.', 'later', 'the', 'department', 'of', 'mathematics', ',', 'mechanics', 'and', 'hydraulic', '##s', 'and', 'department', 'of', 'forestry', 'were', 'settled', '.', 'university', 'buildings', 'are', 'still', 'at', 'their', 'place', 'today', 'and', 'are', 'used', 'for', 'teaching', '.', 'university', 'has', 'launched', 'the', 'first', 'book', 'of', 'electro', '##tech', '##nic', '##s', 'in', 'the', 'world', '[SEP]']\n",
      "\n",
      "token_to_orig_map\n",
      "{13: 0, 14: 1, 15: 1, 16: 1, 17: 2, 18: 3, 19: 4, 20: 5, 21: 6, 22: 7, 23: 8, 24: 9, 25: 10, 26: 11, 27: 12, 28: 13, 29: 14, 30: 15, 31: 15, 32: 16, 33: 16, 34: 16, 35: 17, 36: 18, 37: 18, 38: 19, 39: 19, 40: 19, 41: 19, 42: 19, 43: 20, 44: 20, 45: 21, 46: 22, 47: 23, 48: 23, 49: 24, 50: 25, 51: 26, 52: 27, 53: 27, 54: 28, 55: 29, 56: 30, 57: 31, 58: 32, 59: 33, 60: 34, 61: 35, 62: 36, 63: 37, 64: 38, 65: 39, 66: 40, 67: 41, 68: 42, 69: 43, 70: 44, 71: 45, 72: 45, 73: 45, 74: 46, 75: 47, 76: 47, 77: 48, 78: 49, 79: 50, 80: 51, 81: 51, 82: 52, 83: 53, 84: 54, 85: 55, 86: 56, 87: 56, 88: 57, 89: 58, 90: 59, 91: 59, 92: 60, 93: 61, 94: 62, 95: 63, 96: 64, 97: 65, 98: 65, 99: 66, 100: 67, 101: 68, 102: 69, 103: 70, 104: 71, 105: 72, 106: 73, 107: 74, 108: 75, 109: 76, 110: 77, 111: 78, 112: 78, 113: 79, 114: 80, 115: 81, 116: 82, 117: 83, 118: 84, 119: 85, 120: 86, 121: 86, 122: 86, 123: 86, 124: 87, 125: 88, 126: 89}\n",
      "\n",
      "token_is_max_context\n",
      "{13: True, 14: True, 15: True, 16: True, 17: True, 18: True, 19: True, 20: True, 21: True, 22: True, 23: True, 24: True, 25: True, 26: True, 27: True, 28: True, 29: True, 30: True, 31: True, 32: True, 33: True, 34: True, 35: True, 36: True, 37: True, 38: True, 39: True, 40: True, 41: True, 42: True, 43: True, 44: True, 45: True, 46: True, 47: True, 48: True, 49: True, 50: True, 51: True, 52: True, 53: True, 54: True, 55: True, 56: True, 57: True, 58: True, 59: True, 60: True, 61: True, 62: True, 63: True, 64: True, 65: True, 66: True, 67: True, 68: True, 69: True, 70: True, 71: True, 72: True, 73: True, 74: True, 75: True, 76: True, 77: True, 78: True, 79: True, 80: True, 81: True, 82: True, 83: True, 84: True, 85: True, 86: True, 87: True, 88: True, 89: True, 90: True, 91: True, 92: True, 93: True, 94: True, 95: True, 96: True, 97: True, 98: True, 99: True, 100: True, 101: True, 102: False, 103: False, 104: False, 105: False, 106: False, 107: False, 108: False, 109: False, 110: False, 111: False, 112: False, 113: False, 114: False, 115: False, 116: False, 117: False, 118: False, 119: False, 120: False, 121: False, 122: False, 123: False, 124: False, 125: False, 126: False}\n",
      "\n",
      "input_ids\n",
      "[101, 2054, 2095, 2001, 1996, 7221, 8337, 9875, 3207, 10092, 2631, 1029, 102, 1996, 2088, 1005, 1055, 2034, 5145, 1997, 2974, 2030, 4087, 2118, 2007, 13553, 4087, 2495, 2003, 1996, 7221, 8337, 9875, 3207, 10092, 1999, 7221, 8337, 2358, 2401, 16022, 5555, 1010, 10991, 1010, 2631, 1999, 26063, 1010, 2914, 2144, 2285, 2410, 1010, 20827, 2511, 2011, 3035, 3814, 14781, 1999, 2344, 2000, 3345, 15744, 1997, 3165, 1998, 2751, 5471, 1998, 3384, 7630, 22637, 1999, 10971, 1012, 4252, 2318, 1999, 21488, 1012, 2101, 1996, 2533, 1997, 5597, 1010, 9760, 1998, 14761, 2015, 1998, 2533, 1997, 13116, 2020, 3876, 1012, 2118, 3121, 2024, 2145, 2012, 2037, 2173, 2651, 1998, 2024, 2109, 2005, 4252, 1012, 2118, 2038, 3390, 1996, 2034, 2338, 1997, 16175, 15007, 8713, 2015, 1999, 1996, 2088, 102]\n",
      "\n",
      "input_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "segment_ids\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "start_position\n",
      "47\n",
      "\n",
      "end_position\n",
      "47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_feature = train_features[0]\n",
    "for f in type(sample_feature)._fields:\n",
    "    print(f)\n",
    "    print(getattr(sample_feature, f))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BERTQAExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_extractor = BERTQAExtractor(language=LANGUAGE, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  16%|█▋        | 8/49 [01:06<05:40,  8.31s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 8/49 [01:20<05:40,  8.31s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 16/49 [02:12<04:33,  8.28s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 16/49 [02:30<04:33,  8.28s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 24/49 [03:17<03:25,  8.24s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 24/49 [03:30<03:25,  8.24s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 32/49 [04:22<02:19,  8.22s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 32/49 [04:40<02:19,  8.22s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 40/49 [05:28<01:14,  8.23s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 40/49 [05:40<01:14,  8.23s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 48/49 [06:34<00:08,  8.22s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [06:38<00:00, 398.38s/it]s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 0.113 hrs\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    qa_extractor.fit(train_features,\n",
    "                     num_epochs=NUM_EPOCHS,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     learning_rate=LEARNING_RATE,\n",
    "                     cache_model=True)\n",
    "print(\"Training time : {:.3f} hrs\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Note that the `BERTQAExtractor.predict` only outputs the probabilities of each token being the start and end of the answer span. the `postprocess_answers` method takes these probabilities and generates the final answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 7/7 [01:38<00:00, 13.92s/it]\n"
     ]
    }
   ],
   "source": [
    "qa_results = qa_extractor.predict(dev_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess and Generate the Final Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answers, answer_probs, nbest_answers = postprocess_answer(qa_results,\n",
    "                                                                dev_examples, \n",
    "                                                                dev_features, \n",
    "                                                                do_lower_case=DO_LOWER_CASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph:\n",
      "Immunology is strongly experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th century saw a battle between \"cellular\" and \"humoral\" theories of immunity. According to the cellular theory of immunity, represented in particular by Elie Metchnikoff, it was cells – more precisely, phagocytes – that were responsible for immune responses. In contrast, the humoral theory of immunity, held, among others, by Robert Koch and Emil von Behring, stated that the active immune agents were soluble components (molecules) found in the organism’s “humors” rather than its cells.\n",
      "\n",
      "Question:\n",
      "What two scientists were proponents of the humoral theory of immunity?\n",
      "\n",
      "Ground truth answers:\n",
      "['Robert Koch and Emil von Behring', 'Robert Koch and Emil von Behring', 'Robert Koch and Emil von Behring,']\n",
      "\n",
      "Predicted answer:\n",
      "immunology\n",
      "\n",
      "Top N best answers\n",
      "[OrderedDict([('text', 'immunology'), ('probability', 0.11926427087296353), ('start_logit', 1.5627570152282715), ('end_logit', 2.8968493938446045)]), OrderedDict([('text', 'Immunology is strongly experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology'), ('probability', 0.10079808597672599), ('start_logit', 1.3945345878601074), ('end_logit', 2.8968493938446045)]), OrderedDict([('text', 'Elie Metchnikoff, it was cells – more precisely, phagocytes'), ('probability', 0.09470158230014437), ('start_logit', 1.924297571182251), ('end_logit', 2.3046977519989014)]), OrderedDict([('text', 'Immunology'), ('probability', 0.05555881724407744), ('start_logit', 1.3945345878601074), ('end_logit', 2.3011722564697266)]), OrderedDict([('text', 'cellular theory of immunity, represented in particular by Elie Metchnikoff, it was cells – more precisely, phagocytes'), ('probability', 0.05457365080997463), ('start_logit', 1.3731180429458618), ('end_logit', 2.3046977519989014)]), OrderedDict([('text', 'theoretical attitude. Many theories have been suggested in immunology'), ('probability', 0.05227363480493965), ('start_logit', 0.737907350063324), ('end_logit', 2.8968493938446045)]), OrderedDict([('text', 'Elie Metchnikoff'), ('probability', 0.04491490225902066), ('start_logit', 1.924297571182251), ('end_logit', 1.5587366819381714)]), OrderedDict([('text', 'Elie Metchnikoff, it was cells – more precisely, phago'), ('probability', 0.044421372913633086), ('start_logit', 1.924297571182251), ('end_logit', 1.5476877689361572)]), OrderedDict([('text', 'immunology from the end of the nineteenth'), ('probability', 0.043125743046841615), ('start_logit', 1.5627570152282715), ('end_logit', 1.8796277046203613)]), OrderedDict([('text', 'phagocytes'), ('probability', 0.042253415435895836), ('start_logit', 0.9897353053092957), ('end_logit', 2.4322144985198975)]), OrderedDict([('text', 'Elie Metchnikoff, it was cells – more precisely, ph'), ('probability', 0.04035785696587366), ('start_logit', 1.924297571182251), ('end_logit', 1.4517531394958496)]), OrderedDict([('text', 'immunology from the end of the nineteenth century up to the present time. The end of the 19th century'), ('probability', 0.039195214764241676), ('start_logit', 1.5627570152282715), ('end_logit', 1.7840622663497925)]), OrderedDict([('text', 'experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology'), ('probability', 0.0372611673488718), ('start_logit', 0.39936691522598267), ('end_logit', 2.8968493938446045)]), OrderedDict([('text', 'immunology from the end of the nineteenth century'), ('probability', 0.03591549770700685), ('start_logit', 1.5627570152282715), ('end_logit', 1.69667649269104)]), OrderedDict([('text', 'theories have been suggested in immunology'), ('probability', 0.03461924857749111), ('start_logit', 0.3258250653743744), ('end_logit', 2.8968493938446045)]), OrderedDict([('text', 'immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th century'), ('probability', 0.03408127652524372), ('start_logit', 1.5627570152282715), ('end_logit', 1.6442557573318481)]), OrderedDict([('text', 'immunology from the end of the nineteenth century up to the present time. The end of the 19th'), ('probability', 0.033450711908190045), ('start_logit', 1.5627570152282715), ('end_logit', 1.6255806684494019)]), OrderedDict([('text', 'Metchnikoff, it was cells – more precisely, phagocytes'), ('probability', 0.03237776594665386), ('start_logit', 0.8510388135910034), ('end_logit', 2.3046977519989014)]), OrderedDict([('text', 'Koch'), ('probability', 0.03204079406184822), ('start_logit', 1.4120653867721558), ('end_logit', 1.7332091331481934)]), OrderedDict([('text', 'immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th'), ('probability', 0.02881499053036202), ('start_logit', 1.5627570152282715), ('end_logit', 1.4764033555984497)])]\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Paragraph:\n",
      "Luther devised the catechism as a method of imparting the basics of Christianity to the congregations. In 1529, he wrote the Large Catechism, a manual for pastors and teachers, as well as a synopsis, the Small Catechism, to be memorised by the people themselves. The catechisms provided easy-to-understand instructional and devotional material on the Ten Commandments, the Apostles' Creed, the Lord's Prayer, baptism, and the Lord's Supper. Luther incorporated questions and answers in the catechism so that the basics of Christian faith would not just be learned by rote, \"the way monkeys do it\", but understood.\n",
      "\n",
      "Question:\n",
      "For whom was the Small Catechism meant?\n",
      "\n",
      "Ground truth answers:\n",
      "['the people', 'the people', 'people']\n",
      "\n",
      "Predicted answer:\n",
      "1529\n",
      "\n",
      "Top N best answers\n",
      "[OrderedDict([('text', '1529'), ('probability', 0.16151302798337372), ('start_logit', 2.264540195465088), ('end_logit', 3.7017030715942383)]), OrderedDict([('text', '1529, he wrote the Large Catechism'), ('probability', 0.10582451122342178), ('start_logit', 2.264540195465088), ('end_logit', 3.2788994312286377)]), OrderedDict([('text', 'Luther devised the catechism as a method of imparting the basics of Christianity to the congregations. In 1529'), ('probability', 0.0742695478545613), ('start_logit', 1.4876554012298584), ('end_logit', 3.7017030715942383)]), OrderedDict([('text', 'Luther devised the catechism'), ('probability', 0.0626308193569013), ('start_logit', 1.4876554012298584), ('end_logit', 3.531259536743164)]), OrderedDict([('text', '1529, he wrote the Large Catech'), ('probability', 0.051524455490235835), ('start_logit', 2.264540195465088), ('end_logit', 2.559173822402954)]), OrderedDict([('text', '1529, he wrote the Large Catechism, a manual for pastors and teachers, as well as a synopsis, the Small Catech'), ('probability', 0.05144806642945756), ('start_logit', 2.264540195465088), ('end_logit', 2.557690143585205)]), OrderedDict([('text', 'pastors and teachers, as well as a synopsis, the Small Catechism'), ('probability', 0.045508809610730123), ('start_logit', 1.5053595304489136), ('end_logit', 3.1942038536071777)]), OrderedDict([('text', 'catechism as a method of imparting the basics of Christianity to the congregations. In 1529'), ('probability', 0.04429257714019024), ('start_logit', 0.9707714915275574), ('end_logit', 3.7017030715942383)]), OrderedDict([('text', 'echism as a method of imparting the basics of Christianity to the congregations. In 1529'), ('probability', 0.04366118436077412), ('start_logit', 0.9564138650894165), ('end_logit', 3.7017030715942383)]), OrderedDict([('text', '152'), ('probability', 0.042740639874585214), ('start_logit', 2.264540195465088), ('end_logit', 2.372267484664917)]), OrderedDict([('text', '9'), ('probability', 0.04096666968971866), ('start_logit', 0.892713189125061), ('end_logit', 3.7017030715942383)]), OrderedDict([('text', 'catechism'), ('probability', 0.03735151859482473), ('start_logit', 0.9707714915275574), ('end_logit', 3.531259536743164)]), OrderedDict([('text', 'echism'), ('probability', 0.036819070932852936), ('start_logit', 0.9564138650894165), ('end_logit', 3.531259536743164)]), OrderedDict([('text', 'Christianity to the congregations. In 1529'), ('probability', 0.033915201569494756), ('start_logit', 0.7038177251815796), ('end_logit', 3.7017030715942383)]), OrderedDict([('text', 'Luther incorporated questions and answers in the catechism'), ('probability', 0.030670846162242256), ('start_logit', 1.1955108642578125), ('end_logit', 3.109459161758423)]), OrderedDict([('text', 'catechism as a method of imparting the basics of Christianity to the congregations. In 1529, he wrote the Large Catechism'), ('probability', 0.029020818847931236), ('start_logit', 0.9707714915275574), ('end_logit', 3.2788994312286377)]), OrderedDict([('text', 'echism as a method of imparting the basics of Christianity to the congregations. In 1529, he wrote the Large Catechism'), ('probability', 0.02860712570437512), ('start_logit', 0.9564138650894165), ('end_logit', 3.2788994312286377)]), OrderedDict([('text', 'Large Catechism'), ('probability', 0.027305435219342193), ('start_logit', 0.9098438024520874), ('end_logit', 3.2788994312286377)]), OrderedDict([('text', '9, he wrote the Large Catechism'), ('probability', 0.02684166008461016), ('start_logit', 0.892713189125061), ('end_logit', 3.2788994312286377)]), OrderedDict([('text', 'Large Catechism, a manual for pastors and teachers, as well as a synopsis, the Small Catechism'), ('probability', 0.025088013870376666), ('start_logit', 0.9098438024520874), ('end_logit', 3.1942038536071777)])]\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Paragraph:\n",
      "There would be no more scoring in the third quarter, but early in the fourth, the Broncos drove to the Panthers 41-yard line. On the next play, Ealy knocked the ball out of Manning's hand as he was winding up for a pass, and then recovered it for Carolina on the 50-yard line. A 16-yard reception by Devin Funchess and a 12-yard run by Stewart then set up Gano's 39-yard field goal, cutting the Panthers deficit to one score at 16–10. The next three drives of the game would end in punts.\n",
      "\n",
      "Question:\n",
      "Where was the ball recovered?\n",
      "\n",
      "Ground truth answers:\n",
      "['50-yard line.', 'on the 50-yard line', '50']\n",
      "\n",
      "Predicted answer:\n",
      "39-yard field goal, cutting the Panthers deficit to one score at 16–10\n",
      "\n",
      "Top N best answers\n",
      "[OrderedDict([('text', '39-yard field goal, cutting the Panthers deficit to one score at 16–10'), ('probability', 0.11323057962196438), ('start_logit', 1.7620149850845337), ('end_logit', 2.4382121562957764)]), OrderedDict([('text', '16–10'), ('probability', 0.07917626477693805), ('start_logit', 1.512794852256775), ('end_logit', 2.3296825885772705)]), OrderedDict([('text', \"Gano's 39-yard field goal, cutting the Panthers deficit to one score at 16–10\"), ('probability', 0.07436028054962641), ('start_logit', 1.3415106534957886), ('end_logit', 2.4382121562957764)]), OrderedDict([('text', \"12-yard run by Stewart then set up Gano's 39-yard field goal, cutting the Panthers deficit to one score at 16–10\"), ('probability', 0.07186311362782044), ('start_logit', 1.30735182762146), ('end_logit', 2.4382121562957764)]), OrderedDict([('text', '10'), ('probability', 0.06225450815506263), ('start_logit', 1.272349238395691), ('end_logit', 2.3296825885772705)]), OrderedDict([('text', '39-yard field goal'), ('probability', 0.05830465506357455), ('start_logit', 1.7620149850845337), ('end_logit', 1.7744678258895874)]), OrderedDict([('text', '50-yard line. A 16-yard reception'), ('probability', 0.048687745125823086), ('start_logit', 1.9914547204971313), ('end_logit', 1.3647735118865967)]), OrderedDict([('text', '39'), ('probability', 0.048603613010458485), ('start_logit', 1.7620149850845337), ('end_logit', 1.5924837589263916)]), OrderedDict([('text', '50-yard line. A 16-yard reception by Devin Funchess and a 12-yard run by Stewart then set up Gan'), ('probability', 0.04247073206487171), ('start_logit', 1.9914547204971313), ('end_logit', 1.228161334991455)]), OrderedDict([('text', \"16-yard reception by Devin Funchess and a 12-yard run by Stewart then set up Gano's 39-yard field goal\"), ('probability', 0.04043162445912671), ('start_logit', 1.3959453105926514), ('end_logit', 1.7744678258895874)]), OrderedDict([('text', 'Broncos'), ('probability', 0.03970639425668831), ('start_logit', 1.811021327972412), ('end_logit', 1.3412917852401733)]), OrderedDict([('text', '50-yard line. A 16-yard reception by Devin Funchess and a 12'), ('probability', 0.03891044151227419), ('start_logit', 1.9914547204971313), ('end_logit', 1.140608787536621)]), OrderedDict([('text', '50-yard line'), ('probability', 0.038455434098544), ('start_logit', 1.9914547204971313), ('end_logit', 1.1288461685180664)]), OrderedDict([('text', \"Gano's 39-yard field goal\"), ('probability', 0.038289572678612285), ('start_logit', 1.3415106534957886), ('end_logit', 1.7744678258895874)]), OrderedDict([('text', 'Panthers deficit to one score at 16–10'), ('probability', 0.038014902666407266), ('start_logit', 0.7790965437889099), ('end_logit', 2.3296825885772705)]), OrderedDict([('text', \"12-yard run by Stewart then set up Gano's 39-yard field goal\"), ('probability', 0.03700373225901751), ('start_logit', 1.30735182762146), ('end_logit', 1.7744678258895874)]), OrderedDict([('text', \"16-yard reception by Devin Funchess and a 12-yard run by Stewart then set up Gano's 39\"), ('probability', 0.033704393353375314), ('start_logit', 1.3959453105926514), ('end_logit', 1.5924837589263916)]), OrderedDict([('text', 'third quarter'), ('probability', 0.03360085132383324), ('start_logit', 1.1678296327590942), ('end_logit', 1.817522644996643)]), OrderedDict([('text', \"Gano's 39\"), ('probability', 0.03191874766736679), ('start_logit', 1.3415106534957886), ('end_logit', 1.5924837589263916)]), OrderedDict([('text', 'Broncos drove to the Panthers 41-yard line'), ('probability', 0.03101241372861469), ('start_logit', 1.811021327972412), ('end_logit', 1.0941671133041382)])]\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 10, 100]:\n",
    "    print('Paragraph:')\n",
    "    print(dev_df.iloc[i]['doc_text'])\n",
    "    print()\n",
    "    print('Question:')\n",
    "    print(dev_df.iloc[i]['question_text'])\n",
    "    print()\n",
    "    print('Ground truth answers:')\n",
    "    print(dev_df.iloc[i]['answer_text'])\n",
    "    print()\n",
    "    print('Predicted answer:')\n",
    "    print(final_answers[dev_df.iloc[i]['qa_id']])\n",
    "    print()\n",
    "    print('Top N best answers')\n",
    "    print(nbest_answers[dev_df.iloc[i]['qa_id']])\n",
    "    print('-------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question answering task is usually evaluated on two metrics: exact match (EM) and F1 score.   \n",
    "The exact match is computed by first performing some simple normalization (e.g. remove punctuation and convert to lower case) on the ground truth and predicted answers and check if they match exactly after normalization.   \n",
    "F1 score is computed from token-level precision and recall by comparing the ground truth and predicted answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"exact\": 4.716981132075472,\n",
      "  \"f1\": 9.707557742938592,\n",
      "  \"total\": 106,\n",
      "  \"HasAns_exact\": 4.716981132075472,\n",
      "  \"HasAns_f1\": 9.707557742938592,\n",
      "  \"HasAns_total\": 106\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "evaluation_result = evaluate_qa(qa_ids=dev_df['qa_id'], \n",
    "                                actuals=dev_df['answer_text'], \n",
    "                                preds=final_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:08:43.716598\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_extractor_cached = BERTQAExtractor(language=LANGUAGE, cache_dir='/home/hlu/notebooks/NLP/scenarios/question_answering/temp/', load_from_cache=True)\n",
    "# dev_features_cached = torch.load('/home/hlu/notebooks/NLP/scenarios/question_answering/temp/cached_features')\n",
    "# dev_examples_cached = torch.load('/home/hlu/notebooks/NLP/scenarios/question_answering/temp/cached_examples')\n",
    "# qa_results_new = qa_extractor_cached.predict(dev_features_cached)\n",
    "# final_answers_new, _, _ = postprocess_answers(qa_results_new,\n",
    "#                                               dev_examples_cached, \n",
    "#                                               dev_features_cached, \n",
    "#                                               do_lower_case=DO_LOWER_CASE)\n",
    "# evaluate_qa(qa_ids=dev_df['qa_id'], \n",
    "#             actuals=dev_df['answer_text'], \n",
    "#             preds=final_answers_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_features_short = []\n",
    "# qa_id_short = []\n",
    "# for example in dev_examples_cached[:10]:\n",
    "#     qa_id = example.qa_id\n",
    "#     qa_id_short.append(qa_id)\n",
    "#     for f in dev_features_cached:\n",
    "#         if f.qa_id == qa_id:\n",
    "#             dev_features_short.append(f)\n",
    "# answer_text_short = dev_df.loc[dev_df[\"qa_id\"].isin(qa_id_short), 'answer_text']\n",
    "# qa_results_new_short = qa_extractor_cached.predict(dev_features_short)\n",
    "# final_answers_new_short, _, _ = postprocess_answers(qa_results_new_short,\n",
    "#                                               dev_examples_cached[:10], \n",
    "#                                               dev_features_short, \n",
    "#                                               do_lower_case=DO_LOWER_CASE)\n",
    "# evaluate_qa(qa_ids=qa_id_short, \n",
    "#             actuals=answer_text_short, \n",
    "#             preds=final_answers_new_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_answers_new, final_probs, nbest_answers = postprocess_answers(qa_results_new,\n",
    "#                                                 dev_examples_cached, \n",
    "#                                                 dev_features_cached, \n",
    "#                                                 do_lower_case=DO_LOWER_CASE)\n",
    "# for dev_e in dev_examples_cached:\n",
    "#     qa_id = dev_e.qa_id\n",
    "#     count = 0\n",
    "#     dev_f_list = []\n",
    "#     for dev_f in dev_features_cached:\n",
    "#         if dev_f.qa_id == qa_id:\n",
    "#             count += 1\n",
    "#             dev_f_list.append(dev_f)\n",
    "#     if count == 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina, [*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/abs/1810.04805), ACL, 2018.\n",
    "2. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, [*SQuAD: 100,000+ Questions for Machine Comprehension of Text*](https://arxiv.org/abs/1606.05250), EMNLP, 2016.\n",
    "3. Pranav Rajpurkar, Robin Jia, Percy Liang, [*Know What You Don't Know: Unanswerable Questions for SQuAD*](https://arxiv.org/abs/1806.03822), ACL, 2018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_cpu",
   "language": "python",
   "name": "nlp_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
