{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load and prep for MSR-PC\n",
    "Notebook which allows you to download [Microsoft Paraphrase](https://www.microsoft.com/en-us/download/details.aspx?id=52398) Corpus and then install it. We also provide utilities to load, clean and transform the data into a pandas dataframe. \n",
    "\n",
    "\n",
    "This dataset contains 5800 pairs of sentences which have been extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship. The total size of the dataset is 1.3MB. You can read more details about the dataset here: https://www.microsoft.com/en-us/download/details.aspx?id=52398"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils_nlp.dataset.msrpc import load_pandas_df\n",
    "from utils_nlp.dataset.preprocess import to_spacy_tokens\n",
    "from utils_nlp.dataset.url_utils import maybe_download, download_path\n",
    "\n",
    "INSTALLER_PATH = '../../data/'\n",
    "print(\"System version: {}\".format(sys.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to ../../data/MSRParaphraseCorpus.msi\n"
     ]
    }
   ],
   "source": [
    "url = \"https://download.microsoft.com/download/D/4/6/D46FF87A-F6B9-4252-AA8B\" \\\n",
    "          \"-3604ED519838/MSRParaphraseCorpus.msi\"\n",
    "data_path = maybe_download(url, work_directory=INSTALLER_PATH)\n",
    "print(\"Data downloaded to {}\".format(data_path)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Install dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Windows Installer for Mircosoft Paraphrase Corpus has been downloaded at  ../../data/MSRParaphraseCorpus.msi\n"
     ]
    }
   ],
   "source": [
    "print(\"The Windows Installer for Mircosoft Paraphrase Corpus has been downloaded at \", data_path)\n",
    "data_directory = input(\"Please install and provide the installed directory. Thanks! \\n\")\n",
    "if os.path.exists(data_directory):\n",
    "    print(\"Dataset successfully installed at \", data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Load data\n",
    "In this step we load and preview the data.\n",
    "\n",
    "The MSR Paraphrase Corpus comes with test and train dataset already split. A third dataset containing all the sentences is also provided. We load the train dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DICT = {\n",
    "    \"train\": \"msr_paraphrase_train.txt\",\n",
    "    \"test\": \"msr_paraphrase_test.txt\",\n",
    "    \"all\": \"msr_paraphrase_data.txt\",\n",
    "}\n",
    "\n",
    "file_path = os.path.join(data_directory, DATASET_DICT['train'])\n",
    "df = pd.read_csv(file_path, delimiter=\"\\t\", error_bad_lines=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Clean data\n",
    "From the cell above we can see that the data comes with a ID associated for each of the two sentences. The quality represents a binary similarity score between the two sentences. The IDs are unimportant to our use case. We drop those columns and rename existing #1 String, #2 String to sentence1 and sentence2 for clarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"#1 ID\", \"#2 ID\"])\\\n",
    ".dropna()\\\n",
    ".rename(index=str,columns={\"Quality\": \"score\",\"#1 String\": \"sentence1\",\"#2 String\": \"sentence2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 Preview the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 One shot dataset loading\n",
    "\n",
    "You can use our utils for downloading, installing, loading and cleaning MSR PC dataset to abstract away the dirty parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_pandas_df(local_cache_path = INSTALLER_PATH)\n",
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
