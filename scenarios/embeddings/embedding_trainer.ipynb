{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than use pre-trained embeddings (as we did in the sentence similarity baseline_deep_dive [notebook](../sentence_similarity/baseline_deep_dive.ipynb)), we can train word embeddings using our own dataset. In this notebook, we demonstrate the training process for producing word embeddings using the word2vec, GloVe, and fastText models. We'll utilize the STS Benchmark dataset for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Data Loading and Preprocessing](#Load-and-Preprocess-Data)\n",
    "* [Word2Vec](#Word2Vec)\n",
    "* [fastText](#fastText)\n",
    "* [GloVe](#GloVe)\n",
    "* [Concluding Remarks](#Concluding-Remarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the environment path\n",
    "NLP_PATH = os.path.abspath('../../')\n",
    "if NLP_PATH not in sys.path:\n",
    "    sys.path.insert(0, NLP_PATH)\n",
    "\n",
    "import numpy as np\n",
    "from utils_nlp.dataset.preprocess import (\n",
    "    to_lowercase,\n",
    "    to_spacy_tokens,\n",
    "    rm_spacy_stopwords,\n",
    ")\n",
    "from utils_nlp.dataset import stsbenchmark\n",
    "from utils_nlp.common.timer import Timer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for where your datasets are located\n",
    "BASE_DATA_PATH = os.path.join(NLP_PATH, \"data/\")\n",
    "# Location to save embeddings\n",
    "SAVE_FILES_PATH = os.path.join(BASE_DATA_PATH, \"trained_word_embeddings/\")\n",
    "if not os.path.exists(SAVE_FILES_PATH):\n",
    "    os.makedirs(SAVE_FILES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:01<00:00, 205KB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to /data/home/yijichen/notebooks/nlp_repo/nlp/data/raw/stsbenchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Produce a pandas dataframe for the training set\n",
    "train_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"train\")\n",
    "\n",
    "# Clean the sts dataset\n",
    "sts_train = stsbenchmark.clean_sts(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.00</td>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.60</td>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.25</td>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0   5.00                         A plane is taking off.   \n",
       "1   3.80                A man is playing a large flute.   \n",
       "2   3.80  A man is spreading shreded cheese on a pizza.   \n",
       "3   2.60                   Three men are playing chess.   \n",
       "4   4.25                    A man is playing the cello.   \n",
       "\n",
       "                                           sentence2  \n",
       "0                        An air plane is taking off.  \n",
       "1                          A man is playing a flute.  \n",
       "2  A man is spreading shredded cheese on an uncoo...  \n",
       "3                         Two men are playing chess.  \n",
       "4                 A man seated is playing the cello.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5749, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the size of our dataframe\n",
    "sts_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase\n",
    "df_low = to_lowercase(sts_train)  \n",
    "# Tokenize text\n",
    "sts_tokenize = to_spacy_tokens(df_low) \n",
    "# Tokenize with removal of stopwords\n",
    "sts_train_stop = rm_spacy_stopwords(sts_tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Append together the two sentence columns to get a list of all tokenized sentences.\n",
    "all_sentences =  sts_train_stop[[\"sentence1_tokens_rm_stopwords\", \"sentence2_tokens_rm_stopwords\"]]\n",
    "# Flatten two columns into one list and remove all sentences that are size 0 after tokenization and stop word removal.\n",
    "sentences = [i for i in all_sentences.values.flatten().tolist() if len(i) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11498"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum sentence length is 1 tokens\n",
      "Maximum sentence length is 43 tokens\n",
      "Median sentence length is 6.0 tokens\n"
     ]
    }
   ],
   "source": [
    "sentence_lengths = [len(i) for i in sentences]\n",
    "print(\"Minimum sentence length is {} tokens\".format(min(sentence_lengths)))\n",
    "print(\"Maximum sentence length is {} tokens\".format(max(sentence_lengths)))\n",
    "print(\"Median sentence length is {} tokens\".format(np.median(sentence_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['plane', 'taking', '.'],\n",
       " ['air', 'plane', 'taking', '.'],\n",
       " ['man', 'playing', 'large', 'flute', '.'],\n",
       " ['man', 'playing', 'flute', '.'],\n",
       " ['man', 'spreading', 'shreded', 'cheese', 'pizza', '.'],\n",
       " ['man', 'spreading', 'shredded', 'cheese', 'uncooked', 'pizza', '.'],\n",
       " ['men', 'playing', 'chess', '.'],\n",
       " ['men', 'playing', 'chess', '.'],\n",
       " ['man', 'playing', 'cello', '.'],\n",
       " ['man', 'seated', 'playing', 'cello', '.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a predictive model for learning word embeddings from text (see [original research paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)). Word embeddings are learned such that words that share common contexts in the corpus will be close together in the vector space. There are two different model architectures that can be used to produce word2vec embeddings: continuous bag-of-words (CBOW) or continuous skip-gram. The former uses a window of surrounding words (the \"context\") to predict the current word and the latter uses the current word to predict the surrounding context words. See this [tutorial](https://www.guru99.com/word-embedding-word2vec.html#3) on word2vec for more detailed background on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gensim Word2Vec model has many different parameters (see [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)) but the ones that are useful to know about are:  \n",
    "- size: length of the word embedding/vector (defaults to 100)\n",
    "- window: maximum distance between the word being predicted and the current word (defaults to 5)\n",
    "- min_count: ignores all words that have a frequency lower than this value (defaults to 5)\n",
    "- workers: number of worker threads used to train the model (defaults to 3)\n",
    "- sg: training algorithm; 1 for skip-gram and 0 for CBOW (defaults to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Timer to see how long the model takes to train\n",
    "t = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.start()\n",
    "\n",
    "# Train the Word2vec model\n",
    "word2vec_model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=3, sg=0)\n",
    "\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.6343\n"
     ]
    }
   ],
   "source": [
    "print(\"Time elapsed: {}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained we can:\n",
    "\n",
    "1. Query for the word embeddings of a given word. \n",
    "2. Inspect the model vocabulary\n",
    "3. Save the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for apple: [ 8.94748047e-02  2.41466127e-02  1.40242130e-01 -1.01290472e-01\n",
      " -3.72909606e-02 -8.42960998e-02  1.04651630e-01 -1.51196137e-01\n",
      " -9.76017863e-02  5.58857433e-02 -1.05379172e-01  1.96037576e-01\n",
      "  3.54142562e-02 -1.19922802e-01 -3.23171727e-02  1.97936416e-01\n",
      " -1.08599395e-01 -2.02625617e-02 -1.81590347e-03  1.17398715e-02\n",
      " -1.33693948e-01  1.20712914e-01 -7.43050948e-02  2.78903022e-02\n",
      "  2.78636813e-02  6.78229854e-02 -2.38574557e-02 -7.83610195e-02\n",
      "  8.14385060e-03 -8.23858902e-02 -1.06596418e-01  5.22979647e-02\n",
      "  1.03389891e-02  9.60147753e-02  6.47476837e-02  2.17621550e-01\n",
      " -8.09327960e-02  6.91598132e-02  6.26451075e-02 -1.32119164e-01\n",
      " -8.17936435e-02 -1.01129502e-01  3.28128450e-02  1.44652754e-01\n",
      "  4.50415276e-02  4.17685788e-03  2.75705159e-02 -1.73147812e-01\n",
      " -2.11286023e-02 -5.13567813e-02  1.62356552e-02  4.48382348e-02\n",
      " -4.29275855e-02 -6.81729009e-03 -5.25982417e-02  3.98872010e-02\n",
      "  1.32774189e-01  9.31772217e-02  8.80175233e-02 -1.74944147e-01\n",
      "  4.65075765e-03 -1.78152904e-01  1.31621733e-01  1.43591911e-01\n",
      "  3.28956544e-02 -3.90448957e-04  1.55428024e-02  1.78897038e-01\n",
      " -7.50094280e-02 -2.98591200e-02  7.28873536e-02 -5.43190092e-02\n",
      "  2.24501549e-04 -5.65024726e-02 -1.70839027e-01 -5.67071699e-02\n",
      "  3.65073010e-02 -1.34994999e-01 -2.21150830e-01  2.43736029e-01\n",
      " -1.14970841e-01  1.00075617e-01  3.27857062e-02  1.21146608e-02\n",
      " -5.34152389e-02  1.44616976e-01  1.58683032e-01 -5.93553893e-02\n",
      "  4.99471985e-02  5.39171882e-02 -5.47299534e-02  7.39113390e-02\n",
      " -2.75037112e-03 -1.26201406e-01 -1.31812394e-01  5.37611023e-02\n",
      " -4.52540629e-02 -3.37239467e-02  1.92407414e-01  8.33310559e-02]\n",
      "\n",
      "First 30 vocabulary words: ['plane', 'taking', '.', 'air', 'man', 'playing', 'large', 'flute', 'spreading', 'cheese', 'pizza', 'men', 'seated', 'fighting', 'smoking', 'piano', 'guitar', 'singing', 'woman', 'person']\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's see the word embedding for \"apple\" by accessing the \"wv\" attribute and passing in \"apple\" as the key.\n",
    "print(\"Embedding for apple:\", word2vec_model.wv[\"apple\"])\n",
    "\n",
    "# 2. Inspect the model vocabulary by accessing keys of the \"wv.vocab\" attribute. We'll print the first 20 words.\n",
    "print(\"\\nFirst 30 vocabulary words:\", list(word2vec_model.wv.vocab)[:20])\n",
    "\n",
    "# 3. Save the word embeddings. We can save as binary format (to save space) or ASCII format.\n",
    "word2vec_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"word2vec_model\", binary=True)  # binary format\n",
    "word2vec_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"word2vec_model\", binary=False)  # ASCII format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText is an unsupervised algorithm created by Facebook Research for efficiently learning word embeddings (see [original research paper](https://arxiv.org/pdf/1607.04606.pdf)). fastText is significantly different than word2vec or GloVe in that these two algorithms treat each word as the smallest possible unit to find an embedding for. Conversely, fastText assumes that words are formed by an n-gram of characters (i.e. 2-grams of the word \"language\" would be {la, an, ng, gu, ua, ag, ge}). The embedding for a word is then composed of the sum of these character n-grams. This has advantages when finding word embeddings for rare words and words not present in the dictionary, as these words can still be broken down into character n-grams. Typically, for smaller datasets, fastText performs better than word2vec or GloVe. See this [tutorial](https://fasttext.cc/docs/en/unsupervised-tutorial.html) on fastText for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gensim fastText model has many different parameters (see [here](https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText)) but the ones that are useful to know about are:  \n",
    "- size: length of the word embedding/vector (defaults to 100)\n",
    "- window: maximum distance between the word being predicted and the current word (defaults to 5)\n",
    "- min_count: ignores all words that have a frequency lower than this value (defaults to 5)\n",
    "- workers: number of worker threads used to train the model (defaults to 3)\n",
    "- sg: training algorithm- 1 for skip-gram and 0 for CBOW (defaults to 0)\n",
    "- iter: number of epochs (defaults to 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Timer to see how long the model takes to train\n",
    "t = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.start()\n",
    "\n",
    "# Train the FastText model\n",
    "fastText_model = FastText(size=100, window=5, min_count=5, sentences=sentences, iter=5)\n",
    "\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 11.1728\n"
     ]
    }
   ],
   "source": [
    "print(\"Time elapsed: {}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can utilize the same attributes as we saw above for word2vec due to them both originating from the gensim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for apple: [-0.23510154 -0.15507501  0.0056565   0.45181453  0.5000084  -0.2648049\n",
      " -0.25791287 -0.4212533  -0.06907137  0.0013695   0.3194571   0.01570429\n",
      " -0.03375538 -0.07636142  0.15745506  0.2511224   0.04350953  0.08955397\n",
      " -0.11049644 -0.5870106  -0.14050071 -0.03914891 -0.05926621 -0.48968792\n",
      " -0.15853383 -0.07664221  0.11611713  0.13797617 -0.43066472  0.2673129\n",
      "  0.06168905 -0.04650382 -0.01283566  0.09944137  0.14161733  0.15692197\n",
      "  0.0488883  -0.17440423 -0.42009622 -0.25779897  0.29067218  0.4241775\n",
      "  0.28518778 -0.17275187  0.10912739 -0.092472   -0.42640597  0.30356327\n",
      "  0.03260724  0.14312139 -0.09600725 -0.233319   -0.71152973 -0.4668092\n",
      " -0.15484177  0.083478    0.14034158 -0.32355824 -0.45780435  0.2399303\n",
      " -0.3201641  -0.34011903  0.09115782  0.25974855 -0.08718303  0.05970525\n",
      " -0.10188221  0.13411698  0.32321262 -0.1038212   0.32776234 -0.0280938\n",
      " -0.181011   -0.20158029  0.15832287  0.20536025  0.0343249  -0.2551625\n",
      "  0.00600469 -0.3237772   0.0900947  -0.09987869  0.16938613  0.05218367\n",
      " -0.14907101  0.3039924  -0.05155635 -0.1793785   0.105587    0.20242994\n",
      "  0.3455698   0.01762364  0.71458143  0.5323426  -0.43632066 -0.3977862\n",
      " -0.51023847 -0.17773995 -0.40293783  0.04541631]\n",
      "\n",
      "First 30 vocabulary words: ['plane', 'taking', '.', 'air', 'man', 'playing', 'large', 'flute', 'spreading', 'cheese', 'pizza', 'men', 'seated', 'fighting', 'smoking', 'piano', 'guitar', 'singing', 'woman', 'person']\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's see the word embedding for \"apple\" by accessing the \"wv\" attribute and passing in \"apple\" as the key.\n",
    "print(\"Embedding for apple:\", fastText_model.wv[\"apple\"])\n",
    "\n",
    "# 2. Inspect the model vocabulary by accessing keys of the \"wv.vocab\" attribute. We'll print the first 20 words.\n",
    "print(\"\\nFirst 30 vocabulary words:\", list(fastText_model.wv.vocab)[:20])\n",
    "\n",
    "# 3. Save the word embeddings. We can save as binary format (to save space) or ASCII format.\n",
    "fastText_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"fastText_model\", binary=True)  # binary format\n",
    "fastText_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"fastText_model\", binary=False)  # ASCII format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is an unsupervised algorithm for obtaining word embeddings created by the Stanford NLP group (see [original research paper](https://nlp.stanford.edu/pubs/glove.pdf)). Training occurs on word-word co-occurrence statistics with the objective of learning word embeddings such that the dot product of two words' embeddings is equal to the words' probability of co-occurrence. See this [tutorial](https://nlp.stanford.edu/projects/glove/) on GloVe for more detailed background on the model. \n",
    "\n",
    "Gensim doesn't have an implementation of the GloVe model and the other python packages that implement GloVe are unstable, so we leveraged the code directly from the Stanford NLP [repo](https://github.com/stanfordnlp/GloVe). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model_path = os.path.join(NLP_PATH, \"utils_nlp/models/glove/\")\n",
    "!cd $glove_model_path && make "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GloVe vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training GloVe embeddings requires some data prep and then 4 steps (also documented in the original Stanford NLP repo [here](https://github.com/stanfordnlp/GloVe/tree/master/src))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: Prepare Data**\n",
    "   \n",
    "In order to train our GloVe vectors, we first need to save our corpus as a text file with all words separated by 1+ spaces or tabs. Each document/sentence is separated by a new line character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our corpus as tokens delimited by spaces with new line characters in between sentences.\n",
    "training_corpus_file_path = os.path.join(SAVE_FILES_PATH, \"training-corpus-cleaned.txt\")\n",
    "with open(training_corpus_file_path, 'w', encoding='utf8') as file:\n",
    "    for sent in sentences:\n",
    "        file.write(\" \".join(sent) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Timer to see how long the model takes to train\n",
    "t = Timer()\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Build Vocabulary**\n",
    "\n",
    "Run the vocab_count executable. There are 3 optional parameters:\n",
    "1. min-count: lower limit on how many times a word must appear in dataset. Otherwise the word is discarded from our vocabulary.\n",
    "2. max-vocab: upper bound on the number of vocabulary words to keep\n",
    "3. verbose: 0, 1, or 2 (default)\n",
    "\n",
    "Then provide the path to the text file we created in Step 0 followed by a file path that we'll save the vocabulary to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_count_exe = os.path.join(glove_model_path, 'build/vocab_count')\n",
    "vocab_file_path = os.path.join(SAVE_FILES_PATH, \"vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING VOCABULARY\r\n",
      "Processed 0 tokens.\u001b[0GProcessed 85334 tokens.\r\n",
      "Counted 11716 unique words.\r\n",
      "Truncating vocabulary at min count 5.\r\n",
      "Using vocabulary of size 2943.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!$vocab_count_exe -min-count 5 -verbose 2 <$training_corpus_file_path> $vocab_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Construct Word Co-occurrence Statistics**\n",
    "\n",
    "Run the cooccur executable. There are many optional parameters, but we list the top ones here:\n",
    "1. symmetric: 0 for only looking at left context, 1 (default) for looking at both left and right context\n",
    "2. window-size: number of context words to use (default 15)\n",
    "3. verbose: 0, 1, or 2 (default)\n",
    "4. vocab-file: path/name of the vocabulary file created in Step 1\n",
    "5. memory: soft limit for memory consumption, default 4\n",
    "6. max-product: limit the size of dense co-occurrence array by specifying the max product (integer) of the frequency counts of the two co-occurring words\n",
    "\n",
    "Then provide the path to the text file we created in Step 0 followed by a file path that we'll save the co-occurrences to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccur_exe = os.path.join(glove_model_path, 'build/cooccur')\n",
    "cooccurrence_file_path = os.path.join(SAVE_FILES_PATH, \"cooccurrence.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"/data/home/yijichen/notebooks/nlp_repo/nlp/data/trained_word_embeddings/vocab.txt\"...loaded 2943 words.\n",
      "Building lookup table...table contains 8661250 elements.\n",
      "Processing token: 0\u001b[0GProcessed 85334 tokens.\n",
      "Writing cooccurrences to disk......2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G0 lines.\u001b[39G100000 lines.\u001b[0GMerging cooccurrence files: processed 188154 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!$cooccur_exe -memory 4 -vocab-file $vocab_file_path -verbose 2 -window-size 15 <$training_corpus_file_path> $cooccurrence_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Shuffle the Co-occurrences**\n",
    "\n",
    "Run the shuffle executable. The parameters are as follows:\n",
    "1. verbose: 0, 1, or 2 (default)\n",
    "2. memory: soft limit for memory consumption, default 4\n",
    "3. array-size: limit to the length of the buffer which stores chunks of data to shuffle before writing to disk\n",
    "\n",
    "Then provide the path to the co-occurrence file we created in Step 2 followed by a file path that we'll save the shuffled co-occurrences to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_exe = os.path.join(glove_model_path, 'build/shuffle')\n",
    "cooccurrence_shuf_file_path = os.path.join(SAVE_FILES_PATH, \"cooccurrence.shuf.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 188154 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G188154 lines.\u001b[0GMerging temp files: processed 188154 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!$shuffle_exe -memory 4 -verbose 2 <$cooccurrence_file_path> $cooccurrence_shuf_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Train GloVe model**\n",
    "\n",
    "Run the glove executable. There are many parameter options, but the top ones are listed below:\n",
    "1. verbose: 0, 1, or 2 (default)\n",
    "2. vector-size: dimension of word embeddings (50 is default)\n",
    "3. threads: number threads, default 8\n",
    "4. iter: number of iterations, default 25\n",
    "5. eta: learning rate, default 0.05\n",
    "6. binary: whether to save binary format (0: text = default, 1: binary, 2: both)\n",
    "7. x-max: cutoff for weighting function, default is 100\n",
    "8. vocab-file: file containing vocabulary as produced in Step 1\n",
    "9. save-file: filename to save vectors to \n",
    "10. input-file: filename with co-occurrences as returned from Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_exe = os.path.join(glove_model_path, 'build/glove')\n",
    "glove_vector_file_path = os.path.join(SAVE_FILES_PATH, \"GloVe_vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL\n",
      "Read 188154 lines.\n",
      "Initializing parameters...done.\n",
      "vector size: 50\n",
      "vocab size: 2943\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "08/01/19 - 08:43.48PM, iter: 001, cost: 0.078576\n",
      "08/01/19 - 08:43.48PM, iter: 002, cost: 0.072297\n",
      "08/01/19 - 08:43.48PM, iter: 003, cost: 0.070183\n",
      "08/01/19 - 08:43.48PM, iter: 004, cost: 0.066722\n",
      "08/01/19 - 08:43.49PM, iter: 005, cost: 0.063421\n",
      "08/01/19 - 08:43.49PM, iter: 006, cost: 0.060710\n",
      "08/01/19 - 08:43.49PM, iter: 007, cost: 0.058076\n",
      "08/01/19 - 08:43.49PM, iter: 008, cost: 0.056020\n",
      "08/01/19 - 08:43.49PM, iter: 009, cost: 0.053900\n",
      "08/01/19 - 08:43.49PM, iter: 010, cost: 0.051753\n",
      "08/01/19 - 08:43.49PM, iter: 011, cost: 0.049556\n",
      "08/01/19 - 08:43.49PM, iter: 012, cost: 0.047366\n",
      "08/01/19 - 08:43.49PM, iter: 013, cost: 0.045190\n",
      "08/01/19 - 08:43.49PM, iter: 014, cost: 0.043074\n",
      "08/01/19 - 08:43.49PM, iter: 015, cost: 0.041052\n"
     ]
    }
   ],
   "source": [
    "!$glove_exe -save-file $glove_vector_file_path -threads 8 -input-file \\\n",
    "$cooccurrence_shuf_file_path -x-max 10 -iter 15 -vector-size 50 -binary 2 \\\n",
    "-vocab-file $vocab_file_path -verbose 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 76.7739\n"
     ]
    }
   ],
   "source": [
    "print(\"Time elapsed: {}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did above for the word2vec and fastText models, let's now inspect our word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the saved word vectors.\n",
    "glove_wv = {}\n",
    "glove_vector_txt_file_path = os.path.join(SAVE_FILES_PATH, \"GloVe_vectors.txt\")\n",
    "with open(glove_vector_txt_file_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        split_line = line.split(\" \")\n",
    "        glove_wv[split_line[0]] = [float(i) for i in split_line[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for apple: [-0.031563, -0.002223, -0.02678, 0.032331, -0.052551, 0.033806, 0.027273, -0.040919, -0.032278, 0.144711, -0.056508, -0.006664, 0.18226, 0.087466, -0.072589, -0.003345, 0.058924, 0.054427, 0.012867, 0.013986, -0.083175, -0.02865, 0.044466, -0.095792, -0.042537, 0.019642, -0.032161, -0.038906, 0.253484, 0.090387, -0.03093, 0.081777, -0.085152, -0.113663, 0.10768, 0.068018, -0.05191, -0.177092, -0.06608, -0.223371, -0.016508, 0.232133, 0.002664, -0.132106, 0.078042, 0.111132, 0.052315, 0.010395, -0.031505, 0.128816]\n",
      "\n",
      "First 30 vocabulary words: ['.', ',', 'man', '-', '\"', 'woman', \"'\", 'said', 'dog', 'playing', ':', 'white', 'black', '$', 'killed', 'percent', 'new', 'syria', 'people', 'china']\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's see the word embedding for \"apple\" by passing in \"apple\" as the key.\n",
    "print(\"Embedding for apple:\", glove_wv[\"apple\"])\n",
    "\n",
    "# 2. Inspect the model vocabulary by accessing keys of the \"wv.vocab\" attribute. We'll print the first 20 words.\n",
    "print(\"\\nFirst 30 vocabulary words:\", list(glove_wv.keys())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding Remarks\n",
    "\n",
    "In this notebook we have shown how to train word2vec, GloVe, and fastText word embeddings on the STS Benchmark dataset. We also inspected how long each model took to train on our dataset: word2vec took 0.39 seconds, GloVe took 8.16 seconds, and fastText took 10.41 seconds.\n",
    "\n",
    "FastText is typically regarded as the best baseline for word embeddings (see [blog](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)) and is a good place to start when generating word embeddings. Now that we generated word embeddings on our dataset, we could also repeat the baseline_deep_dive notebook using these embeddings (versus the pre-trained ones from the internet). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_gpu)",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
