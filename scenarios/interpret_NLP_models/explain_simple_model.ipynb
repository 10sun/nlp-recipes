{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand your NLP models\n",
    "\n",
    "This is a torturial on how to utilize Interpreter class to explain certain hidden layers in your NLP models. We provide the explanation by measuring the information of input words ${\\bf x}_1,...,{\\bf x}_n$ that is encoded in hidden state ${\\bf s} = \\Phi({\\bf x})$. The method is from paper [*Towards a Deep and Unified Understanding of Deep Neural Models in NLP*](https://www.microsoft.com/en-us/research/publication/towards-a-deep-and-unified-understanding-of-deep-neural-models-in-nlp/) that is accepted by **ICML 2019**. In this torturial, we provide a simple example for you to start quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "\n",
    "# Set the environment path\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n",
    "\n",
    "# Suppose your input is x, and the sentence is simply \"1 2 3 4 5\"\n",
    "x = torch.randn(5, 256) / 100\n",
    "x = x.to(device)\n",
    "words = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "\n",
    "# Suppose your hidden state s = Phi(x), where\n",
    "# Phi = 10 * word[0] + 20 * word[1] + 5 * word[2] - 20 * word[3] - 10 * word[4]\n",
    "def Phi(x):\n",
    "    W = torch.tensor([10.0, 20.0, 5.0, -20.0, -10.0]).to(device)\n",
    "    return W @ x\n",
    "\n",
    "\n",
    "# Suppose this is your dataset used for training your models\n",
    "dataset = [torch.randn(5, 256) / 100 for _ in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Interpreter instance\n",
    "\n",
    "To explain a certain $\\bf x$ and certain $\\Phi$, you need to create an Interpreter instance, and pass your $\\bf x$, $\\Phi$ and regularization term (which is the standard variance of the hidden state r.v. $\\bf S$) to it. We also provide a simple function to calculate the regularization term that is needed in this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interpreter()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_nlp.interpreter.Interpreter import calculate_regularization, Interpreter\n",
    "\n",
    "# calculate the regularization term.\n",
    "regularization = calculate_regularization(dataset, Phi, device=device)\n",
    "\n",
    "# create the interpreter instance\n",
    "# we recommend you to set hyper-parameter *scale* to 10 * Std[word_embedding_weight], 10 * 0.1 in this example\n",
    "interpreter = Interpreter(\n",
    "    x=x, Phi=Phi, regularization=regularization, scale=10 * 0.1, words=words\n",
    ")\n",
    "interpreter.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Interpreter\n",
    "\n",
    "Then, we need to train our interpreter to let it find the information loss in each input word ${\\bf x}_i$ when they reach hidden state $\\bf s$. You can control the iteration and learning rate when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:05<00:00, 914.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train the interpreter by optimizing the loss\n",
    "interpreter.optimize(iteration=5000, lr=0.5, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show and visualize results\n",
    "\n",
    "After training, we can show the sigma (directly speaking, it is the range that every word can change without changing $\\bf s$ too much) we have got. Sigma somewhat stands for the information loss of word ${\\bf x}_i$ when it reaches $\\bf s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00321503, 0.00201195, 0.00633832, 0.00193789, 0.0031703 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the sigma we get\n",
    "interpreter.get_sigma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAABzCAYAAADNPJaYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABQFJREFUeJzt2z/InXcZxvHr1qZYkkpLDTYYsZM6dLASugQEpYh/ijg4OOgkdtCh4iDo5ugiuhbrIIou0UVRFKyUilqTNq1/UrcKohKlBn1BhOrtkKOTkMRj87uf4+cDL+954Rkufsv3nOc5b3V3AGCal60eAAD/iUABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATDSLTdy8bETd/Std939Um35v/DXPx2tnrBp973+5OoJm/f0ry6vnrB5t915++oJm/a3F36fF4+u1LWuu6FA3XrX3bn3U4/+96vIs+ceXz1h03747Y+snrB5dz7w+dUTNu+N73vr6gmb9txnPnxd17nFB8BIAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASAIFwEgCBcBIAgXASNcMVFU9VFXnq+r8i0dXbsYmALh2oLr7ke4+091nbjlxx83YBABu8QEwk0ABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATCSQAEwkkABMJJAATBSdff1X1z1hyS/funm7O1VSf64esTGOcP9OL/9OcP9TT/D13X3yWtddEOBmq6qznf3mdU7tswZ7sf57c8Z7u9QztAtPgBGEigARjq0QD2yesABcIb7cX77c4b7O4gzPKhnUAAcjkP7BAXAgTiIQFXVF6vqclX9fPWWLaqq11bVY1V1qap+UVUPr960NVX1iqp6sqqe2Z3hp1dv2qKqenlVPV1V31y9ZYuq6vmq+llVXayq86v37OsgbvFV1VuSHCX5Unffu3rP1lTVqSSnuvupqro9yYUk7+3uXy6ethlVVUmOd/dRVR1L8kSSh7v7x4unbUpVfTzJmSSv7O4HV+/Zmqp6PsmZ7p78P1DX7SA+QXX340leWL1jq7r7d9391O71X5JcSvKatau2pa862v15bPez/Xd/N1FVnU7y7iRfWL2FGQ4iUPzvVNU9Se5L8pO1S7Znd3vqYpLLSb7X3c7wxnwuySeS/GP1kA3rJN+tqgtV9dDqMfsSKP6tqk4kOZfkY93959V7tqa7/97db0pyOsn9VeV283WqqgeTXO7uC6u3bNzZ7n5zkncm+eju8cdmCRRJkt1zk3NJvtLdX1+9Z8u6+0qSHyR5x+IpW3I2yXt2z1C+luRtVfXltZO2p7t/u/t9Ock3kty/dtF+BIp/PeB/NMml7v7s6j1bVFUnq+qO3evbkjyQ5Lm1q7ajuz/Z3ae7+54k70/y/e7+wOJZm1JVx3dfckpVHU/y9iSb/mbzQQSqqr6a5EdJ3lBVv6mqD63etDFnk3wwV9+1Xtz9vGv1qI05leSxqno2yU9z9RmUr0pzM706yRNV9UySJ5N8q7u/s3jTXg7ia+YAHJ6D+AQFwOERKABGEigARhIoAEYSKABGEigARhIoAEYSKABG+icn0ALR1i/TJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the information loss of our sigma\n",
    "interpreter.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the second and forth words are important to ${\\bf s} = \\Phi({\\bf x})$, which is reasonable because the weights of them are larger."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
