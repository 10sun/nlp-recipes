{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "75caf421-c00a-4d6d-8a3d-47ebe7493af5"
    }
   },
   "source": [
    "\n",
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0738bb22-14af-45ca-9ad7-e0c068f280cf"
    }
   },
   "source": [
    "# GenSen with Pytorch\n",
    "In this tutorial, you will train a GenSen model for the sentence similarity task. We use the [SNLI](https://nlp.stanford.edu/projects/snli/) dataset in this example. For a more detailed walkthrough about data processing jump to [SNLI Data Prep](../01-prep-data/snli.ipynb). A quickstart version of this notebook can be found [here](../00-quick-start/)\n",
    "\n",
    "## Overview\n",
    "\n",
    "### What is GenSen?\n",
    "\n",
    "GenSen[\\[1\\]](#References) is a technique to learn general purpose, fixed-length representations of sentences via multi-task training. GenSen is to combine the benefits of These representations are useful for transfer and low-resource learning. GenSen is trained on several data sources with multiple training objectives on over 100 milion sentences.\n",
    "\n",
    "### Why GenSen?\n",
    "\n",
    "GenSen model performs the state-of-the-art results on multiple datasets, such as MRPC, SICK-R, SICK-E and STS, for sentence similarity. The reported results are as follows compared with other models [\\[2\\]](#References):\n",
    "\n",
    "| Model | MRPC | SICK-R | SICK-E | STS |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| GenSen (Subramanian et al., 2018) | 78.6/84.4 | 0.888 | 87.8 | 78.9/78.6 |\n",
    "| [InferSent](https://arxiv.org/abs/1705.02364) (Conneau et al., 2017) | 76.2/83.1 | 0.884 | 86.3 | 75.8/75.5 |\n",
    "| [TF-KLD](https://www.aclweb.org/anthology/D13-1090) (Ji and Eisenstein, 2013) | 80.4/85.9 | - | - | - |\n",
    "\n",
    "## Outline\n",
    "This notebook is organized as follows:\n",
    "\n",
    "1. GenSen Theory\n",
    "2. Data preparation and inspection\n",
    "3. Model application, performance and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e91468d4-7bb8-469b-95a6-4e6f4dfcdf55"
    }
   },
   "source": [
    "## 0. Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "a6e277ee-edbb-44a5-81d4-93565d2f3a83"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import os\n",
    "from utils_nlp.dataset.preprocess import to_lowercase, to_nltk_tokens\n",
    "from utils_nlp.dataset import snli\n",
    "from utils_nlp.model.gensen_wrapper import GenSenClassifier\n",
    "from utils_nlp.pretrained_embeddings.glove import download_and_extract \n",
    "\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "BASE_DATA_PATH = '../../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "aee768e5-f317-4dfb-807c-cb4f5f0c0204"
    }
   },
   "source": [
    "## 3. Data Preparation and inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4c570c1b-0e4e-41e9-aa27-5ab1ce8c13a1"
    }
   },
   "source": [
    "The [SNLI](https://nlp.stanford.edu/projects/snli/) corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "99c241e1-2f23-4fb3-9d3c-8f479c6b0030"
    }
   },
   "source": [
    "### 3.1 Load the dataset\n",
    "\n",
    "We provide a function load_pandas_df which does the following\n",
    "\n",
    "* Downloads the SNLI zipfile at the specified directory location\n",
    "* Extracts the file based on the specified split\n",
    "* Loads the split as a pandas dataframe The zipfile contains the following files:\n",
    "    * snli_1.0_dev.txt\n",
    "    * snli_1.0_train.txt\n",
    "    * snli_1.0_test.tx\n",
    "    * snli_1.0_dev.jsonl\n",
    "    * snli_1.0_train.jsonl\n",
    "    * snli_1.0_test.jsonl\n",
    "    \n",
    "The loader defaults to reading from the .txt file; however, you can change this to .jsonl by setting the optional file_type parameter when calling the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "5952e06d-1dae-462d-8fce-66eb7ef536dd"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>captionID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>3416050480.jpg#4</td>\n",
       "      <td>3416050480.jpg#4r1n</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>3416050480.jpg#4</td>\n",
       "      <td>3416050480.jpg#4r1c</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entailment</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>3416050480.jpg#4</td>\n",
       "      <td>3416050480.jpg#4r1e</td>\n",
       "      <td>entailment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
       "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
       "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
       "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "      <td>2267923837.jpg#2</td>\n",
       "      <td>2267923837.jpg#2r1n</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entailment</td>\n",
       "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
       "      <td>( There ( ( are children ) present ) )</td>\n",
       "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
       "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>There are children present</td>\n",
       "      <td>2267923837.jpg#2</td>\n",
       "      <td>2267923837.jpg#2r1e</td>\n",
       "      <td>entailment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gold_label                             sentence1_binary_parse  \\\n",
       "0        neutral  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "1  contradiction  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "2     entailment  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "3        neutral  ( Children ( ( ( smiling and ) waving ) ( at c...   \n",
       "4     entailment  ( Children ( ( ( smiling and ) waving ) ( at c...   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( ( A person ) ( ( is ( ( training ( his horse...   \n",
       "1  ( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...   \n",
       "2  ( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...   \n",
       "3  ( They ( are ( smiling ( at ( their parents ) ...   \n",
       "4             ( There ( ( are children ) present ) )   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
       "1  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
       "2  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
       "3  (ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...   \n",
       "4  (ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...   \n",
       "\n",
       "                                     sentence2_parse  \\\n",
       "0  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...   \n",
       "1  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...   \n",
       "2  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...   \n",
       "3  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...   \n",
       "4  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person on a horse jumps over a broken down a...   \n",
       "2  A person on a horse jumps over a broken down a...   \n",
       "3              Children smiling and waving at camera   \n",
       "4              Children smiling and waving at camera   \n",
       "\n",
       "                                           sentence2         captionID  \\\n",
       "0  A person is training his horse for a competition.  3416050480.jpg#4   \n",
       "1      A person is at a diner, ordering an omelette.  3416050480.jpg#4   \n",
       "2                  A person is outdoors, on a horse.  3416050480.jpg#4   \n",
       "3                  They are smiling at their parents  2267923837.jpg#2   \n",
       "4                         There are children present  2267923837.jpg#2   \n",
       "\n",
       "                pairID         label1 label2 label3 label4 label5  \n",
       "0  3416050480.jpg#4r1n        neutral    NaN    NaN    NaN    NaN  \n",
       "1  3416050480.jpg#4r1c  contradiction    NaN    NaN    NaN    NaN  \n",
       "2  3416050480.jpg#4r1e     entailment    NaN    NaN    NaN    NaN  \n",
       "3  2267923837.jpg#2r1n        neutral    NaN    NaN    NaN    NaN  \n",
       "4  2267923837.jpg#2r1e     entailment    NaN    NaN    NaN    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = snli.load_pandas_df(BASE_DATA_PATH, file_split=\"train\")\n",
    "dev = snli.load_pandas_df(BASE_DATA_PATH, file_split=\"dev\")\n",
    "test = snli.load_pandas_df(BASE_DATA_PATH, file_split=\"test\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6d5f7565-1f84-4489-8d06-dabd6bd99190"
    }
   },
   "source": [
    "### 3.2 Tokenize\n",
    "\n",
    "We have loaded the dataset into pandas.DataFrame, we now convert sentences to tokens. We also clean the data before tokenizing. This includes dropping unneccessary columns and renaming the relevant columns as score, sentence_1, and sentence_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "e6160617-03f0-4809-9360-8b040dc4395f"
    }
   },
   "outputs": [],
   "source": [
    "def clean(df, file_split):\n",
    "    src_file_path = os.path.join(BASE_DATA_PATH, \"raw/snli_1.0/snli_1.0_{}.txt\".format(file_split))\n",
    "    if not os.path.exists(os.path.join(BASE_DATA_PATH, \"clean/snli_1.0\")):\n",
    "        os.makedirs(os.path.join(BASE_DATA_PATH, \"clean/snli_1.0\"))\n",
    "    dest_file_path = os.path.join(BASE_DATA_PATH, \"clean/snli_1.0/snli_1.0_{}.txt\".format(file_split))\n",
    "    clean_df = snli.clean_snli(src_file_path).dropna() # drop rows with any NaN vals\n",
    "    clean_df.to_csv(dest_file_path)\n",
    "    return clean_df\n",
    "\n",
    "train = clean(train, 'train')\n",
    "dev = clean(dev, 'dev')\n",
    "test = clean(test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0331f346-a896-4019-8844-38139382ff36"
    }
   },
   "source": [
    "Once we have the clean pandas dataframes, we do lowercase standardization and tokenization. We use the [NLTK] (https://www.nltk.org/) library for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "92c2e565-a968-42f5-aa77-7ff5f340a436"
    }
   },
   "outputs": [],
   "source": [
    "train_tok = to_nltk_tokens(to_lowercase(train))\n",
    "dev_tok = to_nltk_tokens(to_lowercase(dev))\n",
    "test_tok = to_nltk_tokens(to_lowercase(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "4912b609-8141-4212-a6ad-814d73f724ed"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence1_tokens</th>\n",
       "      <th>sentence2_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>two women are embracing while holding to go pa...</td>\n",
       "      <td>the sisters are hugging goodbye while holding ...</td>\n",
       "      <td>[two, women, are, embracing, while, holding, t...</td>\n",
       "      <td>[the, sisters, are, hugging, goodbye, while, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entailment</td>\n",
       "      <td>two women are embracing while holding to go pa...</td>\n",
       "      <td>two woman are holding packages.</td>\n",
       "      <td>[two, women, are, embracing, while, holding, t...</td>\n",
       "      <td>[two, woman, are, holding, packages, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>two women are embracing while holding to go pa...</td>\n",
       "      <td>the men are fighting outside a deli.</td>\n",
       "      <td>[two, women, are, embracing, while, holding, t...</td>\n",
       "      <td>[the, men, are, fighting, outside, a, deli, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entailment</td>\n",
       "      <td>two young children in blue jerseys, one with t...</td>\n",
       "      <td>two kids in numbered jerseys wash their hands.</td>\n",
       "      <td>[two, young, children, in, blue, jerseys, ,, o...</td>\n",
       "      <td>[two, kids, in, numbered, jerseys, wash, their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>two young children in blue jerseys, one with t...</td>\n",
       "      <td>two kids at a ballgame wash their hands.</td>\n",
       "      <td>[two, young, children, in, blue, jerseys, ,, o...</td>\n",
       "      <td>[two, kids, at, a, ballgame, wash, their, hand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           score                                          sentence1  \\\n",
       "0        neutral  two women are embracing while holding to go pa...   \n",
       "1     entailment  two women are embracing while holding to go pa...   \n",
       "2  contradiction  two women are embracing while holding to go pa...   \n",
       "3     entailment  two young children in blue jerseys, one with t...   \n",
       "4        neutral  two young children in blue jerseys, one with t...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  the sisters are hugging goodbye while holding ...   \n",
       "1                    two woman are holding packages.   \n",
       "2               the men are fighting outside a deli.   \n",
       "3     two kids in numbered jerseys wash their hands.   \n",
       "4           two kids at a ballgame wash their hands.   \n",
       "\n",
       "                                    sentence1_tokens  \\\n",
       "0  [two, women, are, embracing, while, holding, t...   \n",
       "1  [two, women, are, embracing, while, holding, t...   \n",
       "2  [two, women, are, embracing, while, holding, t...   \n",
       "3  [two, young, children, in, blue, jerseys, ,, o...   \n",
       "4  [two, young, children, in, blue, jerseys, ,, o...   \n",
       "\n",
       "                                    sentence2_tokens  \n",
       "0  [the, sisters, are, hugging, goodbye, while, h...  \n",
       "1            [two, woman, are, holding, packages, .]  \n",
       "2     [the, men, are, fighting, outside, a, deli, .]  \n",
       "3  [two, kids, in, numbered, jerseys, wash, their...  \n",
       "4  [two, kids, at, a, ballgame, wash, their, hand...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_tok.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "59494d88-c7c9-4efc-a191-f16d6ac2ac40"
    }
   },
   "source": [
    "##  4. Model application, performance and analysis of the results\n",
    "The model has been implemented as a GenSen class with the specifics hidden inside the fit() method, so that no explicit call is needed. The algorithm operates in three different steps:\n",
    "\n",
    "** Model initialization ** : This is where we tell our class how to train the model. The main parameters to specify are the number of\n",
    "1. config file which contains information about the number of training epochs, the minibatch size etc.\n",
    "2. cache_dir which is the folder where all the data will be saved.\n",
    "3. learning rate for the model\n",
    "4. path to the pretrained embedding vectors.\n",
    "\n",
    "** Model fit ** : This is where we train the model on the data. The method takes two arguments: the training, dev and test set pandas dataframes. Note that the model is trained only on the training set, the test set is used to display the test set accuracy of the trained model, that in turn is an estimation of the generazation capabilities of the algorithm. It is generally useful to look at these quantities to have a first idea of the optimization behaviour.\n",
    "\n",
    "** Model prediction ** : This is where we generate the similarity for a pair of sentences. Once the model has been trained and we are satisfied with its overall accuracy we use the saved model to show the similarity between two provided sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Download pretrained vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector file already exists. No changes made.\n"
     ]
    }
   ],
   "source": [
    "pretrained_embedding_path = download_and_extract(BASE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ab565124-43de-4862-b286-2b5db3a868fe"
    }
   },
   "source": [
    "### 4.1 Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "641a9c74-974c-4aac-8c16-3b44d686f0f3"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%aimport utils_nlp.model.gensen\n",
    "\n",
    "config_filepath = '../../utils_nlp/model/gensen/sample_config.json'\n",
    "clf = GenSenClassifier(config_file = config_filepath, \n",
    "                       pretrained_embedding_path = pretrained_embedding_path,\n",
    "                       learning_rate = 0.0001, \n",
    "                       cache_dir=BASE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5f87d13c-d04f-4d38-820e-fb82082153c4"
    }
   },
   "source": [
    "### 4.2 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "6ea45671-c7a5-4fe8-a450-8b54161f26c5"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/nlp_gpu/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "../../utils_nlp/model/gensen/train.py:419: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  if (\n",
      "../../utils_nlp/model/gensen/utils.py:362: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  else:\n",
      "/data/anaconda/envs/nlp_gpu/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/data/anaconda/envs/nlp_gpu/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "1.096602201461792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../utils_nlp/model/gensen/train.py:510: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  os.chdir(owd)\n",
      "/data/anaconda/envs/nlp_gpu/lib/python3.6/site-packages/horovod/torch/__init__.py:163: UserWarning: optimizer.step(synchronize=True) called after optimizer.synchronize(). This can cause training slowdown. You may want to consider using optimizer.step(synchronize=False) if you use optimizer.synchronize() in your code.\n",
      "  warnings.warn(\"optimizer.step(synchronize=True) called after \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "1.1013386249542236\n",
      "Attempted to log scalar metric Task Loss:\n",
      "10.186532232496473\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.0989704132080078\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "4.695982434532859\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "9.941554069519043\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "9.941554069519043\n",
      "1 1 9.941554069519043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../utils_nlp/model/gensen/train.py:239: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Dev Acc : 0.32869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../utils_nlp/model/gensen/train.py:258: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Test Acc : 0.32767\n",
      "Attempted to log scalar metric loss:\n",
      "1.083627462387085\n",
      "Attempted to log scalar metric Task Loss:\n",
      "8.792654673258463\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.083627462387085\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "89.03370492458343\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "7.597753524780273\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "7.597753524780273\n",
      "2 2 7.597753524780273\n",
      "NLI Dev Acc : 0.32869\n",
      "NLI Test Acc : 0.32767\n",
      "Attempted to log scalar metric loss:\n",
      "1.1974765062332153\n",
      "Attempted to log scalar metric Task Loss:\n",
      "7.339768939548069\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1974765062332153\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "169.33406002521514\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "6.563234806060791\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "6.563234806060791\n",
      "3 3 6.563234806060791\n",
      "NLI Dev Acc : 0.34089\n",
      "NLI Test Acc : 0.34161\n",
      "Attempted to log scalar metric loss:\n",
      "1.1436152458190918\n",
      "Attempted to log scalar metric Task Loss:\n",
      "6.139929718441433\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1436152458190918\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "250.81561884880065\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.94525671005249\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.94525671005249\n",
      "4 4 5.94525671005249\n",
      "NLI Dev Acc : 0.33306\n",
      "NLI Test Acc : 0.32950\n",
      "Attempted to log scalar metric loss:\n",
      "1.1060961484909058\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.703480402628581\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1060961484909058\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "331.5616678953171\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.840088367462158\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.840088367462158\n",
      "5 5 5.840088367462158\n",
      "NLI Dev Acc : 0.33824\n",
      "NLI Test Acc : 0.34283\n",
      "Attempted to log scalar metric loss:\n",
      "1.1157835721969604\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.624355951944987\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1157835721969604\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "412.81678292751315\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.843489646911621\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.843489646911621\n",
      "6 5 5.840088367462158\n",
      "NLI Dev Acc : 0.34119\n",
      "NLI Test Acc : 0.34670\n",
      "Attempted to log scalar metric loss:\n",
      "1.095406413078308\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.458203739590115\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.095406413078308\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "493.76872198581697\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.691181659698486\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.691181659698486\n",
      "7 7 5.691181659698486\n",
      "NLI Dev Acc : 0.35084\n",
      "NLI Test Acc : 0.34701\n",
      "Attempted to log scalar metric loss:\n",
      "1.1702932119369507\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.33010991414388\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1702932119369507\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "574.0262914419175\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.664204120635986\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.664204120635986\n",
      "8 8 5.664204120635986\n",
      "NLI Dev Acc : 0.35379\n",
      "NLI Test Acc : 0.34629\n",
      "Attempted to log scalar metric loss:\n",
      "1.2024047374725342\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.276944531334771\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.2024047374725342\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "655.4772670507431\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.652242660522461\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.652242660522461\n",
      "9 9 5.652242660522461\n",
      "NLI Dev Acc : 0.33824\n",
      "NLI Test Acc : 0.34283\n",
      "Attempted to log scalar metric loss:\n",
      "1.1197906732559204\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.489440441131592\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1197906732559204\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "736.8923524141312\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.589874267578125\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.589874267578125\n",
      "10 10 5.589874267578125\n",
      "NLI Dev Acc : 0.37878\n",
      "NLI Test Acc : 0.38182\n",
      "Attempted to log scalar metric loss:\n",
      "1.1123993396759033\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.306429280175103\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1123993396759033\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "817.8013746738434\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.5273284912109375\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.5273284912109375\n",
      "11 11 5.5273284912109375\n",
      "NLI Dev Acc : 0.33306\n",
      "NLI Test Acc : 0.32940\n",
      "Attempted to log scalar metric loss:\n",
      "1.0702154636383057\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.188673708173964\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.0702154636383057\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "898.4967415571213\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.454174995422363\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.454174995422363\n",
      "12 12 5.454174995422363\n",
      "NLI Dev Acc : 0.33306\n",
      "NLI Test Acc : 0.32950\n",
      "Attempted to log scalar metric loss:\n",
      "1.133014440536499\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.084137174818251\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.133014440536499\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "979.2467672109603\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.484553813934326\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.484553813934326\n",
      "13 12 5.454174995422363\n",
      "NLI Dev Acc : 0.33926\n",
      "NLI Test Acc : 0.34385\n",
      "Attempted to log scalar metric loss:\n",
      "1.1045844554901123\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.149944305419922\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1045844554901123\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1060.17123067379\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.290821552276611\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.290821552276611\n",
      "14 14 5.290821552276611\n",
      "NLI Dev Acc : 0.33824\n",
      "NLI Test Acc : 0.34273\n",
      "Attempted to log scalar metric loss:\n",
      "1.1022413969039917\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.144280327690972\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1022413969039917\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1140.8248002052308\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.343923091888428\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.343923091888428\n",
      "15 14 5.290821552276611\n",
      "NLI Dev Acc : 0.36060\n",
      "NLI Test Acc : 0.35535\n",
      "Attempted to log scalar metric loss:\n",
      "1.0922162532806396\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.9358002874586315\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.0922162532806396\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1221.7035522460938\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.248899459838867\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.248899459838867\n",
      "16 16 5.248899459838867\n",
      "NLI Dev Acc : 0.37614\n",
      "NLI Test Acc : 0.37144\n",
      "Attempted to log scalar metric loss:\n",
      "1.1002861261367798\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.05864953994751\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1002861261367798\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1302.4916563034058\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.312340259552002\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.312340259552002\n",
      "17 16 5.248899459838867\n",
      "NLI Dev Acc : 0.38945\n",
      "NLI Test Acc : 0.39404\n",
      "Attempted to log scalar metric loss:\n",
      "1.1102365255355835\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.979959487915039\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1102365255355835\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1384.1857381105424\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.211462497711182\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.211462497711182\n",
      "18 18 5.211462497711182\n",
      "NLI Dev Acc : 0.33784\n",
      "NLI Test Acc : 0.33876\n",
      "Attempted to log scalar metric loss:\n",
      "1.1019434928894043\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.77244774500529\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1019434928894043\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1464.775280213356\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.194383144378662\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.194383144378662\n",
      "19 19 5.194383144378662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Dev Acc : 0.35409\n",
      "NLI Test Acc : 0.35230\n",
      "Attempted to log scalar metric loss:\n",
      "1.1197246313095093\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.68223958545261\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1197246313095093\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1545.6291225194932\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.1940202713012695\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.1940202713012695\n",
      "20 20 5.1940202713012695\n",
      "NLI Dev Acc : 0.33306\n",
      "NLI Test Acc : 0.32950\n",
      "Attempted to log scalar metric loss:\n",
      "1.1138871908187866\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.983356793721517\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1138871908187866\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1626.0303706884383\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.2733330726623535\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.2733330726623535\n",
      "21 20 5.1940202713012695\n",
      "NLI Dev Acc : 0.33743\n",
      "NLI Test Acc : 0.33886\n",
      "Attempted to log scalar metric loss:\n",
      "1.1087180376052856\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.8265162044101295\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1087180376052856\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1706.9568982124329\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.20107364654541\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.20107364654541\n",
      "22 20 5.1940202713012695\n",
      "NLI Dev Acc : 0.33824\n",
      "NLI Test Acc : 0.34283\n",
      "Attempted to log scalar metric loss:\n",
      "1.1027153730392456\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.711381753285726\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1027153730392456\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1787.5634543895721\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "4.958130359649658\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "4.958130359649658\n",
      "23 23 4.958130359649658\n",
      "NLI Dev Acc : 0.34404\n",
      "NLI Test Acc : 0.35026\n",
      "Attempted to log scalar metric loss:\n",
      "1.086664080619812\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.744175328148736\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.086664080619812\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1867.9857582569123\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.089684963226318\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.089684963226318\n",
      "24 23 4.958130359649658\n",
      "NLI Dev Acc : 0.34221\n",
      "NLI Test Acc : 0.33876\n",
      "Attempted to log scalar metric loss:\n",
      "1.097867488861084\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.6074401007758246\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.097867488861084\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "1948.4608956813813\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "5.056739330291748\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "5.056739330291748\n",
      "25 23 4.958130359649658\n",
      "NLI Dev Acc : 0.33306\n",
      "NLI Test Acc : 0.32950\n",
      "Attempted to log scalar metric loss:\n",
      "1.0889590978622437\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.8467733065287275\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.0889590978622437\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "2029.4085982322692\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "4.943850994110107\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "4.943850994110107\n",
      "26 26 4.943850994110107\n",
      "NLI Dev Acc : 0.33428\n",
      "NLI Test Acc : 0.33062\n",
      "Attempted to log scalar metric loss:\n",
      "1.0951266288757324\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.425179375542535\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.0951266288757324\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "2109.7621256113052\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "4.816421985626221\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "4.816421985626221\n",
      "27 27 4.816421985626221\n",
      "NLI Dev Acc : 0.36050\n",
      "NLI Test Acc : 0.36309\n",
      "Attempted to log scalar metric loss:\n",
      "1.1193431615829468\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.596744802263048\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1193431615829468\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "2190.797572517395\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "4.945044994354248\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "4.945044994354248\n",
      "28 27 4.816421985626221\n",
      "NLI Dev Acc : 0.34505\n",
      "NLI Test Acc : 0.34222\n",
      "Attempted to log scalar metric loss:\n",
      "1.1108286380767822\n",
      "Attempted to log scalar metric Task Loss:\n",
      "5.009236971537272\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1108286380767822\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "2271.578313088417\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "4.912169933319092\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "4.912169933319092\n",
      "29 27 4.816421985626221\n",
      "NLI Dev Acc : 0.38214\n",
      "NLI Test Acc : 0.38742\n",
      "Attempted to log scalar metric loss:\n",
      "1.1069382429122925\n",
      "Attempted to log scalar metric Task Loss:\n",
      "4.764758692847358\n",
      "Attempted to log scalar metric NLI Loss:\n",
      "1.1069382429122925\n",
      "Attempted to log scalar metric Average time per mininbatch : :\n",
      "2352.0282317876818\n",
      "Attempted to log scalar metric Best Validation Loss:\n",
      "4.8790283203125\n",
      "Attempted to log scalar metric Validation Loss:\n",
      "4.8790283203125\n",
      "30 27 4.816421985626221\n",
      "CPU times: user 32min 59s, sys: 7min 47s, total: 40min 46s\n",
      "Wall time: 40min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(train_tok, dev_tok, test_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../utils_nlp/model/gensen/gensen.py:407: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  filename_prefix=\"nli_large_bothskip\",\n",
      "../../utils_nlp/model/gensen/gensen.py:408: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  pretrained_emb=\"./data/embedding/glove.840B.300d.h5\",\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.0000004 , 0.9827571 ],\n",
       "       [0.9827571 , 0.99999976]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "        'hello world . the quick brown foxy',\n",
    "        'the quick brown fox jumped over the lazy dog .'\n",
    "    ]\n",
    "\n",
    "clf.predict(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Subramanian, Sandeep and Trischler, Adam and Bengio, Yoshua and Pal, Christopher J, [*Learning general purpose distributed sentence representations via large scale multi-task learning*](https://arxiv.org/abs/1804.00079), ICLR, 2018.\n",
    "3. Semantic textual similarity. url: http://nlpprogress.com/english/semantic_textual_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_gpu]",
   "language": "python",
   "name": "conda-env-nlp_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
