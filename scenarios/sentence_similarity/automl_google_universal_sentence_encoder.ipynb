{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning#3-setup-a-new-conda-environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to extract features for a sentence similarity task using the pretrained models InferSent and Google Universal Sentence Encoder. Then we will demonstrate how the AutoML package can easily automate model selection and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0612 09:40:01.239300 40168 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning diagnostics collection on. \n",
      "System version: 3.6.7 |Anaconda, Inc.| (default, Dec 10 2018, 20:35:02) [MSC v.1915 64 bit (AMD64)]\n",
      "Azure ML SDK Version: 1.0.41\n",
      "Pandas version: 0.23.4\n",
      "Tensorflow Version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find NLP\n",
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial import distance\n",
    "from utils_nlp.azureml import azureml_utils\n",
    "\n",
    "#tensorflow dependencies for Google Universal Sentence Encoder\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "#AzureML packages\n",
    "import azureml as aml\n",
    "import logging\n",
    "from azureml.telemetry import set_diagnostics_collection\n",
    "set_diagnostics_collection(send_diagnostics=True)\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Azure ML SDK Version:\", aml.core.VERSION)\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Tensorflow Version:\", tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = '../../../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll collect the Google Sentence Encoder encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all this with our util!\n",
    "data = []\n",
    "with open(\"sts-train.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        data.append([l[5].strip().lower(),l[6].strip().lower(), float(l[4])])\n",
    "train = pd.DataFrame(data, columns=['sentence1','sentence2','score'])\n",
    "\n",
    "data = []\n",
    "with open(\"sts-test.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        data.append([l[5].strip().lower(),l[6].strip().lower(), float(l[4])])\n",
    "test = pd.DataFrame(data, columns=['sentence1','sentence2','score'])\n",
    "\n",
    "data = []\n",
    "with open(\"sts-dev.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        data.append([l[5].strip().lower(),l[6].strip().lower(), float(l[4])])\n",
    "dev = pd.DataFrame(data, columns=['sentence1','sentence2','score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "google_USE_embed = hub.Module(module_url)\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_google_universal_sentence_encoder(dataset, embedding_model):\n",
    "    sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "    sts_input2 = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "    sts_encode1 = tf.nn.l2_normalize(embedding_model(sts_input1), axis=1)\n",
    "    sts_encode2 = tf.nn.l2_normalize(embedding_model(sts_input2), axis=1)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        emb1, emb2 = session.run(\n",
    "          [sts_encode1, sts_encode2],\n",
    "          feed_dict={\n",
    "              sts_input1: dataset['sentence1'],\n",
    "              sts_input2: dataset['sentence2']\n",
    "          })\n",
    "    return emb1, emb2\n",
    "    \n",
    "def feature_engineering(dataset, googleUSE_embedding_model):\n",
    "    google_USE_emb1, google_USE_emb2 = embed_google_universal_sentence_encoder(dataset, googleUSE_embedding_model)\n",
    "    n_google = google_USE_emb1.shape[1]    \n",
    "    df = np.concatenate((google_USE_emb1, google_USE_emb2), axis=1)\n",
    "    names = ['USEEmb1_'+str(i) for i in range(n_google)]+['USEEmb2_'+str(i) for i in range(n_google)]\n",
    "    df = pd.DataFrame(df, columns=names)\n",
    "    df['score'] = dataset['score']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = feature_engineering(train, google_USE_embed)\n",
    "validation_data = feature_engineering(dev, google_USE_embed)\n",
    "testing_data = feature_engineering(test, google_USE_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out when notebook is ready for publishing\n",
    "training_data.to_csv(\"Data/training_set.csv\", index=None)\n",
    "testing_data.to_csv(\"Data/testing_set.csv\", index=None)\n",
    "validation_data.to_csv(\"Data/validation_set.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_performance(data):\n",
    "    sent1_googleUSE = data[[i for i in data.columns if 'USEEmb1' in i]].values.tolist()\n",
    "    sent2_googleUSE = data[[i for i in data.columns if 'USEEmb2' in i]].values.tolist()\n",
    "    \n",
    "    predictions_googleUSE = [1-distance.cosine(sent1_googleUSE[i], sent2_googleUSE[i]) for i in range(len(sent1_googleUSE))]\n",
    "    print(\"Google Universal Sentence Encoder Pearson Correlation:\",pearsonr(predictions_googleUSE, data['score'].values.tolist())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Universal Sentence Encoder Pearson Correlation: 0.7640280696312057\n"
     ]
    }
   ],
   "source": [
    "get_baseline_performance(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML - no AmlCompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0612 09:49:07.823286 40168 authentication.py:494] Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0612 09:49:08.846869 29376 _profile.py:1082] Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n",
      "W0612 09:49:20.330181 40168 _profile.py:774] You have logged in. Now let us find all the subscriptions to which you have access...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive authentication successfully completed.\n",
      "Workspace name: MAIDAPTest\n",
      "Azure region: eastus2\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: nlprg\n"
     ]
    }
   ],
   "source": [
    "ws = azureml_utils.get_or_create_workspace(\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    workspace_region=\"<WORKSPACE_REGION>\"\n",
    ")\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out when notebook is ready for publishing\n",
    "training_data = pd.read_csv(\"Data/training_set.csv\")\n",
    "testing_data = pd.read_csv(\"Data/testing_set.csv\")\n",
    "validation_data = pd.read_csv(\"Data/validation_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = training_data['score']\n",
    "train_x = training_data[[i for i in training_data.columns if 'USE' in i]]\n",
    "\n",
    "validation_y = validation_data['score']\n",
    "validation_x = validation_data[[i for i in validation_data.columns if 'USE' in i]]\n",
    "\n",
    "test_y = testing_data['score']\n",
    "test_x = testing_data[[i for i in testing_data.columns if 'USE' in i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5749, 1024)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\" : 15,\n",
    "    \"iterations\" : 50,\n",
    "    \"primary_metric\" : 'spearman_correlation',\n",
    "    \"preprocess\" : True,\n",
    "    \"verbosity\":logging.ERROR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local compute\n",
    "automated_ml_config = AutoMLConfig(task = 'regression',\n",
    "                     debug_log = 'automated_ml_errors.log',\n",
    "                     path = './automated-ml-regression',\n",
    "                     X = train_x.values,\n",
    "                     y = train_y.values.flatten(),\n",
    "                     X_valid = validation_x.values,\n",
    "                     y_valid = validation_y.values.flatten(),\n",
    "                     **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local machine\n",
      "Parent Run ID: AutoML_089672db-2a8f-4d74-84ce-b3bee49733dd\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper RandomForest             0:01:09       0.1834    0.1834\n",
      "         1   MinMaxScaler RandomForest                      0:01:59       0.4272    0.4272\n",
      "         2   StandardScalerWrapper ExtremeRandomTrees       0:00:19       0.2811    0.4272\n",
      "         3   StandardScalerWrapper LightGBM                 0:00:19       0.2845    0.4272\n",
      "         4   RobustScaler DecisionTree                      0:00:26       0.2544    0.4272\n",
      "         5   StandardScalerWrapper LassoLars                0:00:16       0.1246    0.4272\n",
      "         6   StandardScalerWrapper LightGBM                 0:00:20       0.6568    0.6568\n",
      "         7   StandardScalerWrapper RandomForest             0:00:20       0.2186    0.6568\n",
      "         8   StandardScalerWrapper LassoLars                0:00:21       0.0838    0.6568\n",
      "         9   MinMaxScaler ExtremeRandomTrees                0:00:21       0.3632    0.6568\n",
      "        10   RobustScaler ExtremeRandomTrees                0:01:01       0.3490    0.6568\n",
      "        11   StandardScalerWrapper ExtremeRandomTrees       0:01:07       0.2673    0.6568\n",
      "        12   MinMaxScaler ExtremeRandomTrees                0:00:22       0.2580    0.6568\n",
      "        13   RobustScaler RandomForest                      0:00:29       0.3360    0.6568\n",
      "        14   StandardScalerWrapper LassoLars                0:00:15          nan    0.6568\n",
      "        15   StandardScalerWrapper ExtremeRandomTrees       0:00:15       0.2102    0.6568\n",
      "        16   StandardScalerWrapper RandomForest             0:00:28       0.2170    0.6568\n",
      "        17   MinMaxScaler SGD                               0:00:15       0.0965    0.6568\n",
      "        18   StandardScalerWrapper RandomForest             0:00:40       0.3519    0.6568\n",
      "        19   MinMaxScaler RandomForest                      0:00:18       0.1664    0.6568\n",
      "        20   StandardScalerWrapper LightGBM                 0:00:38       0.7423    0.7423\n",
      "        21   StandardScalerWrapper XGBoostRegressor         0:04:14       0.6688    0.7423\n",
      "        22   StandardScalerWrapper DecisionTree             0:05:39       0.2257    0.7423\n",
      "        23   StandardScalerWrapper LightGBM                 0:01:47       0.6779    0.7423\n",
      "        24   StandardScalerWrapper XGBoostRegressor         0:06:50       0.7638    0.7638\n",
      "        25   TruncatedSVDWrapper XGBoostRegressor           0:01:09       0.7484    0.7638\n",
      "        26   StandardScalerWrapper XGBoostRegressor         0:11:05       0.6608    0.7638\n",
      "        27   StandardScalerWrapper RandomForest             0:05:09       0.4328    0.7638\n",
      "        28   MaxAbsScaler LightGBM                          0:00:36       0.6966    0.7638\n",
      "        29                                                  0:15:27          nan    0.7638\n",
      "ERROR: Fit operation exceeded provided timeout, terminating and moving onto the next iteration. Please consider increasing the iteration_timeout_minutes parameter.\n",
      "        30   TruncatedSVDWrapper XGBoostRegressor           0:01:32       0.5744    0.7638\n",
      "        31   StandardScalerWrapper LightGBM                 0:01:19       0.6043    0.7638\n",
      "        32   RobustScaler DecisionTree                      0:01:12       0.2604    0.7638\n",
      "        33   MaxAbsScaler LightGBM                          0:00:33       0.7516    0.7638\n",
      "        34   StandardScalerWrapper LightGBM                 0:00:31       0.6979    0.7638\n",
      "        35   TruncatedSVDWrapper XGBoostRegressor           0:00:34       0.6998    0.7638\n",
      "        36   MaxAbsScaler LightGBM                          0:00:38       0.7226    0.7638\n",
      "        37   MaxAbsScaler LightGBM                          0:01:03       0.7107    0.7638\n",
      "        38   StandardScalerWrapper XGBoostRegressor         0:01:08       0.6529    0.7638\n",
      "        39   StandardScalerWrapper XGBoostRegressor         0:02:09       0.6202    0.7638\n",
      "        40   TruncatedSVDWrapper XGBoostRegressor           0:00:58       0.7196    0.7638\n",
      "        41   MaxAbsScaler LightGBM                          0:00:44       0.6948    0.7638\n",
      "        42   MaxAbsScaler LightGBM                          0:05:25       0.6742    0.7638\n",
      "        43   StandardScalerWrapper LightGBM                 0:01:25       0.5840    0.7638\n",
      "        44   MaxAbsScaler LightGBM                          0:00:49       0.7087    0.7638\n",
      "        45   TruncatedSVDWrapper XGBoostRegressor           0:00:37       0.6802    0.7638\n",
      "        46   StandardScalerWrapper XGBoostRegressor         0:01:31       0.6529    0.7638\n",
      "        47   MaxAbsScaler LightGBM                          0:00:39       0.6008    0.7638\n",
      "        48   VotingEnsemble                                 0:01:40       0.8146    0.8146\n",
      "        49   StackEnsemble                                  0:09:34       0.8142    0.8146\n"
     ]
    }
   ],
   "source": [
    "experiment=Experiment(ws, 'automated-ml-regression')\n",
    "local_run = experiment.submit(automated_ml_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f441d10d3d42c9b950a7e3f2cbc7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_AutoMLWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 'sd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(local_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget_data = RunDetails(local_run).get_widget_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model AutoML089672db2best\n",
      "AutoML089672db2best\n"
     ]
    }
   ],
   "source": [
    "description = 'AutoML Sentence Similarity Model'\n",
    "tags = None\n",
    "model = local_run.register_model(description = description, tags = tags)\n",
    "\n",
    "print(local_run.model_id) # This will be written to the script file later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: automated-ml-regression,\n",
      "Id: AutoML_089672db-2a8f-4d74-84ce-b3bee49733dd_48,\n",
      "Type: None,\n",
      "Status: Completed)\n",
      "RegressionPipeline(pipeline=Pipeline(memory=None,\n",
      "     steps=[('datatransformer', DataTransformer(enable_feature_sweeping=None, feature_sweeping_timeout=None,\n",
      "        is_onnx_compatible=None, logger=None, observer=None, task=None)), ('prefittedsoftvotingregressor', PreFittedSoftVotingRegressor(estimators=[('24', Pipeline(memory=None,\n",
      "     steps=[('stand...333333333333, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667]))]),\n",
      "          stddev=None)\n"
     ]
    }
   ],
   "source": [
    "lookup_metric = \"spearman_correlation\"\n",
    "best_run, fitted_model = local_run.get_output(metric = lookup_metric)\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7817644762555442\n"
     ]
    }
   ],
   "source": [
    "y_pred = fitted_model.predict(test_x.values)\n",
    "print(pearsonr(y_pred, test_y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimators': [('24', Pipeline(memory=None,\n",
       "        steps=[('standardscalerwrapper', <automl.client.core.common.model_wrappers.StandardScalerWrapper object at 0x000002669728A1D0>), ('xgboostregressor', XGBoostRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "            colsample_bytree=1, eta=0.01, gamma=0, learning_rate=0.1,\n",
       "            ma...ale_pos_weight=1, seed=None,\n",
       "            silent=True, subsample=0.7, tree_method='auto', verbose=-10))])),\n",
       "  ('33', Pipeline(memory=None,\n",
       "        steps=[('maxabsscaler', MaxAbsScaler(copy=True)), ('lightgbmregressor', LightGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "            colsample_bytree=0.7000000000000001, importance_type='split',\n",
       "            learning_rate=0.16842263157894738, max_bin=7, max_depth=3,\n",
       "            min_child_samples=14,...ue, subsample=0.5499999999999999,\n",
       "            subsample_for_bin=200000, subsample_freq=3, verbose=-1))])),\n",
       "  ('25', Pipeline(memory=None,\n",
       "        steps=[('truncatedsvdwrapper', TruncatedSVDWrapper(n_components=0.2573684210526316, random_state=None)), ('xgboostregressor', XGBoostRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "            colsample_bytree=1, eta=0.01, gamma=0, learning_rate=0.1,\n",
       "            max_delta_step=0, max_dept...scale_pos_weight=1, seed=None,\n",
       "            silent=True, subsample=1, tree_method='auto', verbose=-10))])),\n",
       "  ('20', Pipeline(memory=None,\n",
       "        steps=[('standardscalerwrapper', <automl.client.core.common.model_wrappers.StandardScalerWrapper object at 0x00000266972A9710>), ('lightgbmregressor', LightGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1,\n",
       "            importance_type='split', learning_rate=0.2, max_bin=63,\n",
       "        ...425, silent=True, subsample=0.85,\n",
       "            subsample_for_bin=200000, subsample_freq=2, verbose=-1))])),\n",
       "  ('40', Pipeline(memory=None,\n",
       "        steps=[('truncatedsvdwrapper', TruncatedSVDWrapper(n_components=0.3563157894736842, random_state=None)), ('xgboostregressor', XGBoostRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "            colsample_bytree=0.9, eta=0.3, gamma=0, learning_rate=0.1,\n",
       "            max_delta_step=0, max_dep...ale_pos_weight=1, seed=None, silent=True,\n",
       "            subsample=0.9, tree_method='auto', verbose=-10))])),\n",
       "  ('37', Pipeline(memory=None,\n",
       "        steps=[('maxabsscaler', MaxAbsScaler(copy=True)), ('lightgbmregressor', LightGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "            colsample_bytree=0.9, importance_type='split',\n",
       "            learning_rate=0.11579368421052631, max_bin=63, max_depth=10,\n",
       "            min_child_samples=16, min_child_we...e, subsample=0.44999999999999996,\n",
       "            subsample_for_bin=200000, subsample_freq=3, verbose=-1))])),\n",
       "  ('44', Pipeline(memory=None,\n",
       "        steps=[('maxabsscaler', MaxAbsScaler(copy=True)), ('lightgbmregressor', LightGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "            colsample_bytree=0.7000000000000001, importance_type='split',\n",
       "            learning_rate=0.16842263157894738, max_bin=7, max_depth=9,\n",
       "            min_child_samples=60,...    subsample=0.7999999999999999, subsample_for_bin=200000,\n",
       "            subsample_freq=7, verbose=-1))])),\n",
       "  ('45', Pipeline(memory=None,\n",
       "        steps=[('truncatedsvdwrapper', TruncatedSVDWrapper(n_components=0.10894736842105263, random_state=None)), ('xgboostregressor', XGBoostRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "            colsample_bytree=0.7, eta=0.05, gamma=0.1, learning_rate=0.1,\n",
       "            max_delta_step=0, max...ale_pos_weight=1, seed=None,\n",
       "            silent=True, subsample=0.7, tree_method='auto', verbose=-10))]))],\n",
       " 'weights': [0.2,\n",
       "  0.2,\n",
       "  0.2,\n",
       "  0.13333333333333333,\n",
       "  0.06666666666666667,\n",
       "  0.06666666666666667,\n",
       "  0.06666666666666667,\n",
       "  0.06666666666666667],\n",
       " 'flatten_transform': None}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model.pipeline.steps[1][1].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence_similarity_regressor.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "model_path = 'sentence_similarity_regressor.pkl'\n",
    "\n",
    "joblib.dump(fitted_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = joblib.load('sentence_similarity_regressor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionPipeline(pipeline=Pipeline(memory=None,\n",
       "     steps=[('datatransformer', DataTransformer(enable_feature_sweeping=None, feature_sweeping_timeout=None,\n",
       "        is_onnx_compatible=None, logger=None, observer=None, task=None)), ('prefittedsoftvotingregressor', PreFittedSoftVotingRegressor(estimators=[('24', Pipeline(memory=None,\n",
       "     steps=[('stand...333333333333, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667]))]),\n",
       "          stddev=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
