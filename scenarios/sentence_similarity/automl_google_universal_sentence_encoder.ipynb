{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/automated-machine-learning#3-setup-a-new-conda-environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to extract features for a sentence similarity task using the pretrained models InferSent and Google Universal Sentence Encoder. Then we will demonstrate how the AutoML package can easily automate model selection and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0612 14:28:37.904844 30000 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning diagnostics collection on. \n",
      "System version: 3.6.7 |Anaconda, Inc.| (default, Dec 10 2018, 20:35:02) [MSC v.1915 64 bit (AMD64)]\n",
      "Azure ML SDK Version: 1.0.41\n",
      "Pandas version: 0.23.4\n",
      "Tensorflow Version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find NLP\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# import utils\n",
    "from utils_nlp.azureml import azureml_utils\n",
    "from utils_nlp.dataset import stsbenchmark\n",
    "from utils_nlp.dataset.preprocess import (\n",
    "    to_lowercase,\n",
    "    to_spacy_tokens,\n",
    "    rm_spacy_stopwords,\n",
    ")\n",
    "\n",
    "#tensorflow dependencies for Google Universal Sentence Encoder\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "#AzureML packages\n",
    "import azureml as aml\n",
    "import logging\n",
    "from azureml.telemetry import set_diagnostics_collection\n",
    "set_diagnostics_collection(send_diagnostics=True)\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Azure ML SDK Version:\", aml.core.VERSION)\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Tensorflow Version:\", tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = '../../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll collect the Google Sentence Encoder encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 401/401 [00:01<00:00, 252KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to ../../data\\raw\\stsbenchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 401/401 [00:01<00:00, 307KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to ../../data\\raw\\stsbenchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 401/401 [00:01<00:00, 277KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to ../../data\\raw\\stsbenchmark\n"
     ]
    }
   ],
   "source": [
    "train_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"train\")\n",
    "dev_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"dev\")\n",
    "test_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = stsbenchmark.clean_sts(train_raw)\n",
    "dev = stsbenchmark.clean_sts(dev_raw)\n",
    "test = stsbenchmark.clean_sts(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.00</td>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0   5.00                         A plane is taking off.   \n",
       "1   3.80                A man is playing a large flute.   \n",
       "2   3.80  A man is spreading shreded cheese on a pizza.   \n",
       "\n",
       "                                           sentence2  \n",
       "0                        An air plane is taking off.  \n",
       "1                          A man is playing a flute.  \n",
       "2  A man is spreading shredded cheese on an uncoo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all this with our util!\n",
    "data = []\n",
    "with open(\"sts-train.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        data.append([l[5].strip().lower(),l[6].strip().lower(), float(l[4])])\n",
    "train_old = pd.DataFrame(data, columns=['sentence1','sentence2','score'])\n",
    "\n",
    "data = []\n",
    "with open(\"sts-test.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        data.append([l[5].strip().lower(),l[6].strip().lower(), float(l[4])])\n",
    "test_old = pd.DataFrame(data, columns=['sentence1','sentence2','score'])\n",
    "\n",
    "data = []\n",
    "with open(\"sts-dev.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        l = line.strip().split(\"\\t\")\n",
    "        data.append([l[5].strip().lower(),l[6].strip().lower(), float(l[4])])\n",
    "dev_old = pd.DataFrame(data, columns=['sentence1','sentence2','score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "google_USE_embed = hub.Module(module_url)\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_google_universal_sentence_encoder(dataset, embedding_model):\n",
    "    sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "    sts_input2 = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "    sts_encode1 = tf.nn.l2_normalize(embedding_model(sts_input1), axis=1)\n",
    "    sts_encode2 = tf.nn.l2_normalize(embedding_model(sts_input2), axis=1)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        emb1, emb2 = session.run(\n",
    "          [sts_encode1, sts_encode2],\n",
    "          feed_dict={\n",
    "              sts_input1: dataset['sentence1'],\n",
    "              sts_input2: dataset['sentence2']\n",
    "          })\n",
    "    return emb1, emb2\n",
    "    \n",
    "def feature_engineering(dataset, googleUSE_embedding_model):\n",
    "    google_USE_emb1, google_USE_emb2 = embed_google_universal_sentence_encoder(dataset, googleUSE_embedding_model)\n",
    "    n_google = google_USE_emb1.shape[1]    \n",
    "    df = np.concatenate((google_USE_emb1, google_USE_emb2), axis=1)\n",
    "    names = ['USEEmb1_'+str(i) for i in range(n_google)]+['USEEmb2_'+str(i) for i in range(n_google)]\n",
    "    df = pd.DataFrame(df, columns=names)\n",
    "    df['score'] = dataset['score'].tolist()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = feature_engineering(train, google_USE_embed)\n",
    "validation_data = feature_engineering(dev, google_USE_embed)\n",
    "testing_data = feature_engineering(test, google_USE_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out when notebook is ready for publishing\n",
    "training_data.to_csv(\"training_set.csv\", index=None)\n",
    "testing_data.to_csv(\"testing_set.csv\", index=None)\n",
    "validation_data.to_csv(\"validation_set.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_performance(data):\n",
    "    sent1_googleUSE = data[[i for i in data.columns if 'USEEmb1' in i]].values.tolist()\n",
    "    sent2_googleUSE = data[[i for i in data.columns if 'USEEmb2' in i]].values.tolist()\n",
    "    \n",
    "    predictions_googleUSE = [1-distance.cosine(sent1_googleUSE[i], sent2_googleUSE[i]) for i in range(len(sent1_googleUSE))]\n",
    "    print(\"Google Universal Sentence Encoder Pearson Correlation:\",pearsonr(predictions_googleUSE, data['score'].values.tolist())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Universal Sentence Encoder Pearson Correlation: 0.7640271333273213\n"
     ]
    }
   ],
   "source": [
    "get_baseline_performance(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML - no AmlCompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0612 14:38:35.410748 27824 _profile.py:1082] Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n",
      "W0612 14:38:47.482707 30000 _profile.py:774] You have logged in. Now let us find all the subscriptions to which you have access...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive authentication successfully completed.\n",
      "Workspace name: MAIDAPTest\n",
      "Azure region: eastus2\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: nlprg\n"
     ]
    }
   ],
   "source": [
    "ws = azureml_utils.get_or_create_workspace(\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    workspace_region=\"<WORKSPACE_REGION>\"\n",
    ")\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out when notebook is ready for publishing\n",
    "training_data = pd.read_csv(\"training_set.csv\")\n",
    "testing_data = pd.read_csv(\"testing_set.csv\")\n",
    "validation_data = pd.read_csv(\"validation_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = training_data['score']\n",
    "train_x = training_data[[i for i in training_data.columns if 'USE' in i]]\n",
    "\n",
    "validation_y = validation_data['score']\n",
    "validation_x = validation_data[[i for i in validation_data.columns if 'USE' in i]]\n",
    "\n",
    "test_y = testing_data['score']\n",
    "test_x = testing_data[[i for i in testing_data.columns if 'USE' in i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5749, 1024)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\" : 15,\n",
    "    \"iterations\" : 50,\n",
    "    \"primary_metric\" : 'spearman_correlation',\n",
    "    \"preprocess\" : True,\n",
    "    \"verbosity\":logging.ERROR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local compute\n",
    "automated_ml_config = AutoMLConfig(task = 'regression',\n",
    "                     debug_log = 'automated_ml_errors.log',\n",
    "                     path = './automated-ml-regression',\n",
    "                     X = train_x.values,\n",
    "                     y = train_y.values.flatten(),\n",
    "                     X_valid = validation_x.values,\n",
    "                     y_valid = validation_y.values.flatten(),\n",
    "                     **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local machine\n",
      "Parent Run ID: AutoML_be89b4c3-aaa6-4b11-aaff-65d63d4d73be\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper RandomForest             0:00:38       0.1752    0.1752\n",
      "         1   MinMaxScaler RandomForest                      0:00:58       0.4233    0.4233\n",
      "         2   "
     ]
    }
   ],
   "source": [
    "experiment=Experiment(ws, 'automated-ml-regression')\n",
    "local_run = experiment.submit(automated_ml_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(local_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget_data = RunDetails(local_run).get_widget_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model AutoML089672db2best\n",
      "AutoML089672db2best\n"
     ]
    }
   ],
   "source": [
    "description = 'AutoML Sentence Similarity Model'\n",
    "tags = None\n",
    "model = local_run.register_model(description = description, tags = tags)\n",
    "\n",
    "print(local_run.model_id) # This will be written to the script file later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_metric = \"spearman_correlation\"\n",
    "best_run, fitted_model = local_run.get_output(metric = lookup_metric)\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fitted_model.predict(test_x.values)\n",
    "print(pearsonr(y_pred, test_y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimators': [('24', Pipeline(memory=None,\n",
       "        steps=[('standardscalerwrapper', <automl.client.core.common.model_wrappers.StandardScalerWrapper object at 0x000002669728A1D0>), ('xgboostregressor', XGBoostRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "            colsample_bytree=1, eta=0.01, gamma=0, learning_rate=0.1,\n",
       "            ma...ale_pos_weight=1, seed=None,\n",
       "            silent=True, subsample=0.7, tree_method='auto', verbose=-10))])),\n",
       "  ('33', Pipeline(memory=None,\n",
       "        steps=[('maxabsscaler', MaxAbsScaler(copy=True)), ('lightgbmregressor', LightGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "            colsample_bytree=0.7000000000000001, importance_type='split',\n",
       "            learning_rate=0.16842263157894738, max_bin=7, max_depth=3,\n",
       "            min_child_samples=14,...ue, subsample=0.5499999999999999,\n",
       "            subsample_for_bin=200000, subsample_freq=3, verbose=-1))])),\n",
       "  ('25', Pipeline(memory=None,\n",
       "        steps=[('truncatedsvdwrapper', TruncatedSVDWrapper(n_components=0.2573684210526316, random_state=None)), ('xgboostregressor', XGBoostRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "            colsample_bytree=1, eta=0.01, gamma=0, learning_rate=0.1,\n",
       "            max_delta_step=0, max_dept...scale_pos_weight=1, seed=None,\n",
       "            silent=True, subsample=1, tree_method='auto', verbose=-10))])),\n",
       "  ('20', Pipeline(memory=None,\n",
       "        steps=[('standardscalerwrapper', <automl.client.core.common.model_wrappers.StandardScalerWrapper object at 0x00000266972A9710>), ('lightgbmregressor', LightGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1,\n",
       "            importance_type='split', learning_rate=0.2, max_bin=63,\n",
       "        ...425, silent=True, subsample=0.85,\n",
       "            subsample_for_bin=200000, subsample_freq=2, verbose=-1))])),\n",
       "  ('40', Pipeline(memory=None,\n",
       "        steps=[('truncatedsvdwrapper', TruncatedSVDWrapper(n_components=0.3563157894736842, random_state=None)), ('xgboostregressor', XGBoostRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "            colsample_bytree=0.9, eta=0.3, gamma=0, learning_rate=0.1,\n",
       "            max_delta_step=0, max_dep...ale_pos_weight=1, seed=None, silent=True,\n",
       "            subsample=0.9, tree_method='auto', verbose=-10))])),\n",
       "  ('37', Pipeline(memory=None,\n",
       "        steps=[('maxabsscaler', MaxAbsScaler(copy=True)), ('lightgbmregressor', LightGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "            colsample_bytree=0.9, importance_type='split',\n",
       "            learning_rate=0.11579368421052631, max_bin=63, max_depth=10,\n",
       "            min_child_samples=16, min_child_we...e, subsample=0.44999999999999996,\n",
       "            subsample_for_bin=200000, subsample_freq=3, verbose=-1))])),\n",
       "  ('44', Pipeline(memory=None,\n",
       "        steps=[('maxabsscaler', MaxAbsScaler(copy=True)), ('lightgbmregressor', LightGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "            colsample_bytree=0.7000000000000001, importance_type='split',\n",
       "            learning_rate=0.16842263157894738, max_bin=7, max_depth=9,\n",
       "            min_child_samples=60,...    subsample=0.7999999999999999, subsample_for_bin=200000,\n",
       "            subsample_freq=7, verbose=-1))])),\n",
       "  ('45', Pipeline(memory=None,\n",
       "        steps=[('truncatedsvdwrapper', TruncatedSVDWrapper(n_components=0.10894736842105263, random_state=None)), ('xgboostregressor', XGBoostRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "            colsample_bytree=0.7, eta=0.05, gamma=0.1, learning_rate=0.1,\n",
       "            max_delta_step=0, max...ale_pos_weight=1, seed=None,\n",
       "            silent=True, subsample=0.7, tree_method='auto', verbose=-10))]))],\n",
       " 'weights': [0.2,\n",
       "  0.2,\n",
       "  0.2,\n",
       "  0.13333333333333333,\n",
       "  0.06666666666666667,\n",
       "  0.06666666666666667,\n",
       "  0.06666666666666667,\n",
       "  0.06666666666666667],\n",
       " 'flatten_transform': None}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model.pipeline.steps[1][1].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence_similarity_regressor.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "model_path = 'sentence_similarity_regressor.pkl'\n",
    "\n",
    "joblib.dump(fitted_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = joblib.load('sentence_similarity_regressor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionPipeline(pipeline=Pipeline(memory=None,\n",
       "     steps=[('datatransformer', DataTransformer(enable_feature_sweeping=None, feature_sweeping_timeout=None,\n",
       "        is_onnx_compatible=None, logger=None, observer=None, task=None)), ('prefittedsoftvotingregressor', PreFittedSoftVotingRegressor(estimators=[('24', Pipeline(memory=None,\n",
       "     steps=[('stand...333333333333, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667]))]),\n",
       "          stddev=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
