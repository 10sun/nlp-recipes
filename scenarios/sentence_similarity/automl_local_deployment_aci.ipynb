{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local AutoML Model with ACI Deployment for Predicting Sentence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use Azure AutoML locally to automate machine learning model selection and tuning and how to use Azure Container Instance (ACI) for deployment. We utilize the STS Benchmark dataset to predict sentence similarity and utilize AutoML's text preprocessing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#1.-Introduction)  \n",
    "    * 1.1 [What is Azure AutoML?](#1.1-What-is-Azure-AutoML?)  \n",
    "    * 1.2 [Modeling Problem](#1.2-Modeling-Problem)  \n",
    "    \n",
    "    \n",
    "2. [Data Preparation](#2.-Data-Preparation)  \n",
    "\n",
    "\n",
    "3. [Create AutoML Run](#3.-Create-AutoML-Run)    \n",
    "    * 3.1 [Link to or create a Workspace](#3.1-Link-to-or-create-a-Workspace)  \n",
    "    * 3.2 [Create AutoMLConfig object](#3.2-Create-AutoMLConfig-object)\n",
    "    * 3.3 [Run Experiment](#3.3-Run-Experiment)\n",
    "    \n",
    "    \n",
    "4. [Deploy Sentence Similarity Model](#4.-Deploy-Sentence-Similarity-Model)  \n",
    "    4.1 [Retrieve the Best Model](#4.1-Retrieve-the-Best-Model)  \n",
    "    4.2 [Register the Fitted Model for Deployment](#4.2-Register-the-Fitted-Model-for-Deployment)   \n",
    "    4.3 [Create Scoring Script](#4.3-Create-Scoring-Script)   \n",
    "    4.4 [Create a YAML File for the Environment](#4.4-Create-a-YAML-File-for-the-Environment)  \n",
    "    4.5 [Create a Container Image](#4.5-Create-a-Container-Image)    \n",
    "    4.6 [Deploy the Image as a Web Service to Azure Container Instance](#4.6-Deploy-the-Image-as-a-Web-Service-to-Azure-Container-Instance)  \n",
    "    4.7 [Test Deployed Model](#4.7-Test-Deployed-Model)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What is Azure AutoML?\n",
    "\n",
    "Automated machine learning (AutoML) is a capability of Microsoft's Azure Machine Learning service. The goal of AutoML is to \"improve the productivity of data scientists and democratize AI\" [1] by allowing for the rapid development and deployment of machine learning models. To acheive this goal, AutoML automates the process of selecting a ML model and tuning the model. All the user is required to provide is a dataset (suitable for a classification, regression, or time-series forecasting problem) and a metric to optimize in choosing the model and hyperparameters. The user is also given the ability to set time and cost constraints for the model selection and tuning.\n",
    "\n",
    "[1]https://azure.microsoft.com/en-us/blog/new-automated-machine-learning-capabilities-in-azure-machine-learning-service/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://nlpbp.blob.core.windows.net/images/automl.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoML model selection and tuning process can be easily tracked through the Azure portal or directly in python notebooks through the use of widgets. AutoML quickly selects a high quilty machine learning model tailored for your prediction problem. In this notebook, we walk through the steps of preparing data, setting up an AutoML experiment, and evaluating the results of our best model. More information about running AutoML experiments in Python can be found [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Modeling Problem\n",
    "\n",
    "The regression problem we will demonstrate is predicting sentence similarity scores on the STS Benchmark dataset. The [STS Benchmark dataset](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark#STS_benchmark_dataset_and_companion_dataset) contains a selection of English datasets that were used in Semantic Textual Similarity (STS) tasks 2012-2017. The dataset contains 8,628 sentence pairs with a human-labeled integer representing the sentences' similarity (ranging from 0, for no meaning overlap, to 5, meaning equivalence). The sentence pairs will be embedded using AutoML's built-in preprocessing, so we'll pass the sentences directly into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning diagnostics collection on. \n",
      "System version: 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n",
      "Azure ML SDK Version: 1.0.43\n",
      "Pandas version: 0.23.4\n",
      "Tensorflow Version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Set the environment path to find NLP\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial import distance\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "\n",
    "# Import utils\n",
    "from utils_nlp.azureml import azureml_utils\n",
    "from utils_nlp.dataset import stsbenchmark\n",
    "from utils_nlp.dataset.preprocess import (\n",
    "    to_lowercase,\n",
    "    to_spacy_tokens,\n",
    "    rm_spacy_stopwords,\n",
    ")\n",
    "from utils_nlp.common.timer import Timer\n",
    "\n",
    "# Tensorflow dependencies for Google Universal Sentence Encoder\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) # reduce logging output\n",
    "\n",
    "# AzureML packages\n",
    "import azureml as aml\n",
    "import logging\n",
    "from azureml.telemetry import set_diagnostics_collection\n",
    "set_diagnostics_collection(send_diagnostics=True)\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Azure ML SDK Version:\", aml.core.VERSION)\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Tensorflow Version:\", tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = '../../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STS Benchmark Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, the STS Benchmark dataset contains 8.6K sentence pairs along with a human-annotated score for how similiar the two sentences are. We will load the training, development (validation), and test sets provided by STS Benchmark and preprocess the data (lowercase the text, drop irrelevant columns, and rename the remaining columns) using the utils contained in this repo. Each dataset will ultimately have three columns: _sentence1_ and _sentence2_ which contain the text of the sentences in the sentence pair, and _score_ which contains the human-annotated similarity score of the sentence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 401/401 [00:01<00:00, 258KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to ../../data\\raw\\stsbenchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 401/401 [00:01<00:00, 294KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to ../../data\\raw\\stsbenchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 401/401 [00:01<00:00, 252KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to ../../data\\raw\\stsbenchmark\n"
     ]
    }
   ],
   "source": [
    "# Load in the raw datasets as pandas dataframes\n",
    "train_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"train\")\n",
    "dev_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"dev\")\n",
    "test_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean each dataset by lowercasing text, removing irrelevant columns,\n",
    "# and renaming the remaining columns\n",
    "train_clean = stsbenchmark.clean_sts(train_raw)\n",
    "dev_clean = stsbenchmark.clean_sts(dev_raw)\n",
    "test_clean = stsbenchmark.clean_sts(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase\n",
    "train = to_lowercase(train_clean)\n",
    "dev = to_lowercase(dev_clean)\n",
    "test = to_lowercase(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 5749 sentences\n",
      "Development set has 1500 sentences\n",
      "Testing set has 1379 sentences\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has {} sentences\".format(len(train)))\n",
    "print(\"Development set has {} sentences\".format(len(dev)))\n",
    "print(\"Testing set has {} sentences\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.00</td>\n",
       "      <td>a plane is taking off.</td>\n",
       "      <td>an air plane is taking off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.80</td>\n",
       "      <td>a man is playing a large flute.</td>\n",
       "      <td>a man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.80</td>\n",
       "      <td>a man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>a man is spreading shredded cheese on an uncoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.60</td>\n",
       "      <td>three men are playing chess.</td>\n",
       "      <td>two men are playing chess.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.25</td>\n",
       "      <td>a man is playing the cello.</td>\n",
       "      <td>a man seated is playing the cello.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0   5.00                         a plane is taking off.   \n",
       "1   3.80                a man is playing a large flute.   \n",
       "2   3.80  a man is spreading shreded cheese on a pizza.   \n",
       "3   2.60                   three men are playing chess.   \n",
       "4   4.25                    a man is playing the cello.   \n",
       "\n",
       "                                           sentence2  \n",
       "0                        an air plane is taking off.  \n",
       "1                          a man is playing a flute.  \n",
       "2  a man is spreading shredded cheese on an uncoo...  \n",
       "3                         two men are playing chess.  \n",
       "4                 a man seated is playing the cello.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create AutoML Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML can be used for classification, regression or timeseries experiments. Each experiment type has corresponding machine learning models and metrics that can be optimized (see [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train)) and the options will be delineated below. As a first step we connect to an existing workspace or create one if it doesn't exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Link to or create a Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n",
      "WARNING - You have logged in. Now let us find all the subscriptions to which you have access...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive authentication successfully completed.\n"
     ]
    }
   ],
   "source": [
    "ws = azureml_utils.get_or_create_workspace(\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    workspace_region=\"<WORKSPACE_REGION>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create AutoMLConfig object\n",
    "Next, we specify the parameters for the AutoMLConfig class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task**  \n",
    "AutoML supports the following base learners for the regression task: Elastic Net, Light GBM, Gradient Boosting, Decision Tree, K-nearest Neighbors, LARS Lasso, Stochastic Gradient Descent, Random Forest, Extremely Randomized Trees, XGBoost, DNN Regressor, Linear Regression. In addition, AutoML also supports two kinds of ensemble methods: voting (weighted average of the output of multiple base learners) and stacking (training a second \"metalearner\" which uses the base algorithms' predictions to predict the target variable). Specific base learners can be included or excluded in the parameters for the AutoMLConfig class (whitelist_models and blacklist_models) and the voting/stacking ensemble options can be specified as well (enable_voting_ensemble and enable_stack_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**preprocess**  \n",
    "AutoML also has advanced preprocessing methods, eliminating the need for users to perform this manually. Data is automatically scaled and normalized but an additional parameter in the AutoMLConfig class enables the use of more advanced techniques including imputation, generating additional features, transformations, word embeddings, etc. (full list found [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-create-portal-experiments#preprocess)). Note that algorithm-specific preprocessing will be applied even if preprocess=False. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**primary_metric**  \n",
    "The regression metrics available are the following: Spearman Correlation (spearman_correlation), Normalized RMSE (normalized_root_mean_squared_error), Normalized MAE (normalized_mean_absolute_error), and R2 score (r2_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constraints:**  \n",
    "There is a cost_mode parameter to set cost prediction modes (see options [here](https://docs.microsoft.com/en-us/python/api/azureml-train-automl/azureml.train.automl.automlconfig?view=azure-ml-py)). To set constraints on time there are multiple parameters including experiment_exit_score (target score to exit the experiment after achieving), experiment_timeout_minutes (maximum amount of time for all combined iterations), and iterations (total number of different algorithm and parameter combinations to try)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"task\": 'regression', #type of task: classification, regression or forecasting\n",
    "    \"debug_log\": 'automated_ml_errors.log',\n",
    "    \"path\": './automated-ml-regression',\n",
    "    \"iteration_timeout_minutes\" : 15, #How long each iteration can take before moving on\n",
    "    \"iterations\" : 50, #Number of algorithm options to try\n",
    "    \"primary_metric\" : 'spearman_correlation', #Metric to optimize\n",
    "    \"preprocess\" : True, #Whether dataset preprocessing should be applied\n",
    "    \"verbosity\":logging.ERROR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(\"score\", axis=1).values\n",
    "y_train = train['score'].values.flatten()\n",
    "X_validation = dev.drop(\"score\", axis=1).values\n",
    "y_validation = dev['score'].values.flatten()\n",
    "\n",
    "# local compute\n",
    "automated_ml_config = AutoMLConfig(\n",
    "     X = X_train,\n",
    "     y = y_train,\n",
    "     X_valid = X_validation,\n",
    "     y_valid = y_validation,\n",
    "     **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Run Experiment\n",
    "\n",
    "Run the experiment locally and inspect the results using a widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local machine\n",
      "Parent Run ID: AutoML_ad20c29f-7d03-4079-8699-3133d24d3631\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper RandomForest             0:00:11       0.0606    0.0606\n",
      "         1   MaxAbsScaler RandomForest                      0:00:47       0.2127    0.2127\n",
      "         2   StandardScalerWrapper ExtremeRandomTrees       0:00:21       0.2173    0.2173\n",
      "         3   StandardScalerWrapper LightGBM                 0:00:15       0.2905    0.2905\n",
      "         4   StandardScalerWrapper RandomForest             0:00:12       0.0669    0.2905\n",
      "         5   MaxAbsScaler ExtremeRandomTrees                0:00:31       0.2224    0.2905\n",
      "         6   StandardScalerWrapper ExtremeRandomTrees       0:00:17       0.1769    0.2905\n",
      "         7   MaxAbsScaler DecisionTree                      0:00:14       0.1186    0.2905\n",
      "         8   MaxAbsScaler ExtremeRandomTrees                0:00:17       0.1891    0.2905\n",
      "         9   MaxAbsScaler SGD                               0:00:10       0.1448    0.2905\n",
      "        10   StandardScalerWrapper RandomForest             0:00:12       0.0199    0.2905\n",
      "        11   StandardScalerWrapper DecisionTree             0:00:14       0.1245    0.2905\n",
      "        12   MaxAbsScaler SGD                               0:00:13       0.1310    0.2905\n",
      "        13   MaxAbsScaler DecisionTree                      0:00:12       0.1370    0.2905\n",
      "        14   MaxAbsScaler SGD                               0:00:12       0.0572    0.2905\n",
      "        15   StandardScalerWrapper RandomForest             0:00:19       0.1924    0.2905\n",
      "        16   MaxAbsScaler RandomForest                      0:00:11       0.0187    0.2905\n",
      "        17   MaxAbsScaler ElasticNet                        0:00:10          nan    0.2905\n",
      "ERROR: Run AutoML_ad20c29f-7d03-4079-8699-3133d24d3631_17 failed with exception \"Primary metric spearman_correlation is not available.\".\n",
      "        18   MaxAbsScaler ExtremeRandomTrees                0:00:11       0.0972    0.2905\n",
      "        19   MaxAbsScaler DecisionTree                      0:00:13       0.1686    0.2905\n",
      "        20   StandardScalerWrapper LightGBM                 0:00:26       0.6102    0.6102\n",
      "        21   MaxAbsScaler RandomForest                      0:05:00       0.1617    0.6102\n",
      "        22   StandardScalerWrapper LightGBM                 0:00:25       0.3608    0.6102\n",
      "        23   StandardScalerWrapper RandomForest             0:02:32       0.2200    0.6102\n",
      "        24   MaxAbsScaler DecisionTree                      0:01:13       0.2027    0.6102\n",
      "        25   TruncatedSVDWrapper LightGBM                   0:00:31       0.3707    0.6102\n",
      "        26   StandardScalerWrapper ExtremeRandomTrees       0:00:21       0.1498    0.6102\n",
      "        27   MaxAbsScaler DecisionTree                      0:00:11       0.1748    0.6102\n",
      "        28   MaxAbsScaler LightGBM                          0:00:18       0.4395    0.6102\n",
      "        29   MaxAbsScaler LightGBM                          0:00:23       0.4191    0.6102\n",
      "        30   TruncatedSVDWrapper LightGBM                   0:00:43       0.4102    0.6102\n",
      "        31   MaxAbsScaler LightGBM                          0:00:27       0.5077    0.6102\n",
      "        32   MaxAbsScaler LightGBM                          0:00:44       0.6012    0.6102\n",
      "        33   MaxAbsScaler LightGBM                          0:00:48       0.4611    0.6102\n",
      "        34   MaxAbsScaler LightGBM                          0:00:39       0.5135    0.6102\n",
      "        35   MaxAbsScaler LightGBM                          0:00:24       0.2219    0.6102\n",
      "        36   SparseNormalizer LightGBM                      0:00:24       0.2888    0.6102\n",
      "        37   StandardScalerWrapper LightGBM                 0:00:38       0.5663    0.6102\n",
      "        38   MaxAbsScaler LightGBM                          0:00:31       0.3793    0.6102\n",
      "        39   MaxAbsScaler LightGBM                          0:00:40       0.3672    0.6102\n",
      "        40   MaxAbsScaler LightGBM                          0:00:21       0.2507    0.6102\n",
      "        41   MaxAbsScaler LightGBM                          0:00:37       0.3352    0.6102\n",
      "        42   StandardScalerWrapper LightGBM                 0:01:05       0.5460    0.6102\n",
      "        43   MaxAbsScaler LightGBM                          0:00:36       0.5104    0.6102\n",
      "        44   SparseNormalizer LightGBM                      0:02:38       0.4208    0.6102\n",
      "        45   TruncatedSVDWrapper LightGBM                   0:00:37       0.2362    0.6102\n",
      "        46   StandardScalerWrapper LightGBM                 0:00:41       0.4394    0.6102\n",
      "        47   MaxAbsScaler LightGBM                          0:00:46       0.3982    0.6102\n",
      "        48   VotingEnsemble                                 0:02:21       0.6408    0.6408\n",
      "        49   StackEnsemble                                  0:03:01       0.6409    0.6409\n"
     ]
    }
   ],
   "source": [
    "experiment=Experiment(ws, 'automated-ml-regression')\n",
    "local_run = experiment.submit(automated_ml_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the completed run can be visualized in two ways. First, by using a RunDetails widget as shown in the cell below. Second, by accessing the [Azure portal](https://portal.azure.com), selecting your workspace, clicking on _Experiments_ and then selecting the name and run number of the experiment you want to inspect. Both these methods will show the results and duration for each iteration (algorithm tried), a visualization of the results, and information about the run including the compute target, primary metric, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the run details using the provided widget\n",
    "RunDetails(local_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://nlpbp.blob.core.windows.net/images/autoMLwidget.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deploy Sentence Similarity Model\n",
    "\n",
    "## 4.1 Retrieve the Best Model\n",
    "Now we can identify the model that maximized performance on a given metric (spearman correlation in our case) using the get_output method which returns the best run and fitted model across all iterations. Overloads on get_output allow you to retrieve the best run and fitted model for any logged metric or for a particular iteration. The object returned by AutoML is a Pipeline class which chains together multiple steps in a machine learning workflow in order to provide a \"reproducible mechanism for building, evaluating, deploying, and running ML systems\" (see [here](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb) for additional information about Pipelines). \n",
    "\n",
    "The different steps that make up the pipeline can be accessed through `fitted_model.named_steps` and information about data preprocessing is available through `fitted_model.named_steps['datatransformer'].get_featurization_summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Register the Fitted Model for Deployment\n",
    "If neither metric nor iteration are specified in the register_model call, the iteration with the best primary metric is registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model AutoMLad20c29f7best\n",
      "AutoMLad20c29f7best\n"
     ]
    }
   ],
   "source": [
    "description = 'AutoML Model'\n",
    "tags = {'area': \"nlp\", 'type': \"sentence similarity automl\"}\n",
    "name = 'automl'\n",
    "model = local_run.register_model(description = description, tags = tags)\n",
    "\n",
    "print(local_run.model_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Create Scoring Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import pickle\n",
    "import json\n",
    "import numpy\n",
    "import azureml.train.automl\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path(model_name = '<<modelid>>') # this name is model.id of model that we want to deploy\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(rawdata):\n",
    "    try:\n",
    "        data = json.loads(rawdata)['data']\n",
    "        data = numpy.array(data)\n",
    "        result = model.predict(data)\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"error\": result})\n",
    "    return json.dumps({\"result\":result.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute the actual model id in the script file.\n",
    "script_file_name = 'score.py'\n",
    "\n",
    "with open(script_file_name, 'r') as cefr:\n",
    "    content = cefr.read()\n",
    "\n",
    "with open(script_file_name, 'w') as cefw:\n",
    "    cefw.write(content.replace('<<modelid>>', local_run.model_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Create a YAML File for the Environment\n",
    "\n",
    "To ensure the fit results are consistent with the training results, the SDK dependency versions need to be the same as the environment that trains the model. The following cells create a file, autoenv.yml, which specifies the dependencies from the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, 'automated-ml-regression')\n",
    "ml_run = AutoMLRun(experiment = experiment, run_id = local_run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No issues found in the SDK package versions.\n"
     ]
    }
   ],
   "source": [
    "best_iteration = int(best_run.id.split(\"_\")[-1]) #get the appended iteration number for the best model\n",
    "dependencies = ml_run.get_run_sdk_dependencies(iteration = best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'azureml-train-automl': '1.0.43.1',\n",
       " 'azureml-automl-core': '1.0.43',\n",
       " 'azureml': '0.2.7',\n",
       " 'azureml-widgets': '1.0.43.1',\n",
       " 'azureml-train': '1.0.43',\n",
       " 'azureml-train-restclients-hyperdrive': '1.0.43',\n",
       " 'azureml-train-core': '1.0.43',\n",
       " 'azureml-telemetry': '1.0.43',\n",
       " 'azureml-sdk': '1.0.43',\n",
       " 'azureml-pipeline': '1.0.43',\n",
       " 'azureml-pipeline-steps': '1.0.43',\n",
       " 'azureml-pipeline-core': '1.0.43',\n",
       " 'azureml-dataprep': '1.1.5',\n",
       " 'azureml-dataprep-native': '13.0.0',\n",
       " 'azureml-core': '1.0.43.1',\n",
       " 'azureml-contrib-brainwave': '1.0.33'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dependencies in the yaml file from the above cell. You must specify the version of \"azureml-sdk[automl]\" while creating the yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automlenv.yml'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn','py-xgboost<=0.80'],\n",
    "                                 pip_packages=['azureml-sdk[automl]==1.0.43.*'], \n",
    "                                 python_version = '3.6.8')\n",
    "\n",
    "conda_env_file_name = 'automlenv.yml'\n",
    "myenv.save_to_file('.', conda_env_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Create a Container Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image\n",
      "Running..................................................\n",
      "Succeeded\n",
      "Image creation operation finished for image automl-image:8, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "image_config = ContainerImage.image_configuration(execution_script = script_file_name,\n",
    "                                                  runtime = \"python\",\n",
    "                                                  conda_file = conda_env_file_name,\n",
    "                                                  description = \"Image with automl model\",\n",
    "                                                  tags = {'area': \"nlp\", 'type': \"sentencesimilarity automl\"})\n",
    "\n",
    "image = ContainerImage.create(name = \"automl-image\",\n",
    "                              # this is the model object\n",
    "                              models = [model],\n",
    "                              image_config = image_config,\n",
    "                              workspace = ws)\n",
    "\n",
    "image.wait_for_creation(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above step fails then use below command to see logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.image_build_log_uri) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Deploy the Image as a Web Service to Azure Container Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the web service configuration\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating service\n",
      "Running.......................\n",
      "SucceededACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "# deploy image as web service\n",
    "aci_service_name ='aci-automl-service'\n",
    "aci_service = Webservice.deploy_from_image(workspace = ws, \n",
    "                                           name = aci_service_name,\n",
    "                                           image = image,\n",
    "                                           deployment_config = aci_config)\n",
    "\n",
    "aci_service.wait_for_deployment(show_output = True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch logs to debug incase of failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aci_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Test Deployed Model\n",
    "We test the web sevice by passing data. The run method expects input in json format. Run() method retrieves API keys behind the scenes to make sure that call is authenticated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test['score'].values.flatten()\n",
    "test_x = test.drop(\"score\", axis=1).values.tolist()\n",
    "\n",
    "data = {'data': test_x}\n",
    "data = json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Timer to see how long the model takes to predict\n",
    "t = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 2.7085\n",
      "Number of samples predicted: 1379\n"
     ]
    }
   ],
   "source": [
    "t.start()\n",
    "score = aci_service.run(input_data = data)\n",
    "t.stop()\n",
    "print(\"Time elapsed: {}\".format(t))\n",
    "\n",
    "result = json.loads(score)\n",
    "try:\n",
    "    output = result[\"result\"]\n",
    "    print('Number of samples predicted: {0}'.format(len(output)))\n",
    "except:\n",
    "    print(result['error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll calculate the Pearson Correlation on the test set.\n",
    "\n",
    "**What is Pearson Correlation?**\n",
    "\n",
    "Our evaluation metric is Pearson correlation ($\\rho$) which is a measure of the linear correlation between two variables. The formula for calculating Pearson correlation is as follows:  \n",
    "\n",
    "$$\\rho_{X,Y} = \\frac{E[(X-\\mu_X)(Y-\\mu_Y)]}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "This metric takes a value in [-1,1] where -1 represents a perfect negative correlation, 1 represents a perfect positive correlation, and 0 represents no correlation. We utilize the Pearson correlation metric as this is the metric that [SentEval](http://nlpprogress.com/english/semantic_textual_similarity.html), a widely-used evaluation toolkit for evaluation sentence representations, uses for the STS Benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6038286237427414\n"
     ]
    }
   ],
   "source": [
    "print(pearsonr(output, test_y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
