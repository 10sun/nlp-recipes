{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentEval with AzureML\n",
    "[SentEval](https://github.com/facebookresearch/SentEval) is a widely used benchmarking tool for evaluating general-purpose sentence embeddings. It provides a simple interface for evaluating your embeddings on up to 17 supported downstream tasks (such as sentiment classification, natural language inference, semantic similarity, etc.)\n",
    "\n",
    "This notebook shows how to run SentEval for [Gensen](https://github.com/Maluuba/gensen) with the AzureML SDK, where\n",
    "- the model weights are on AzureML Datastore. To download the pre-trained Gensen model, run `bash download_models.sh` from the gensen/data/models directory. \n",
    "- the embeddings are on AzureML Datastore. To download the pre-trained embeddings, run `bash glove2h5.sh` from the gensen/data/embedding directory.\n",
    "- the data for the SentEval transfer tasks are on AzureML Datastore. To download these datasets, run `bash get_transfer_data.bash` from the SentEval/data/downstream directory.\n",
    "- evaluation runs on the AzureML Workspace GPU Compute Target (no extra provisioning/config needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import scrapbook as sb\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "from azureml.core import Datastore\n",
    "import azureml.data\n",
    "from azureml.data.azure_storage_datastore import AzureFileDatastore\n",
    "\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils_nlp.azureml.azureml_utils import get_or_create_workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PATH_TO_GENSEN = (\n",
    "    \"../../../gensen\"\n",
    ")  # Set this path to where you have cloned the gensen source code\n",
    "PATH_TO_SENTEVAL = (\n",
    "    \"../../../SentEval\"\n",
    ")  # Set this path to where you have cloned the senteval source code\n",
    "PATH_TO_SER = \"../../utils_nlp/eval/senteval.py\"\n",
    "PATH_TO_AML = \"../../utils_nlp/azureml/azureml_utils.py\"\n",
    "config_path = (\n",
    "    \"./.azureml\"\n",
    ")  # Path to the directory containing config.json with azureml credentials\n",
    "\n",
    "AZUREML_VERBOSE = True\n",
    "cluster_name = \"eval-gpu\"  # Name of AzureML Compute Target cluster\n",
    "experiment_name = \"senteval-pytorch-gensen\"  # Name of the AzureML experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the AzureML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ws = get_or_create_workspace(\n",
    "    config_path=config_path,\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    workspace_region=\"<WORKSPACE_REGION>\",\n",
    ")\n",
    "\n",
    "if AZUREML_VERBOSE:\n",
    "    print(\"Workspace name: {}\".format(ws.name))\n",
    "    print(\"Resource group: {}\".format(ws.resource_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach the gpu-enabled compute target, or create a new one if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found compute target: {}\".format(cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target: {}\".format(cluster_name))\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC6\", max_nodes=4\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "if AZUREML_VERBOSE:\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the datastore. Here we will use the default datastore and then upload our external dependencies. \n",
    "\n",
    "If your data is already on the cloud, you can register your resource on any Azure storage account as the datastore. (Currently, the list of supported Azure storage services that can be registered as datastores are Azure Blob Container, Azure File Share, Azure Data Lake, Azure Data Lake Gen2, Azure SQL Database, Azure PostgreSQL, and Databricks File System. Learn more about the Datastore module [here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore?view=azure-ml-py).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "if AZUREML_VERBOSE:\n",
    "    print(\"Default datastore: {}\".format(ds.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload the gensen dependency\n",
    "ds.upload(\n",
    "    src_dir=os.path.join(PATH_TO_GENSEN),\n",
    "    target_path=os.path.join(experiment_name, \"gensen_lib\"),\n",
    "    overwrite=False,\n",
    "    show_progress=AZUREML_VERBOSE,\n",
    ");\n",
    "\n",
    "# Upload the senteval dependency\n",
    "ds.upload(\n",
    "    src_dir=os.path.join(PATH_TO_SENTEVAL),\n",
    "    target_path=os.path.join(experiment_name, \"senteval_lib\"),\n",
    "    overwrite=False,\n",
    "    show_progress=AZUREML_VERBOSE,\n",
    ");\n",
    "\n",
    "# Upload the utils_nlp dependencies\n",
    "ds.upload_files(\n",
    "    files=[\n",
    "        os.path.join(os.path.commonprefix([PATH_TO_SER, PATH_TO_AML]), \"__init__.py\"),\n",
    "    ],\n",
    "    target_path=os.path.join(experiment_name, \"utils_nlp\"),\n",
    "    overwrite=True,\n",
    "    show_progress=AZUREML_VERBOSE,\n",
    ");\n",
    "\n",
    "ds.upload_files(\n",
    "    files=[PATH_TO_SER],\n",
    "    target_path=os.path.join(experiment_name, \"utils_nlp/eval\"),\n",
    "    overwrite=True,\n",
    "    show_progress=AZUREML_VERBOSE,\n",
    ");\n",
    "\n",
    "ds.upload_files(\n",
    "    files=[PATH_TO_AML],\n",
    "    target_path=os.path.join(experiment_name, \"utils_nlp/azureml\"),\n",
    "    overwrite=True,\n",
    "    show_progress=AZUREML_VERBOSE,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after the upload is complete, you can safely delete the dependencies from your local machine to free up some memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = os.path.join(os.getcwd(), experiment_name)\n",
    "os.makedirs(src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $src_dir/evaluate.py\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "from azureml.core.run import Run\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--ds_gensen\", type=str, dest=\"ds_gensen\")\n",
    "    parser.add_argument(\"--ds_senteval\", type=str, dest=\"ds_senteval\")\n",
    "    parser.add_argument(\"--ds_utils\", type=str, dest=\"ds_utils\")\n",
    "    parser.add_argument(\"--ds_utils_azureml\", type=str, dest=\"ds_utils_azureml\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Import the dependencies\n",
    "    sys.path.append(args.ds_gensen)\n",
    "    from gensen import GenSen, GenSenSingle\n",
    "\n",
    "    sys.path.append(args.ds_utils)\n",
    "    from eval.senteval import SentEvalRunner\n",
    "    sys.path.append(args.ds_utils_azureml)\n",
    "    from azureml_utils import log_metrics_table\n",
    "    \n",
    "    \n",
    "    TRANSFER_TASKS = [\"STSBenchmark\", \"STS12\", \"STS13\", \"STS14\", \"STS15\", \"STS16\"]\n",
    "\n",
    "    # Define the model\n",
    "    model_params = {}\n",
    "    model_params[\"folder_path\"] = os.path.join(args.ds_gensen, \"data/models\")\n",
    "    model_params[\"prefix\"] = \"nli_large_bothskip\"\n",
    "    model_params[\"pretrain\"] = os.path.join(\n",
    "        args.ds_gensen, \"data/embedding/glove.840B.300d.h5\"\n",
    "    )\n",
    "    model_params[\"cuda\"] = torch.cuda.is_available()\n",
    "\n",
    "    gensen = GenSenSingle(\n",
    "        model_folder=model_params[\"folder_path\"],\n",
    "        filename_prefix=model_params[\"prefix\"],\n",
    "        pretrained_emb=model_params[\"pretrain\"],\n",
    "        cuda=model_params[\"cuda\"],\n",
    "    )\n",
    "\n",
    "    # Define SentEval Runner\n",
    "    ser = SentEvalRunner(path_to_senteval=args.ds_senteval)\n",
    "    ser.set_transfer_data_path(relative_path=\"data\")\n",
    "    ser.set_transfer_tasks(TRANSFER_TASKS)\n",
    "    ser.set_model(gensen)\n",
    "    ser.set_params({\"usepytorch\": True, \"kfold\": 10})\n",
    "\n",
    "    # Define the batcher and prepare functions for SentEval\n",
    "    def prepare(params, samples):\n",
    "        vocab = set()\n",
    "        for sample in samples:\n",
    "            if params.current_task != \"TREC\":\n",
    "                sample = \" \".join(sample).lower().split()\n",
    "            else:\n",
    "                sample = \" \".join(sample).split()\n",
    "            for word in sample:\n",
    "                if word not in vocab:\n",
    "                    vocab.add(word)\n",
    "\n",
    "        vocab.add(\"<s>\")\n",
    "        vocab.add(\"<pad>\")\n",
    "        vocab.add(\"<unk>\")\n",
    "        vocab.add(\"</s>\")\n",
    "        # Optional vocab expansion\n",
    "        # params[\"model\"].vocab_expansion(vocab)\n",
    "\n",
    "    def batcher(params, batch):\n",
    "        # batch contains list of words\n",
    "        max_tasks = [\"MR\", \"CR\", \"SUBJ\", \"MPQA\", \"ImageCaptionRetrieval\"]\n",
    "        if params.current_task in max_tasks:\n",
    "            strategy = \"max\"\n",
    "        else:\n",
    "            strategy = \"last\"\n",
    "\n",
    "        sentences = [\" \".join(s).lower() for s in batch]\n",
    "        _, embeddings = params[\"model\"].get_representation(\n",
    "            sentences, pool=strategy, return_numpy=True\n",
    "        )\n",
    "        return embeddings\n",
    "\n",
    "    # Run SentEval\n",
    "    results = ser.run(batcher, prepare)\n",
    "\n",
    "    # Log results as scalars in AzureML\n",
    "    eval_metrics = ser.log_mean(results, selected_metrics=[\"pearson\", \"spearman\"])\n",
    "    log_metrics_table(eval_metrics, Run.get_context(), as_scalar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pytorch Estimator to submit the evaluation script to the compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = PyTorch(\n",
    "    source_directory=src_dir,\n",
    "    script_params={\n",
    "        \"--ds_gensen\": ds.path(\"{}/gensen_lib\".format(experiment_name)).as_mount(),\n",
    "        \"--ds_senteval\": ds.path(\"{}/senteval_lib\".format(experiment_name)).as_mount(),\n",
    "        \"--ds_utils\": ds.path(\"{}/utils_nlp\".format(experiment_name)).as_mount(),\n",
    "        \"--ds_utils_azureml\": ds.path(\"{}/utils_nlp/azureml\".format(experiment_name)).as_mount(),\n",
    "    },\n",
    "    compute_target=compute_target,\n",
    "    entry_script=\"evaluate.py\",\n",
    "    node_count=4,\n",
    "    process_count_per_node=1,\n",
    "    distributed_training=MpiConfiguration(),\n",
    "    use_gpu=True,\n",
    "    framework_version=\"1.0\",\n",
    "    conda_packages=[\"scikit-learn==0.20.3\", \"h5py\", \"nltk\"],\n",
    "    pip_packages=[\"pandas\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, name=experiment_name)\n",
    "run = experiment.submit(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the run via a Jupyter widget. Alternatively, block until the script has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()\n",
    "# run.wait_for_completion(show_output=AZUREML_VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist properties of the run so we can access the logged metrics later\n",
    "sb.glue(\"run_id\", run.get_details()[\"runId\"])\n",
    "sb.glue(\"experiment_name\", experiment_name)\n",
    "sb.glue(\"ws_config\", config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "shutil.rmtree(src_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_cpu)",
   "language": "python",
   "name": "nlp_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
