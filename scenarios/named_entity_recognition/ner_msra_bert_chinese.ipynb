{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*  \n",
    "*Licensed under the MIT License.*\n",
    "# Named Entity Recognition Using BERT on Chinese\n",
    "## Summary\n",
    "This notebook demonstrates how to fine tune [pretrained BERT model](https://github.com/huggingface/pytorch-pretrained-BERT) for named entity recognition (NER) task on Chinese text. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, and model evaluation. \n",
    "\n",
    "[BERT (Bidirectional Transformers for Language Understanding)](https://arxiv.org/pdf/1810.04805.pdf) is a powerful pre-trained lanaguage model that can be used for multiple NLP tasks, including text classification, question answering, named entity recognition, etc. It's able to achieve state of the art performance with only a few epochs of fine tuning on task specific datasets.  \n",
    "The figure below illustrates how BERT can be fine tuned for NER tasks. The input data is a list of tokens representing a sentence. In the training data, each token has an entity label. After fine tuning, the model predicts an entity label for each token in a given testing sentence. \n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/bert_architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages\n",
    "* pytorch\n",
    "* pytorch-pretrained-bert\n",
    "* pandas\n",
    "* seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "nlp_path = os.path.abspath('../../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "\n",
    "from utils_nlp.bert.token_classification import BERTTokenClassifier, postprocess_token_labels, create_label_map\n",
    "from utils_nlp.bert.common import Language, Tokenizer\n",
    "from utils_nlp.dataset.msra_ner import load_pandas_df, get_unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path configurations\n",
    "DATA_DIR = \"./data\"\n",
    "TRAIN_DATA_FILE = \"../MSRA/msra-bakeoff3-training-utf8.2col\"\n",
    "Test_DATA_FILE = \"../MSRA/bakeoff3_goldstandard.txt\"\n",
    "CACHE_DIR=\"./temp\"\n",
    "\n",
    "# set random seeds\n",
    "RANDOM_SEED = 100\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# model configurations\n",
    "LANGUAGE = Language.CHINESE\n",
    "DO_LOWER_CASE = True\n",
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "# training configurations\n",
    "BATCH_SIZE = 16\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "\n",
    "# optimizer configuration\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_PROPORTION = 0.1\n",
    "\n",
    "TEXT_COL = \"sentence\"\n",
    "LABEL_COL = \"labels\"\n",
    "\n",
    "CACHE_DIR = \"../../../\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and testing data\n",
    "The dataset used in this notebook is the [wikigold dataset](https://www.aclweb.org/anthology/W09-3302). The wikigold dataset consists of 145 mannually labelled Wikipedia articles, including 1841 sentences and 40k tokens in total. The dataset can be directly downloaded from [here](https://github.com/juand-r/entity-recognition-datasets/tree/master/data/wikigold). The `download` function downloads the data file to a user-specified directory.  \n",
    "\n",
    "The helper function `get_train_test_data` splits the dataset into training and testing sets according to `test_percentage`. Because this is a relatively small dataset, we set `test_percentage` to 0.5 in order to have enough data for model evaluation. Running this notebook multiple times with different random seeds produces similar results.   \n",
    "\n",
    "The helper function `get_unique_labels` returns the unique entity labels in the dataset. There are 5 unique labels in the   original dataset: 'O' (non-entity), 'I-LOC' (location), 'I-MISC' (miscellaneous), 'I-PER' (person), and 'I-ORG' (organization). An 'X' label is added for the trailing word pieces generated by BERT, because BERT uses WordPiece tokenizer.  \n",
    "\n",
    "The maximum number of words in a sentence is 144, so we set MAX_SEQ_LENGTH to 200 above, because the number of tokens will grow after WordPiece tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length in train data is: 746\n",
      "Maximum sequence length in test data is: 439\n",
      "Number of sentences in training data: 45000\n",
      "Number of sentences in testing data: 3940\n",
      "Unique labels: ['O', 'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "train_df = load_pandas_df(local_cache_path=CACHE_DIR, file_split=\"train\")\n",
    "test_df = load_pandas_df(local_cache_path=CACHE_DIR, file_split=\"test\")\n",
    "label_list = get_unique_labels()\n",
    "print(\"Number of sentences in training data: {}\".format(train_df.shape[0]))\n",
    "print(\"Number of sentences in testing data: {}\".format(test_df.shape[0]))\n",
    "print(\"Unique labels: {}\".format(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>当 希 望 工 程 救 助 的 百 万 儿 童 成 长 起 来 ， 科 教 兴 国 蔚 然 ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>藏 书 本 来 就 是 所 有 传 统 收 藏 门 类 中 的 第 一 大 户 ， 只 是 ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>因 有 关 日 寇 在 京 掠 夺 文 物 详 情 ， 藏 界 较 为 重 视 ， 也 是 ...</td>\n",
       "      <td>[O, O, O, B-LOC, O, O, B-LOC, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我 们 藏 有 一 册 １ ９ ４ ５ 年 ６ 月 油 印 的 《 北 京 文 物 保 存 ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>以 家 乡 的 历 史 文 献 、 特 定 历 史 时 期 书 刊 、 某 一 名 家 或 ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  当 希 望 工 程 救 助 的 百 万 儿 童 成 长 起 来 ， 科 教 兴 国 蔚 然 ...   \n",
       "1  藏 书 本 来 就 是 所 有 传 统 收 藏 门 类 中 的 第 一 大 户 ， 只 是 ...   \n",
       "2  因 有 关 日 寇 在 京 掠 夺 文 物 详 情 ， 藏 界 较 为 重 视 ， 也 是 ...   \n",
       "3  我 们 藏 有 一 册 １ ９ ４ ５ 年 ６ 月 油 印 的 《 北 京 文 物 保 存 ...   \n",
       "4  以 家 乡 的 历 史 文 献 、 特 定 历 史 时 期 书 刊 、 某 一 名 家 或 ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, B-LOC, O, O, B-LOC, O, O, O, O, O, O...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Preprocessing\n",
    "The `preprocess_ner_tokens` method of the `Tokenizer` class converts raw string data to numerical features, involving the following steps:\n",
    "1. WordPiece tokenization.\n",
    "2. Convert tokens and labels to numerical values, i.e. token ids and label ids.\n",
    "3. Sequence padding or truncation according to the `max_seq_length` configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dictionary that maps labels to numerical values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_map = create_label_map(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize input text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(language=LANGUAGE, \n",
    "                      to_lower=DO_LOWER_CASE, \n",
    "                      cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create numerical features**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_token_ids, train_input_mask, train_trailing_token_mask, train_label_ids = \\\n",
    "    tokenizer.tokenize_preprocess_ner_text(text=train_df[TEXT_COL],\n",
    "                                           label_map=label_map,\n",
    "                                           max_len=MAX_SEQ_LENGTH,\n",
    "                                           labels=train_df[LABEL_COL])\n",
    "test_token_ids, test_input_mask, test_trailing_token_mask, test_label_ids = \\\n",
    "    tokenizer.tokenize_preprocess_ner_text(text=test_df[TEXT_COL],\n",
    "                                           label_map=label_map,\n",
    "                                           max_len=MAX_SEQ_LENGTH,\n",
    "                                           labels=test_df[LABEL_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokenizer.preprocess_ner_tokens` outputs three or four lists of numerical features lists, each sublist contains features of an input sentence: \n",
    "1. token ids: list of numerical values each corresponds to a token.\n",
    "2. attention mask: list of 1s and 0s, 1 for input tokens and 0 for padded tokens, so that padded tokens are not attended to. \n",
    "3. trailing word piece mask: boolean list, `True` for the first word piece of each original word, `False` for the trailing word pieces, e.g. ##ing. This mask is useful for removing predictions on trailing word pieces, so that each original word in the input text has a unique predicted label. \n",
    "4. label ids: list of numerical values each corresponds to an entity label, if `labels` is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample token ids:\n",
      "[2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample attention mask:\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample trailing token mask:\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "\n",
      "Sample label ids:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample token ids:\\n{}\\n\".format(train_token_ids[0]))\n",
    "print(\"Sample attention mask:\\n{}\\n\".format(train_input_mask[0]))\n",
    "print(\"Sample trailing token mask:\\n{}\\n\".format(train_trailing_token_mask[0]))\n",
    "print(\"Sample label ids:\\n{}\\n\".format(train_label_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Token Classifier\n",
    "The value of the `language` argument determines which BERT model is used:\n",
    "* Language.ENGLISH: \"bert-base-uncased\"\n",
    "* Language.ENGLISHCASED: \"bert-base-cased\"\n",
    "* Language.ENGLISHLARGE: \"bert-large-uncased\"\n",
    "* Language.ENGLISHLARGECASED: \"bert-large-cased\"\n",
    "* Language.CHINESE: \"bert-base-chinese\"\n",
    "* Language.MULTILINGUAL: \"bert-base-multilingual-cased\"\n",
    "\n",
    "Here we use the base, uncased pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_classifier = BERTTokenClassifier(language=LANGUAGE,\n",
    "                                       num_labels=len(label_list),\n",
    "                                       cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/2813 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Only 1 CUDA device is available. Data parallelism is not possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   1%|          | 26/2813 [00:31<55:41,  1.20s/it]\u001b[A\n",
      "Iteration:   1%|          | 26/2813 [00:49<55:41,  1.20s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 51/2813 [01:01<55:19,  1.20s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 51/2813 [01:20<55:19,  1.20s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 76/2813 [01:31<54:57,  1.20s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 76/2813 [01:50<54:57,  1.20s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 101/2813 [02:02<54:39,  1.21s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 101/2813 [02:20<54:39,  1.21s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 126/2813 [02:32<54:20,  1.21s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 126/2813 [02:50<54:20,  1.21s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 151/2813 [03:03<53:57,  1.22s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 151/2813 [03:20<53:57,  1.22s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 176/2813 [03:33<53:34,  1.22s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 176/2813 [03:50<53:34,  1.22s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 201/2813 [04:04<53:12,  1.22s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 201/2813 [04:20<53:12,  1.22s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 226/2813 [04:35<52:53,  1.23s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 226/2813 [04:50<52:53,  1.23s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 251/2813 [05:06<52:24,  1.23s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 251/2813 [05:20<52:24,  1.23s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 276/2813 [05:37<51:57,  1.23s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 276/2813 [05:50<51:57,  1.23s/it]\u001b[A\n",
      "Iteration:  11%|█         | 301/2813 [06:08<51:32,  1.23s/it]\u001b[A\n",
      "Iteration:  11%|█         | 301/2813 [06:20<51:32,  1.23s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 326/2813 [06:38<50:59,  1.23s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 326/2813 [06:50<50:59,  1.23s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 351/2813 [07:09<50:31,  1.23s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 351/2813 [07:20<50:31,  1.23s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 376/2813 [07:40<50:05,  1.23s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 376/2813 [08:00<50:05,  1.23s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 401/2813 [08:11<49:30,  1.23s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 401/2813 [08:30<49:30,  1.23s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 426/2813 [08:41<48:57,  1.23s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 426/2813 [09:00<48:57,  1.23s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 451/2813 [09:12<48:24,  1.23s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 451/2813 [09:30<48:24,  1.23s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 476/2813 [09:43<47:51,  1.23s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 476/2813 [10:00<47:51,  1.23s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 501/2813 [10:14<47:21,  1.23s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 501/2813 [10:30<47:21,  1.23s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 526/2813 [10:44<46:48,  1.23s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 526/2813 [11:00<46:48,  1.23s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 551/2813 [11:15<46:16,  1.23s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 551/2813 [11:30<46:16,  1.23s/it]\u001b[A\n",
      "Iteration:  20%|██        | 576/2813 [11:46<45:47,  1.23s/it]\u001b[A\n",
      "Iteration:  20%|██        | 576/2813 [12:00<45:47,  1.23s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 601/2813 [12:16<45:16,  1.23s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 601/2813 [12:30<45:16,  1.23s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 625/2813 [12:46<45:04,  1.24s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 625/2813 [13:00<45:04,  1.24s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 650/2813 [13:17<44:26,  1.23s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 650/2813 [13:30<44:26,  1.23s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 675/2813 [13:48<43:57,  1.23s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 675/2813 [14:00<43:57,  1.23s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 700/2813 [14:19<43:20,  1.23s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 700/2813 [14:30<43:20,  1.23s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 724/2813 [14:49<43:05,  1.24s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 724/2813 [15:00<43:05,  1.24s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 749/2813 [15:19<42:27,  1.23s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 749/2813 [15:30<42:27,  1.23s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 774/2813 [15:50<41:50,  1.23s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 774/2813 [16:10<41:50,  1.23s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 799/2813 [16:21<41:17,  1.23s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 799/2813 [16:40<41:17,  1.23s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 824/2813 [16:51<40:43,  1.23s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 824/2813 [17:10<40:43,  1.23s/it]\u001b[A\n",
      "Iteration:  30%|███       | 849/2813 [17:22<40:16,  1.23s/it]\u001b[A\n",
      "Iteration:  30%|███       | 849/2813 [17:40<40:16,  1.23s/it]\u001b[A\n",
      "Iteration:  31%|███       | 874/2813 [17:53<39:46,  1.23s/it]\u001b[A\n",
      "Iteration:  31%|███       | 874/2813 [18:10<39:46,  1.23s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 899/2813 [18:23<39:11,  1.23s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 899/2813 [18:40<39:11,  1.23s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 924/2813 [18:54<38:38,  1.23s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 924/2813 [19:10<38:38,  1.23s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 949/2813 [19:25<38:08,  1.23s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 949/2813 [19:40<38:08,  1.23s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 974/2813 [19:55<37:35,  1.23s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 974/2813 [20:10<37:35,  1.23s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 999/2813 [20:26<37:05,  1.23s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 999/2813 [20:40<37:05,  1.23s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 1024/2813 [20:57<36:35,  1.23s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 1024/2813 [21:10<36:35,  1.23s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 1049/2813 [21:27<36:04,  1.23s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 1049/2813 [21:40<36:04,  1.23s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 1074/2813 [21:58<35:35,  1.23s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 1074/2813 [22:10<35:35,  1.23s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 1099/2813 [22:29<35:08,  1.23s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 1099/2813 [22:40<35:08,  1.23s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 1124/2813 [23:00<34:36,  1.23s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 1124/2813 [23:20<34:36,  1.23s/it]\u001b[A\n",
      "Iteration:  41%|████      | 1149/2813 [23:30<34:02,  1.23s/it]\u001b[A\n",
      "Iteration:  41%|████      | 1149/2813 [23:50<34:02,  1.23s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 1174/2813 [24:01<33:31,  1.23s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 1174/2813 [24:20<33:31,  1.23s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 1199/2813 [24:32<32:58,  1.23s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 1199/2813 [24:50<32:58,  1.23s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 1224/2813 [25:02<32:29,  1.23s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 1224/2813 [25:20<32:29,  1.23s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 1249/2813 [25:33<31:56,  1.23s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 1249/2813 [25:50<31:56,  1.23s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 1274/2813 [26:04<31:30,  1.23s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 1274/2813 [26:20<31:30,  1.23s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 1299/2813 [26:34<30:59,  1.23s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 1299/2813 [26:50<30:59,  1.23s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 1324/2813 [27:05<30:30,  1.23s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 1324/2813 [27:20<30:30,  1.23s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 1349/2813 [27:36<29:58,  1.23s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 1349/2813 [27:50<29:58,  1.23s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 1374/2813 [28:07<29:31,  1.23s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 1374/2813 [28:20<29:31,  1.23s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 1399/2813 [28:37<28:58,  1.23s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 1399/2813 [28:50<28:58,  1.23s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 1424/2813 [29:08<28:26,  1.23s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 1424/2813 [29:20<28:26,  1.23s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 1449/2813 [29:39<27:58,  1.23s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 1449/2813 [29:50<27:58,  1.23s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 1474/2813 [30:10<27:28,  1.23s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 1474/2813 [30:30<27:28,  1.23s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 1499/2813 [30:41<26:59,  1.23s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 1499/2813 [31:00<26:59,  1.23s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 1524/2813 [31:11<26:27,  1.23s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 1524/2813 [31:30<26:27,  1.23s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 1549/2813 [31:42<25:58,  1.23s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 1549/2813 [32:00<25:58,  1.23s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 1574/2813 [32:13<25:26,  1.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  56%|█████▌    | 1574/2813 [32:30<25:26,  1.23s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 1599/2813 [32:44<24:53,  1.23s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 1599/2813 [33:00<24:53,  1.23s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 1624/2813 [33:14<24:21,  1.23s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 1624/2813 [33:30<24:21,  1.23s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 1649/2813 [33:45<23:51,  1.23s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 1649/2813 [34:00<23:51,  1.23s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 1674/2813 [34:16<23:19,  1.23s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 1674/2813 [34:30<23:19,  1.23s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 1699/2813 [34:47<22:48,  1.23s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 1699/2813 [35:00<22:48,  1.23s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 1724/2813 [35:17<22:19,  1.23s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 1724/2813 [35:30<22:19,  1.23s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1749/2813 [35:48<21:47,  1.23s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1749/2813 [36:00<21:47,  1.23s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1774/2813 [36:19<21:16,  1.23s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1774/2813 [36:30<21:16,  1.23s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1799/2813 [36:49<20:44,  1.23s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1799/2813 [37:00<20:44,  1.23s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1824/2813 [37:20<20:16,  1.23s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1824/2813 [37:40<20:16,  1.23s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1849/2813 [37:51<19:47,  1.23s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1849/2813 [38:10<19:47,  1.23s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1874/2813 [38:22<19:14,  1.23s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1874/2813 [38:40<19:14,  1.23s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1899/2813 [38:52<18:43,  1.23s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1899/2813 [39:10<18:43,  1.23s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1924/2813 [39:23<18:12,  1.23s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1924/2813 [39:40<18:12,  1.23s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1949/2813 [39:54<17:40,  1.23s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1949/2813 [40:10<17:40,  1.23s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 1974/2813 [40:24<17:08,  1.23s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 1974/2813 [40:40<17:08,  1.23s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1999/2813 [40:55<16:38,  1.23s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1999/2813 [41:10<16:38,  1.23s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 2024/2813 [41:26<16:08,  1.23s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 2024/2813 [41:40<16:08,  1.23s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 2049/2813 [41:56<15:37,  1.23s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 2049/2813 [42:10<15:37,  1.23s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 2074/2813 [42:27<15:06,  1.23s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 2074/2813 [42:40<15:06,  1.23s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 2099/2813 [42:58<14:36,  1.23s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 2099/2813 [43:10<14:36,  1.23s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 2124/2813 [43:29<14:06,  1.23s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 2124/2813 [43:40<14:06,  1.23s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 2149/2813 [43:59<13:35,  1.23s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 2149/2813 [44:10<13:35,  1.23s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 2174/2813 [44:30<13:05,  1.23s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 2174/2813 [44:50<13:05,  1.23s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 2199/2813 [45:01<12:33,  1.23s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 2199/2813 [45:20<12:33,  1.23s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 2224/2813 [45:31<12:03,  1.23s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 2224/2813 [45:50<12:03,  1.23s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 2249/2813 [46:02<11:32,  1.23s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 2249/2813 [46:20<11:32,  1.23s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 2274/2813 [46:33<11:01,  1.23s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 2274/2813 [46:50<11:01,  1.23s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 2299/2813 [47:03<10:30,  1.23s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 2299/2813 [47:20<10:30,  1.23s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 2324/2813 [47:34<09:59,  1.23s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 2324/2813 [47:50<09:59,  1.23s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 2349/2813 [48:05<09:28,  1.23s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 2349/2813 [48:20<09:28,  1.23s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 2374/2813 [48:35<08:58,  1.23s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 2374/2813 [48:50<08:58,  1.23s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 2399/2813 [49:06<08:27,  1.23s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 2399/2813 [49:20<08:27,  1.23s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 2424/2813 [49:36<07:56,  1.22s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 2424/2813 [49:50<07:56,  1.22s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 2449/2813 [50:07<07:26,  1.23s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 2449/2813 [50:20<07:26,  1.23s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 2474/2813 [50:38<06:55,  1.23s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 2474/2813 [50:50<06:55,  1.23s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 2499/2813 [51:08<06:24,  1.23s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 2499/2813 [51:20<06:24,  1.23s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 2524/2813 [51:39<05:54,  1.23s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 2524/2813 [51:50<05:54,  1.23s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 2549/2813 [52:10<05:24,  1.23s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 2549/2813 [52:30<05:24,  1.23s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 2574/2813 [52:41<04:53,  1.23s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 2574/2813 [53:00<04:53,  1.23s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 2599/2813 [53:11<04:22,  1.23s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 2599/2813 [53:30<04:22,  1.23s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 2624/2813 [53:42<03:52,  1.23s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 2624/2813 [54:00<03:52,  1.23s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 2649/2813 [54:13<03:21,  1.23s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 2649/2813 [54:30<03:21,  1.23s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 2674/2813 [54:43<02:50,  1.23s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 2674/2813 [55:00<02:50,  1.23s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 2699/2813 [55:14<02:19,  1.23s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 2699/2813 [55:30<02:19,  1.23s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 2724/2813 [55:45<01:49,  1.23s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 2724/2813 [56:00<01:49,  1.23s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 2749/2813 [56:15<01:18,  1.23s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 2749/2813 [56:30<01:18,  1.23s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 2774/2813 [56:46<00:47,  1.23s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 2774/2813 [57:00<00:47,  1.23s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 2799/2813 [57:17<00:17,  1.23s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 2799/2813 [57:30<00:17,  1.23s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [57:34<00:00, 3454.07s/it].23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07014742273803971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "token_classifier.fit(token_ids=train_token_ids, \n",
    "                     input_mask=train_input_mask, \n",
    "                     labels=train_label_ids,\n",
    "                     num_epochs=NUM_TRAIN_EPOCHS, \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Iteration:   0%|          | 0/247 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Only 1 CUDA device is available. Data parallelism is not possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 247/247 [01:40<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 0.017158769807770757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = token_classifier.predict(token_ids=test_token_ids, \n",
    "                                       input_mask=test_input_mask, \n",
    "                                       labels=test_label_ids, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       probabilities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = predictions.probabilities\n",
    "pred_label_ids = predictions.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "The `predict` method of the token classifier outputs label ids for all tokens, including the padded tokens. `postprocess_token_labels` is a helper function that removes the predictions on padded tokens. If a `label_map` is provided, it maps the numerical label ids back to original token labels which are usually string type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      PER       0.98      0.94      0.96      1055\n",
      "      LOC       0.93      0.94      0.93      1987\n",
      "      ORG       0.82      0.91      0.86       846\n",
      "\n",
      "micro avg       0.92      0.93      0.92      3888\n",
      "macro avg       0.92      0.93      0.93      3888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_tags_no_padding = postprocess_token_labels(pred_label_ids, \n",
    "                                                test_input_mask, \n",
    "                                                label_map)\n",
    "true_tags_no_padding = postprocess_token_labels(test_label_ids, \n",
    "                                                test_input_mask, \n",
    "                                                label_map)\n",
    "print(classification_report(true_tags_no_padding, pred_tags_no_padding, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`postprocess_token_labels` also provides an option to remove the predictions on trailing word pieces, e.g. ##ing, so that the final predicted labels correspond to the original words in the input text. The `trailing_token_mask` is obtained from `tokenizer.preprocess_ner_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      PER       0.98      0.94      0.96      1055\n",
      "      LOC       0.93      0.94      0.93      1987\n",
      "      ORG       0.82      0.91      0.86       846\n",
      "\n",
      "micro avg       0.92      0.93      0.92      3888\n",
      "macro avg       0.92      0.93      0.93      3888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_tags_no_padding_no_trailing = postprocess_token_labels(pred_label_ids, \n",
    "                                                            test_input_mask, \n",
    "                                                            label_map, \n",
    "                                                            remove_trailing_word_pieces=True, \n",
    "                                                            trailing_token_mask=test_trailing_token_mask)\n",
    "true_tags_no_padding_no_trailing = postprocess_token_labels(test_label_ids, \n",
    "                                                            test_input_mask, \n",
    "                                                            label_map, \n",
    "                                                            remove_trailing_word_pieces=True, \n",
    "                                                            trailing_token_mask=test_trailing_token_mask)\n",
    "print(classification_report(true_tags_no_padding_no_trailing, pred_tags_no_padding_no_trailing, digits=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_pytorch",
   "language": "python",
   "name": "nlp_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
