{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*  \n",
    "*Licensed under the MIT License.*\n",
    "# Named Entity Recognition Using BERT on Chinese\n",
    "## Summary\n",
    "This notebook demonstrates how to fine tune [pretrained BERT model](https://github.com/huggingface/pytorch-pretrained-BERT) for named entity recognition (NER) task on Chinese text. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring and model evaluation.\n",
    "\n",
    "[BERT (Bidirectional Transformers for Language Understanding)](https://arxiv.org/pdf/1810.04805.pdf) is a powerful pre-trained lanaguage model that can be used for multiple NLP tasks, including text classification, question answering, named entity recognition, etc. It's able to achieve state of the art performance with only a few epochs of fine tuning on task specific datasets.  \n",
    "The figure below illustrates how BERT can be fine tuned for NER tasks. The input data is a list of tokens representing a sentence. In the training data, each token has an entity label. After fine tuning, the model predicts an entity label for each token in a given testing sentence. \n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/bert_architecture.png\">\n",
    "\n",
    "Named Entity Recognition on non-English text is not very differnt from that on English text. The only difference is the model used, which is configured by the `LANGUAGE` variable below. For non-English languages including Chinese, the *bert-base-multilingual-cased* model can be used by setting `LANGUAGE = Language.MULTILINGUAL`. For Chinese, the *bert-base-chinese* model can also be used by setting `LANGUAGE = Language.CHINESE`. On Chinese text, the performance of *bert-base-chinese* is usually better than *bert-base-multilingual-cased* because the *bert-base-chinese* model is pretrained on Chinese data only. On this particular dataset, the performances of the Chinese-only model and multilingual model are very similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages\n",
    "* pytorch\n",
    "* pytorch-pretrained-bert\n",
    "* pandas\n",
    "* seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "\n",
    "nlp_path = os.path.abspath('../../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "\n",
    "from utils_nlp.bert.token_classification import BERTTokenClassifier, postprocess_token_labels, create_label_map\n",
    "from utils_nlp.bert.common import Language, Tokenizer\n",
    "from utils_nlp.dataset.msra_ner import load_pandas_df, get_unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path configurations\n",
    "CACHE_DIR = \"./temp\"\n",
    "\n",
    "# set random seeds\n",
    "RANDOM_SEED = 100\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# model configurations\n",
    "LANGUAGE = Language.CHINESE\n",
    "DO_LOWER_CASE = True\n",
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "# training configurations\n",
    "BATCH_SIZE = 16\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "\n",
    "# optimizer configuration\n",
    "LEARNING_RATE = 3e-5\n",
    "\n",
    "TEXT_COL = \"sentence\"\n",
    "LABEL_COL = \"labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and testing data\n",
    "The dataset used in this notebook is the MSRA NER dataset. The dataset consists of 45000 training sentences and 3442 testing sentences. \n",
    "\n",
    "The helper function `load_pandas_df` downloads the data files if they don't exist in `local_cache_path`. It returns the training or testing data frame based on `file_split`\n",
    "\n",
    "The helper function `get_unique_labels` returns the unique entity labels in the dataset. There are 7 unique labels in the   dataset: \n",
    "* 'O': non-entity \n",
    "* 'B-LOC': beginning of location entity\n",
    "* 'I-LOC': within location entity\n",
    "* 'B-PER': beginning of person entity\n",
    "* 'I-PER': within person entity\n",
    "* 'B-ORG': beginning of organization entity\n",
    "* 'I-ORG': within organization entity\n",
    "\n",
    "The maximum number of words in a sentence is 2427. We set MAX_SEQ_LENGTH to 200 above to reduce the GPU memory needed to run this notebook. Less than 1% of testing data are longer than 200, so this should have negligible impact on the model performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length in train data is: 746\n",
      "Maximum sequence length in test data is: 2427\n",
      "Number of sentences in training data: 45000\n",
      "Number of sentences in testing data: 3442\n",
      "Unique labels: ['O', 'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "train_df = load_pandas_df(local_cache_path=CACHE_DIR, file_split=\"train\")\n",
    "test_df = load_pandas_df(local_cache_path=CACHE_DIR, file_split=\"test\")\n",
    "label_list = get_unique_labels()\n",
    "print(\"Number of sentences in training data: {}\".format(train_df.shape[0]))\n",
    "print(\"Number of sentences in testing data: {}\".format(test_df.shape[0]))\n",
    "print(\"Unique labels: {}\".format(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[当, 希, 望, 工, 程, 救, 助, 的, 百, 万, 儿, 童, 成, 长, 起, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[藏, 书, 本, 来, 就, 是, 所, 有, 传, 统, 收, 藏, 门, 类, 中, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[因, 有, 关, 日, 寇, 在, 京, 掠, 夺, 文, 物, 详, 情, ，, 藏, ...</td>\n",
       "      <td>[O, O, O, B-LOC, O, O, B-LOC, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[我, 们, 藏, 有, 一, 册, １, ９, ４, ５, 年, ６, 月, 油, 印, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[以, 家, 乡, 的, 历, 史, 文, 献, 、, 特, 定, 历, 史, 时, 期, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [当, 希, 望, 工, 程, 救, 助, 的, 百, 万, 儿, 童, 成, 长, 起, ...   \n",
       "1  [藏, 书, 本, 来, 就, 是, 所, 有, 传, 统, 收, 藏, 门, 类, 中, ...   \n",
       "2  [因, 有, 关, 日, 寇, 在, 京, 掠, 夺, 文, 物, 详, 情, ，, 藏, ...   \n",
       "3  [我, 们, 藏, 有, 一, 册, １, ９, ４, ５, 年, ６, 月, 油, 印, ...   \n",
       "4  [以, 家, 乡, 的, 历, 史, 文, 献, 、, 特, 定, 历, 史, 时, 期, ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, B-LOC, O, O, B-LOC, O, O, O, O, O, O...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Preprocessing\n",
    "The `tokenize_ner` method of the `Tokenizer` class converts raw string data to numerical features, involving the following steps:\n",
    "1. WordPiece tokenization.\n",
    "2. Convert tokens and labels to numerical values, i.e. token ids and label ids.\n",
    "3. Sequence padding or truncation according to the `max_seq_length` configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dictionary that maps labels to numerical values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_map = create_label_map(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize input text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(language=LANGUAGE, \n",
    "                      to_lower=DO_LOWER_CASE, \n",
    "                      cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create numerical features**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_token_ids, train_input_mask, train_trailing_token_mask, train_label_ids = \\\n",
    "    tokenizer.tokenize_ner(text=train_df[TEXT_COL],\n",
    "                           label_map=label_map,\n",
    "                           max_len=MAX_SEQ_LENGTH,\n",
    "                           labels=train_df[LABEL_COL])\n",
    "test_token_ids, test_input_mask, test_trailing_token_mask, test_label_ids = \\\n",
    "    tokenizer.tokenize_ner(text=test_df[TEXT_COL],\n",
    "                           label_map=label_map,\n",
    "                           max_len=MAX_SEQ_LENGTH,\n",
    "                           labels=test_df[LABEL_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokenizer.tokenize_ner` outputs three or four lists of numerical features lists, each sublist contains features of an input sentence: \n",
    "1. token ids: list of numerical values each corresponds to a token.\n",
    "2. attention mask: list of 1s and 0s, 1 for input tokens and 0 for padded tokens, so that padded tokens are not attended to. \n",
    "3. trailing word piece mask: boolean list, `True` for the first word piece of each original word, `False` for the trailing word pieces, e.g. ##ize. This mask is useful for removing predictions on trailing word pieces, so that each original word in the input text has a unique predicted label. \n",
    "4. label ids: list of numerical values each corresponds to an entity label, if `labels` is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample token ids:\n",
      "[2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample attention mask:\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample trailing token mask:\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "\n",
      "Sample label ids:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((\"Sample token ids:\\n{}\\n\".format(train_token_ids[0])))\n",
    "print(\"Sample attention mask:\\n{}\\n\".format(train_input_mask[0]))\n",
    "print(\"Sample trailing token mask:\\n{}\\n\".format(train_trailing_token_mask[0]))\n",
    "print(\"Sample label ids:\\n{}\\n\".format(train_label_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Token Classifier\n",
    "The value of the `language` argument determines which BERT model is used:\n",
    "* Language.ENGLISH: \"bert-base-uncased\"\n",
    "* Language.ENGLISHCASED: \"bert-base-cased\"\n",
    "* Language.ENGLISHLARGE: \"bert-large-uncased\"\n",
    "* Language.ENGLISHLARGECASED: \"bert-large-cased\"\n",
    "* Language.CHINESE: \"bert-base-chinese\"\n",
    "* Language.MULTILINGUAL: \"bert-base-multilingual-cased\"\n",
    "\n",
    "Here we use the base model pre-trained only on Chinese data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_classifier = BERTTokenClassifier(language=LANGUAGE,\n",
    "                                       num_labels=len(label_map),\n",
    "                                       cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/2813 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Only 1 CUDA device is available. Data parallelism is not possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   1%|          | 25/2813 [00:30<57:34,  1.24s/it]\u001b[A\n",
      "Iteration:   1%|          | 25/2813 [00:49<57:34,  1.24s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 50/2813 [01:01<56:32,  1.23s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 50/2813 [01:20<56:32,  1.23s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 75/2813 [01:31<55:48,  1.22s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 75/2813 [01:50<55:48,  1.22s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 100/2813 [02:01<55:13,  1.22s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 100/2813 [02:20<55:13,  1.22s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 125/2813 [02:32<54:45,  1.22s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 125/2813 [02:50<54:45,  1.22s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 150/2813 [03:03<54:20,  1.22s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 150/2813 [03:20<54:20,  1.22s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 175/2813 [03:33<53:54,  1.23s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 175/2813 [03:50<53:54,  1.23s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 200/2813 [04:04<53:27,  1.23s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 200/2813 [04:20<53:27,  1.23s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 225/2813 [04:35<53:00,  1.23s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 225/2813 [04:50<53:00,  1.23s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 250/2813 [05:06<52:33,  1.23s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 250/2813 [05:20<52:33,  1.23s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 275/2813 [05:37<52:03,  1.23s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 275/2813 [05:50<52:03,  1.23s/it]\u001b[A\n",
      "Iteration:  11%|█         | 300/2813 [06:07<51:34,  1.23s/it]\u001b[A\n",
      "Iteration:  11%|█         | 300/2813 [06:20<51:34,  1.23s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 325/2813 [06:38<51:05,  1.23s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 325/2813 [06:50<51:05,  1.23s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 350/2813 [07:09<50:36,  1.23s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 350/2813 [07:20<50:36,  1.23s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 375/2813 [07:40<50:07,  1.23s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 375/2813 [08:00<50:07,  1.23s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 400/2813 [08:11<49:36,  1.23s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 400/2813 [08:30<49:36,  1.23s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 425/2813 [08:42<49:06,  1.23s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 425/2813 [09:00<49:06,  1.23s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 450/2813 [09:12<48:34,  1.23s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 450/2813 [09:30<48:34,  1.23s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 475/2813 [09:43<48:07,  1.23s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 475/2813 [10:00<48:07,  1.23s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 500/2813 [10:14<47:33,  1.23s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 500/2813 [10:30<47:33,  1.23s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 525/2813 [10:45<47:02,  1.23s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 525/2813 [11:00<47:02,  1.23s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 550/2813 [11:16<46:29,  1.23s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 550/2813 [11:30<46:29,  1.23s/it]\u001b[A\n",
      "Iteration:  20%|██        | 575/2813 [11:47<45:56,  1.23s/it]\u001b[A\n",
      "Iteration:  20%|██        | 575/2813 [12:00<45:56,  1.23s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 600/2813 [12:17<45:23,  1.23s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 600/2813 [12:30<45:23,  1.23s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 625/2813 [12:48<44:51,  1.23s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 625/2813 [13:00<44:51,  1.23s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 650/2813 [13:19<44:21,  1.23s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 650/2813 [13:30<44:21,  1.23s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 675/2813 [13:50<43:51,  1.23s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 675/2813 [14:00<43:51,  1.23s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 700/2813 [14:20<43:21,  1.23s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 700/2813 [14:40<43:21,  1.23s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 725/2813 [14:51<42:49,  1.23s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 725/2813 [15:10<42:49,  1.23s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 750/2813 [15:22<42:18,  1.23s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 750/2813 [15:40<42:18,  1.23s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 775/2813 [15:53<41:47,  1.23s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 775/2813 [16:10<41:47,  1.23s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 800/2813 [16:23<41:17,  1.23s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 800/2813 [16:40<41:17,  1.23s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 825/2813 [16:54<40:45,  1.23s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 825/2813 [17:10<40:45,  1.23s/it]\u001b[A\n",
      "Iteration:  30%|███       | 850/2813 [17:25<40:13,  1.23s/it]\u001b[A\n",
      "Iteration:  30%|███       | 850/2813 [17:40<40:13,  1.23s/it]\u001b[A\n",
      "Iteration:  31%|███       | 875/2813 [17:55<39:42,  1.23s/it]\u001b[A\n",
      "Iteration:  31%|███       | 875/2813 [18:10<39:42,  1.23s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 900/2813 [18:26<39:12,  1.23s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 900/2813 [18:40<39:12,  1.23s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 925/2813 [18:57<38:42,  1.23s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 925/2813 [19:10<38:42,  1.23s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 950/2813 [19:28<38:12,  1.23s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 950/2813 [19:40<38:12,  1.23s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 975/2813 [19:59<37:43,  1.23s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 975/2813 [20:10<37:43,  1.23s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 1000/2813 [20:29<37:11,  1.23s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 1000/2813 [20:40<37:11,  1.23s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 1025/2813 [21:00<36:41,  1.23s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 1025/2813 [21:20<36:41,  1.23s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 1050/2813 [21:31<36:09,  1.23s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 1050/2813 [21:50<36:09,  1.23s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 1075/2813 [22:02<35:38,  1.23s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 1075/2813 [22:20<35:38,  1.23s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 1100/2813 [22:32<35:06,  1.23s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 1100/2813 [22:50<35:06,  1.23s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 1125/2813 [23:03<34:37,  1.23s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 1125/2813 [23:20<34:37,  1.23s/it]\u001b[A\n",
      "Iteration:  41%|████      | 1150/2813 [23:34<34:04,  1.23s/it]\u001b[A\n",
      "Iteration:  41%|████      | 1150/2813 [23:50<34:04,  1.23s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 1175/2813 [24:05<33:33,  1.23s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 1175/2813 [24:20<33:33,  1.23s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 1200/2813 [24:35<33:02,  1.23s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 1200/2813 [24:50<33:02,  1.23s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 1225/2813 [25:06<32:31,  1.23s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 1225/2813 [25:20<32:31,  1.23s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 1250/2813 [25:37<32:00,  1.23s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 1250/2813 [25:50<32:00,  1.23s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 1275/2813 [26:07<31:30,  1.23s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 1275/2813 [26:20<31:30,  1.23s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 1300/2813 [26:38<30:58,  1.23s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 1300/2813 [26:50<30:58,  1.23s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 1325/2813 [27:09<30:28,  1.23s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 1325/2813 [27:20<30:28,  1.23s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 1350/2813 [27:40<29:58,  1.23s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 1350/2813 [28:00<29:58,  1.23s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 1375/2813 [28:10<29:27,  1.23s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 1375/2813 [28:30<29:27,  1.23s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 1400/2813 [28:41<28:56,  1.23s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 1400/2813 [29:00<28:56,  1.23s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 1425/2813 [29:12<28:26,  1.23s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 1425/2813 [29:30<28:26,  1.23s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 1450/2813 [29:43<27:55,  1.23s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 1450/2813 [30:00<27:55,  1.23s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 1475/2813 [30:13<27:25,  1.23s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 1475/2813 [30:30<27:25,  1.23s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 1500/2813 [30:44<26:55,  1.23s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 1500/2813 [31:00<26:55,  1.23s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 1525/2813 [31:15<26:25,  1.23s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 1525/2813 [31:30<26:25,  1.23s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 1550/2813 [31:46<25:54,  1.23s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 1550/2813 [32:00<25:54,  1.23s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 1575/2813 [32:17<25:24,  1.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  56%|█████▌    | 1575/2813 [32:30<25:24,  1.23s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 1600/2813 [32:47<24:52,  1.23s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 1600/2813 [33:00<24:52,  1.23s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 1625/2813 [33:18<24:22,  1.23s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 1625/2813 [33:30<24:22,  1.23s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 1650/2813 [33:49<23:50,  1.23s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 1650/2813 [34:00<23:50,  1.23s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 1675/2813 [34:20<23:19,  1.23s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 1675/2813 [34:30<23:19,  1.23s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 1700/2813 [34:50<22:48,  1.23s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 1700/2813 [35:10<22:48,  1.23s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 1725/2813 [35:21<22:17,  1.23s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 1725/2813 [35:40<22:17,  1.23s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1750/2813 [35:52<21:45,  1.23s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1750/2813 [36:10<21:45,  1.23s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1775/2813 [36:22<21:14,  1.23s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1775/2813 [36:40<21:14,  1.23s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1800/2813 [36:53<20:43,  1.23s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1800/2813 [37:10<20:43,  1.23s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1825/2813 [37:24<20:12,  1.23s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1825/2813 [37:40<20:12,  1.23s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1850/2813 [37:54<19:42,  1.23s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1850/2813 [38:10<19:42,  1.23s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1875/2813 [38:25<19:11,  1.23s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1875/2813 [38:40<19:11,  1.23s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1900/2813 [38:56<18:41,  1.23s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1900/2813 [39:10<18:41,  1.23s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1925/2813 [39:27<18:11,  1.23s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1925/2813 [39:40<18:11,  1.23s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1950/2813 [39:57<17:40,  1.23s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1950/2813 [40:10<17:40,  1.23s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 1975/2813 [40:28<17:09,  1.23s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 1975/2813 [40:40<17:09,  1.23s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 2000/2813 [40:59<16:38,  1.23s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 2000/2813 [41:10<16:38,  1.23s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 2025/2813 [41:29<16:08,  1.23s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 2025/2813 [41:40<16:08,  1.23s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 2050/2813 [42:00<15:37,  1.23s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 2050/2813 [42:20<15:37,  1.23s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 2075/2813 [42:31<15:07,  1.23s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 2075/2813 [42:50<15:07,  1.23s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 2100/2813 [43:02<14:36,  1.23s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 2100/2813 [43:20<14:36,  1.23s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 2125/2813 [43:32<14:06,  1.23s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 2125/2813 [43:50<14:06,  1.23s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 2150/2813 [44:03<13:35,  1.23s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 2150/2813 [44:20<13:35,  1.23s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 2175/2813 [44:34<13:04,  1.23s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 2175/2813 [44:50<13:04,  1.23s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 2200/2813 [45:05<12:33,  1.23s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 2200/2813 [45:20<12:33,  1.23s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 2225/2813 [45:35<12:03,  1.23s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 2225/2813 [45:50<12:03,  1.23s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 2250/2813 [46:06<11:31,  1.23s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 2250/2813 [46:20<11:31,  1.23s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 2275/2813 [46:37<11:00,  1.23s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 2275/2813 [46:50<11:00,  1.23s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 2300/2813 [47:07<10:29,  1.23s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 2300/2813 [47:20<10:29,  1.23s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 2325/2813 [47:38<09:59,  1.23s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 2325/2813 [47:50<09:59,  1.23s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 2350/2813 [48:09<09:28,  1.23s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 2350/2813 [48:20<09:28,  1.23s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 2375/2813 [48:39<08:57,  1.23s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 2375/2813 [48:50<08:57,  1.23s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 2400/2813 [49:10<08:26,  1.23s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 2400/2813 [49:30<08:26,  1.23s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 2425/2813 [49:41<07:56,  1.23s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 2425/2813 [50:00<07:56,  1.23s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 2450/2813 [50:12<07:25,  1.23s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 2450/2813 [50:30<07:25,  1.23s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 2475/2813 [50:42<06:55,  1.23s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 2475/2813 [51:00<06:55,  1.23s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 2500/2813 [51:13<06:24,  1.23s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 2500/2813 [51:30<06:24,  1.23s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 2525/2813 [51:44<05:53,  1.23s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 2525/2813 [52:00<05:53,  1.23s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 2550/2813 [52:14<05:23,  1.23s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 2550/2813 [52:30<05:23,  1.23s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 2575/2813 [52:45<04:52,  1.23s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 2575/2813 [53:00<04:52,  1.23s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 2600/2813 [53:16<04:21,  1.23s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 2600/2813 [53:30<04:21,  1.23s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 2625/2813 [53:47<03:51,  1.23s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 2625/2813 [54:00<03:51,  1.23s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 2650/2813 [54:17<03:20,  1.23s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 2650/2813 [54:30<03:20,  1.23s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 2675/2813 [54:48<02:49,  1.23s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 2675/2813 [55:00<02:49,  1.23s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 2700/2813 [55:19<02:19,  1.23s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 2700/2813 [55:30<02:19,  1.23s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 2725/2813 [55:50<01:48,  1.23s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 2725/2813 [56:10<01:48,  1.23s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 2750/2813 [56:21<01:17,  1.23s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 2750/2813 [56:40<01:17,  1.23s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 2775/2813 [56:51<00:46,  1.23s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 2775/2813 [57:10<00:46,  1.23s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 2800/2813 [57:22<00:15,  1.23s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [57:38<00:00, 3458.43s/it].23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07014742273803971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "token_classifier.fit(token_ids=train_token_ids, \n",
    "                     input_mask=train_input_mask, \n",
    "                     labels=train_label_ids,\n",
    "                     num_epochs=NUM_TRAIN_EPOCHS, \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Test Data\n",
    "The `predict` method of the token classifier optionally returns the softmax probability of the predicted class, which is a NxM array, where N is size of the testing data and M is the number of tokens in the testing sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Iteration:   0%|          | 0/216 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Only 1 CUDA device is available. Data parallelism is not possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 216/216 [01:28<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 0.03504765126746389\n",
      "Sample predicted probabilities:\n",
      "[0.9922516  0.9720795  0.8725999  0.7776708  0.8586385  0.91872704\n",
      " 0.9996006  0.9998628  0.99951696 0.99942696 0.9973278  0.9959351\n",
      " 0.9978994  0.9984732  0.9971475  0.9968125  0.99599177 0.99230534\n",
      " 0.98520017 0.9921949  0.9941393  0.9965797  0.9982722  0.9959889\n",
      " 0.998398   0.9981147  0.9996132  0.99947053 0.9987999  0.9997645\n",
      " 0.9997558  0.9998029  0.9998293  0.99985814 0.9997954  0.9994717\n",
      " 0.9992329  0.9998041  0.99953985 0.99959916 0.9955279  0.98726255\n",
      " 0.9762349  0.97292095 0.9446313  0.9501264  0.94189286 0.9638973\n",
      " 0.9649065  0.98749185 0.99119383 0.99357057 0.99253017 0.9919087\n",
      " 0.9881097  0.9850428  0.9849273  0.9764776  0.9668529  0.9686475\n",
      " 0.96223867 0.9669822  0.98228574 0.9906934  0.99391055 0.99365366\n",
      " 0.9967108  0.99211806 0.9812552  0.98241097 0.9620526  0.98186976\n",
      " 0.98395514 0.98158413 0.9779302  0.95493126 0.9570392  0.9544368\n",
      " 0.96813667 0.9672687  0.9663258  0.9765286  0.98573685 0.9870762\n",
      " 0.9936091  0.9940317  0.99791735 0.99687296 0.9971859  0.9972082\n",
      " 0.9968747  0.997393   0.9962565  0.9978744  0.99607915 0.99727863\n",
      " 0.99870515 0.9978684  0.99538815 0.98852646 0.98247313 0.9764449\n",
      " 0.9818155  0.97462183 0.97708374 0.94641095 0.9839552  0.98784244\n",
      " 0.9943922  0.9969235  0.9922571  0.99347633 0.93547153 0.970549\n",
      " 0.983114   0.9714504  0.97813374 0.9756016  0.9756758  0.9844542\n",
      " 0.9544812  0.9531884  0.94890225 0.96054167 0.95855224 0.9649013\n",
      " 0.9686859  0.9646467  0.97077954 0.96501786 0.9858531  0.9879219\n",
      " 0.97912836 0.98094946 0.97730654 0.9665618  0.9449111  0.98184603\n",
      " 0.96909297 0.9828242  0.99211085 0.982251   0.98121053 0.9826109\n",
      " 0.96345145 0.96129507 0.94783396 0.9486903  0.9622593  0.9838395\n",
      " 0.9814006  0.9904428  0.9925829  0.9911281  0.9957289  0.99171066\n",
      " 0.99489754 0.98259616 0.99336535 0.9958482  0.99561536 0.9935796\n",
      " 0.99078524 0.99494463 0.99474686 0.99455655 0.9910732  0.99521387\n",
      " 0.988351   0.989589   0.98032206 0.9965029  0.9978023  0.9941559\n",
      " 0.9941354  0.9938877  0.99896574 0.9990275  0.9984664  0.9961593\n",
      " 0.9933113  0.97690433 0.97582495 0.9613233  0.93561184 0.96972376\n",
      " 0.9259933  0.96845776 0.9823611  0.9540374  0.9465987  0.9449947\n",
      " 0.95694584 0.9538518  0.9639879  0.96973366 0.9692007  0.9704836\n",
      " 0.97443515 0.97360295]\n",
      "\n",
      "Sample predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = token_classifier.predict(token_ids=test_token_ids, \n",
    "                                       input_mask=test_input_mask, \n",
    "                                       labels=test_label_ids, \n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       probabilities=True)\n",
    "probabilities = predictions.probabilities\n",
    "pred_label_ids = predictions.classes\n",
    "print(\"Sample predicted probabilities:\\n{}\\n\".format(probabilities[5]))\n",
    "print(\"Sample predicted labels:\\n{}\\n\".format(pred_label_ids[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "The `predict` method of the token classifier outputs label ids for all tokens, including the padded tokens. `postprocess_token_labels` is a helper function that removes the predictions on padded tokens. If a `label_map` is provided, it maps the numerical label ids back to original token labels which are usually string type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC       0.92      0.93      0.92      2803\n",
      "      PER       0.93      0.90      0.91      1382\n",
      "      ORG       0.77      0.89      0.82      1296\n",
      "\n",
      "micro avg       0.88      0.91      0.89      5481\n",
      "macro avg       0.88      0.91      0.90      5481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_tags_no_padding = postprocess_token_labels(pred_label_ids, \n",
    "                                                test_input_mask, \n",
    "                                                label_map)\n",
    "true_tags_no_padding = postprocess_token_labels(test_label_ids, \n",
    "                                                test_input_mask, \n",
    "                                                label_map)\n",
    "print(classification_report(true_tags_no_padding, pred_tags_no_padding, digits=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_pytorch",
   "language": "python",
   "name": "nlp_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
