{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*  \n",
    "*Licensed under the MIT License.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before You Start\n",
    "\n",
    "The running time shown in this notebook is on a Standard_NC24s_v3 Azure Deep Learning Virtual Machine with 4 NVIDIA Tesla V100 GPUs. If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`** to run the notebook on a small subset of the data and a smaller number of epochs. \n",
    "The table below provides some reference running time on different machine configurations.  \n",
    "\n",
    "|QUICK_RUN|Machine Configurations|Running time|\n",
    "|:---------|:----------------------|:------------|\n",
    "|True|4 **CPU**s, 14GB memory| ~ 2 minutes|\n",
    "|False|4 **CPU**s, 14GB memory| ~1.5 hours|\n",
    "|True|1 NVIDIA Tesla K80 GPUs, 12GB GPU memory| |\n",
    "|False|1 NVIDIA Tesla K80 GPUs, 12GB GPU memory| |\n",
    "\n",
    "If you run into CUDA out-of-memory error, try reducing the `BATCH_SIZE` and `MAX_SEQ_LENGTH`, but note that model performance will be compromised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "startTime = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = False\n",
    "\n",
    "TRAIN_DATA_USED_PERCENT = 1\n",
    "TEST_DATA_USED_PERCENT = 1\n",
    "NUM_TRAIN_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUICK_RUN:\n",
    "    TRAIN_DATA_USED_PERCENT = 0.1\n",
    "    TEST_DATA_USED_PERCENT = 0.1\n",
    "    NUM_TRAIN_EPOCHS = 1\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = 16\n",
    "else:\n",
    "    BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This notebook demonstrates how to fine tune [pretrained BERT model](https://github.com/huggingface/pytorch-pretrained-BERT) for named entity recognition (NER) task. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, and model evaluation. \n",
    "\n",
    "[BERT (Bidirectional Transformers for Language Understanding)](https://arxiv.org/pdf/1810.04805.pdf) is a powerful pre-trained lanaguage model that can be used for multiple NLP tasks, including text classification, question answering, named entity recognition, etc. It's able to achieve state of the art performance with only a few epochs of fine tuning on task specific datasets.  \n",
    "The figure below illustrates how BERT can be fine tuned for NER tasks. The input data is a list of tokens representing a sentence. In the training data, each token has an entity label. After fine tuning, the model predicts an entity label for each token in a given testing sentence. \n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/bert_architecture.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "nlp_path = os.path.abspath('../../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "\n",
    "from utils_nlp.models.bert.token_classification import BERTTokenClassifier, create_label_map, postprocess_token_labels\n",
    "from utils_nlp.models.bert.common import Language, Tokenizer\n",
    "from utils_nlp.dataset.wikigold import load_train_test_dfs, get_unique_labels\n",
    "from utils_nlp.common.timer import Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path configuration\n",
    "CACHE_DIR=\"./temp\"\n",
    "\n",
    "# set random seeds\n",
    "RANDOM_SEED = 100\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# model configurations\n",
    "LANGUAGE = Language.ENGLISHCASED\n",
    "DO_LOWER_CASE = False\n",
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "# optimizer configuration\n",
    "LEARNING_RATE = 3e-5\n",
    "\n",
    "# data configurations\n",
    "TEXT_COL = \"sentence\"\n",
    "LABELS_COL = \"labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and testing data\n",
    "The dataset used in this notebook is the [wikigold dataset](https://www.aclweb.org/anthology/W09-3302). The wikigold dataset consists of 145 mannually labelled Wikipedia articles, including 1841 sentences and 40k tokens in total. The dataset can be directly downloaded from [here](https://github.com/juand-r/entity-recognition-datasets/tree/master/data/wikigold). \n",
    "\n",
    "The helper function `load_train_test_dfs` downloads the data file if it doesn't exist in `local_cache_path`. It splits the dataset into training and testing sets according to `test_percentage`. Because this is a relatively small dataset, we set `test_percentage` to 0.5 in order to have enough data for model evaluation. Running this notebook multiple times with different random seeds produces similar results.   \n",
    "\n",
    "The helper function `get_unique_labels` returns the unique entity labels in the dataset. There are 5 unique labels in the   original dataset: 'O' (non-entity), 'I-LOC' (location), 'I-MISC' (miscellaneous), 'I-PER' (person), and 'I-ORG' (organization). \n",
    "\n",
    "The maximum number of words in a sentence is 144, so we set MAX_SEQ_LENGTH to 200 above, because the number of tokens will grow after WordPiece tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length in the  data is: 144\n",
      "\n",
      "Unique entity labels: \n",
      "['O', 'I-LOC', 'I-MISC', 'I-PER', 'I-ORG']\n",
      "\n",
      "Sample sentence: \n",
      "['Two', ',', 'Samsung', 'based', ',', 'electronic', 'cash', 'registers', 'were', 'reconstructed', 'in', 'order', 'to', 'expand', 'their', 'functions', 'and', 'adapt', 'them', 'for', 'networking', '.']\n",
      "\n",
      "Sample sentence labels: \n",
      "['O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_train_test_dfs(local_cache_path=CACHE_DIR, test_percentage=0.5,random_seed=RANDOM_SEED)\n",
    "label_list = get_unique_labels()\n",
    "print('\\nUnique entity labels: \\n{}\\n'.format(label_list))\n",
    "print('Sample sentence: \\n{}\\n'.format(train_df[TEXT_COL][0]))\n",
    "print('Sample sentence labels: \\n{}\\n'.format(train_df[LABELS_COL][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=TRAIN_DATA_USED_PERCENT).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=TEST_DATA_USED_PERCENT).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the input text are lists of words instead of raw sentences. This format ensures matching between input words and token labels when the words are further tokenized by Tokenizer.tokenize_ner.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dictionary that maps labels to numerical values**  \n",
    "Note there is an argument called `trailing_piece_tag`. BERT uses a WordPiece tokenizer which breaks down some words into multiple tokens, e.g. \"criticize\" is tokenized into \"critic\" and \"##ize\". Since the input data only come with one token label for \"criticize\", within Tokenizer.prerocess_ner_tokens, the original token label is assigned to the first token \"critic\" and the second token \"##ize\" is labeled as \"X\". By default, `trailing_piece_tag` is set to \"X\". If \"X\" already exists in your data, you can set `trailing_piece_tag` to another value that doesn't exist in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_map = create_label_map(label_list, trailing_piece_tag=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(language=LANGUAGE, \n",
    "                      to_lower=DO_LOWER_CASE, \n",
    "                      cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize and preprocess text**  \n",
    "The `tokenize_ner` method of the `Tokenizer` class converts text and labels in strings to numerical features, involving the following steps:\n",
    "1. WordPiece tokenization.\n",
    "2. Convert tokens and labels to numerical values, i.e. token ids and label ids.\n",
    "3. Sequence padding or truncation according to the `max_seq_length` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_token_ids, train_input_mask, train_trailing_token_mask, train_label_ids = \\\n",
    "    tokenizer.tokenize_ner(text=train_df[TEXT_COL],\n",
    "                           label_map=label_map,\n",
    "                           max_len=MAX_SEQ_LENGTH,\n",
    "                           labels=train_df[LABELS_COL],\n",
    "                           trailing_piece_tag=\"X\")\n",
    "test_token_ids, test_input_mask, test_trailing_token_mask, test_label_ids = \\\n",
    "    tokenizer.tokenize_ner(text=test_df[TEXT_COL],\n",
    "                           label_map=label_map,\n",
    "                           max_len=MAX_SEQ_LENGTH,\n",
    "                           labels=test_df[LABELS_COL],\n",
    "                           trailing_piece_tag=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokenizer.tokenize_ner` outputs three or four lists of numerical features lists, each sublist contains features of an input sentence: \n",
    "1. token ids: list of numerical values each corresponds to a token.\n",
    "2. attention mask: list of 1s and 0s, 1 for input tokens and 0 for padded tokens, so that padded tokens are not attended to. \n",
    "3. trailing word piece mask: boolean list, `True` for the first word piece of each original word, `False` for the trailing word pieces, e.g. ##ize. This mask is useful for removing predictions on trailing word pieces, so that each original word in the input text has a unique predicted label. \n",
    "4. label ids: list of numerical values each corresponds to an entity label, if `labels` is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample token ids:\n",
      "[107, 1124, 1674, 183, 112, 189, 1541, 1474, 1155, 1115, 1277, 1133, 3093, 1106, 1243, 1103, 2261, 1694, 1268, 119, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample attention mask:\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample trailing token mask:\n",
      "[True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "\n",
      "Sample label ids:\n",
      "[0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample token ids:\\n{}\\n\".format(train_token_ids[0]))\n",
    "print(\"Sample attention mask:\\n{}\\n\".format(train_input_mask[0]))\n",
    "print(\"Sample trailing token mask:\\n{}\\n\".format(train_trailing_token_mask[0]))\n",
    "print(\"Sample label ids:\\n{}\\n\".format(train_label_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Token Classifier\n",
    "The value of the `language` argument determines which BERT model is used:\n",
    "* Language.ENGLISH: \"bert-base-uncased\"\n",
    "* Language.ENGLISHCASED: \"bert-base-cased\"\n",
    "* Language.ENGLISHLARGE: \"bert-large-uncased\"\n",
    "* Language.ENGLISHLARGECASED: \"bert-large-cased\"\n",
    "* Language.CHINESE: \"bert-base-chinese\"\n",
    "* Language.MULTILINGUAL: \"bert-base-multilingual-cased\"\n",
    "\n",
    "Here we use the base, cased pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_classifier = BERTTokenClassifier(language=LANGUAGE,\n",
    "                                       num_labels=len(label_map),\n",
    "                                       cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/116 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 4/116 [00:34<16:02,  8.59s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/116 [00:50<16:02,  8.59s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/116 [01:08<15:26,  8.57s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/116 [01:20<15:26,  8.57s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/116 [01:41<14:40,  8.47s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/116 [02:00<14:40,  8.47s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/116 [02:14<13:58,  8.38s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/116 [02:30<13:58,  8.38s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/116 [02:47<13:22,  8.35s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/116 [03:00<13:22,  8.35s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/116 [03:20<12:47,  8.35s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/116 [03:40<12:47,  8.35s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/116 [03:54<12:19,  8.40s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/116 [04:10<12:19,  8.40s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/116 [04:28<11:45,  8.40s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/116 [04:40<11:45,  8.40s/it]\u001b[A\n",
      "Iteration:  31%|███       | 36/116 [05:01<11:06,  8.34s/it]\u001b[A\n",
      "Iteration:  31%|███       | 36/116 [05:20<11:06,  8.34s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 40/116 [05:33<10:30,  8.30s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 40/116 [05:50<10:30,  8.30s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 44/116 [06:07<09:57,  8.30s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 44/116 [06:20<09:57,  8.30s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 48/116 [06:39<09:22,  8.27s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 48/116 [06:50<09:22,  8.27s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 52/116 [07:12<08:47,  8.24s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 52/116 [07:30<08:47,  8.24s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 56/116 [07:44<08:11,  8.20s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 56/116 [08:00<08:11,  8.20s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 60/116 [08:17<07:38,  8.19s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 60/116 [08:30<07:38,  8.19s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 64/116 [08:50<07:06,  8.21s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 64/116 [09:10<07:06,  8.21s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 68/116 [09:23<06:34,  8.22s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 68/116 [09:40<06:34,  8.22s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 72/116 [09:55<06:00,  8.18s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 72/116 [10:10<06:00,  8.18s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 76/116 [10:29<05:30,  8.27s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 76/116 [10:40<05:30,  8.27s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 80/116 [11:02<04:56,  8.24s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 80/116 [11:20<04:56,  8.24s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 84/116 [11:35<04:24,  8.25s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 84/116 [11:50<04:24,  8.25s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 88/116 [12:09<03:52,  8.31s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 88/116 [12:20<03:52,  8.31s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 92/116 [12:42<03:19,  8.30s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 92/116 [13:00<03:19,  8.30s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 96/116 [13:15<02:45,  8.25s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 96/116 [13:30<02:45,  8.25s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 100/116 [13:48<02:12,  8.25s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 100/116 [14:00<02:12,  8.25s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 104/116 [14:21<01:39,  8.27s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 104/116 [14:40<01:39,  8.27s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 108/116 [14:54<01:06,  8.26s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 108/116 [15:10<01:06,  8.26s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 112/116 [15:27<00:33,  8.29s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 112/116 [15:40<00:33,  8.29s/it]\u001b[A\n",
      "Epoch:  20%|██        | 1/5 [15:54<1:03:36, 954.24s/it]s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/116 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3545071521677591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   3%|▎         | 4/116 [00:32<15:20,  8.22s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/116 [00:45<15:20,  8.22s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/116 [01:06<14:51,  8.25s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/116 [01:25<14:51,  8.25s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/116 [01:39<14:17,  8.25s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/116 [01:55<14:17,  8.25s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/116 [02:12<13:45,  8.26s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/116 [02:25<13:45,  8.26s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/116 [02:45<13:11,  8.24s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/116 [02:55<13:11,  8.24s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/116 [03:17<12:36,  8.22s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/116 [03:35<12:36,  8.22s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/116 [03:51<12:05,  8.25s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/116 [04:05<12:05,  8.25s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/116 [04:24<11:33,  8.26s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/116 [04:35<11:33,  8.26s/it]\u001b[A\n",
      "Iteration:  31%|███       | 36/116 [04:57<11:02,  8.28s/it]\u001b[A\n",
      "Iteration:  31%|███       | 36/116 [05:15<11:02,  8.28s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 40/116 [05:30<10:31,  8.31s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 40/116 [05:45<10:31,  8.31s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 44/116 [06:03<09:56,  8.29s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 44/116 [06:15<09:56,  8.29s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 48/116 [06:36<09:21,  8.26s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 48/116 [06:55<09:21,  8.26s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 52/116 [07:10<08:49,  8.28s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 52/116 [07:25<08:49,  8.28s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 56/116 [07:43<08:18,  8.30s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 56/116 [07:55<08:18,  8.30s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 60/116 [08:16<07:43,  8.28s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 60/116 [08:35<07:43,  8.28s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 64/116 [08:51<07:16,  8.40s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 64/116 [09:05<07:16,  8.40s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 68/116 [09:26<06:51,  8.57s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 68/116 [09:45<06:51,  8.57s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 72/116 [10:03<06:24,  8.75s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 72/116 [10:15<06:24,  8.75s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 76/116 [10:40<05:54,  8.85s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 76/116 [10:55<05:54,  8.85s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 80/116 [11:16<05:21,  8.94s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 80/116 [11:35<05:21,  8.94s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 84/116 [11:52<04:46,  8.94s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 84/116 [12:05<04:46,  8.94s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 88/116 [12:29<04:13,  9.07s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 88/116 [12:45<04:13,  9.07s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 92/116 [13:06<03:39,  9.13s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 92/116 [13:25<03:39,  9.13s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 96/116 [13:42<03:00,  9.05s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 96/116 [13:56<03:00,  9.05s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 100/116 [14:19<02:26,  9.13s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 100/116 [14:36<02:26,  9.13s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 104/116 [14:57<01:50,  9.22s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 104/116 [15:16<01:50,  9.22s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 108/116 [15:32<01:12,  9.08s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 108/116 [15:46<01:12,  9.08s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 112/116 [16:08<00:36,  9.04s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 112/116 [16:26<00:36,  9.04s/it]\u001b[A\n",
      "Epoch:  40%|████      | 2/5 [32:31<48:21, 967.24s/it]  s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/116 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10110071701286681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   3%|▎         | 4/116 [00:37<17:36,  9.43s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/116 [00:48<17:36,  9.43s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/116 [01:13<16:42,  9.28s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/116 [01:28<16:42,  9.28s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/116 [01:49<15:56,  9.19s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/116 [02:08<15:56,  9.19s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/116 [02:24<15:09,  9.10s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/116 [02:38<15:09,  9.10s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/116 [03:00<14:26,  9.02s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/116 [03:18<14:26,  9.02s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/116 [03:36<13:52,  9.05s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/116 [03:48<13:52,  9.05s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/116 [04:12<13:11,  8.99s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/116 [04:28<13:11,  8.99s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/116 [04:45<12:17,  8.78s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/116 [04:58<12:17,  8.78s/it]\u001b[A\n",
      "Iteration:  31%|███       | 36/116 [05:18<11:29,  8.62s/it]\u001b[A\n",
      "Iteration:  31%|███       | 36/116 [05:28<11:29,  8.62s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 39/116 [05:54<12:21,  9.62s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 39/116 [06:08<12:21,  9.62s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 43/116 [06:27<11:16,  9.27s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 43/116 [06:38<11:16,  9.27s/it]\u001b[A\n",
      "Iteration:  41%|████      | 47/116 [07:00<10:18,  8.96s/it]\u001b[A\n",
      "Iteration:  41%|████      | 47/116 [07:18<10:18,  8.96s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 51/116 [07:33<09:27,  8.73s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 51/116 [07:48<09:27,  8.73s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 55/116 [08:07<08:47,  8.64s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 55/116 [08:18<08:47,  8.64s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 59/116 [08:41<08:09,  8.59s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 59/116 [08:58<08:09,  8.59s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 63/116 [09:15<07:32,  8.54s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 63/116 [09:28<07:32,  8.54s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 67/116 [09:48<06:55,  8.49s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 67/116 [09:58<06:55,  8.49s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 71/116 [10:22<06:21,  8.47s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 71/116 [10:38<06:21,  8.47s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 75/116 [10:55<05:46,  8.44s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 75/116 [11:08<05:46,  8.44s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 79/116 [11:28<05:08,  8.35s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 79/116 [11:38<05:08,  8.35s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 83/116 [12:00<04:34,  8.31s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 83/116 [12:18<04:34,  8.31s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 87/116 [12:35<04:03,  8.38s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 87/116 [12:48<04:03,  8.38s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 91/116 [13:10<03:32,  8.52s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 91/116 [13:28<03:32,  8.52s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 95/116 [13:49<03:06,  8.86s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 95/116 [14:08<03:06,  8.86s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 99/116 [14:22<02:28,  8.73s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 99/116 [14:38<02:28,  8.73s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 103/116 [14:58<01:54,  8.78s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 103/116 [15:08<01:54,  8.78s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 107/116 [15:31<01:17,  8.60s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 107/116 [15:48<01:17,  8.60s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 111/116 [16:04<00:42,  8.54s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 111/116 [16:18<00:42,  8.54s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 115/116 [16:37<00:08,  8.45s/it]\u001b[A\n",
      "Epoch:  60%|██████    | 3/5 [49:11<32:33, 976.98s/it]62s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/116 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05429209667611225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   3%|▎         | 4/116 [00:32<15:05,  8.09s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/116 [00:48<15:05,  8.09s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/116 [01:04<14:34,  8.10s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/116 [01:18<14:34,  8.10s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/116 [01:37<14:05,  8.13s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/116 [01:48<14:05,  8.13s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/116 [02:10<13:33,  8.14s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/116 [02:28<13:33,  8.14s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/116 [02:42<13:00,  8.13s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/116 [02:58<13:00,  8.13s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/116 [03:16<12:33,  8.19s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/116 [03:28<12:33,  8.19s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/116 [03:48<12:01,  8.20s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/116 [04:08<12:01,  8.20s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/116 [04:21<11:29,  8.21s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/116 [04:38<11:29,  8.21s/it]\u001b[A\n",
      "Iteration:  31%|███       | 36/116 [04:54<10:57,  8.22s/it]\u001b[A\n",
      "Iteration:  31%|███       | 36/116 [05:08<10:57,  8.22s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 40/116 [05:27<10:24,  8.22s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 40/116 [05:38<10:24,  8.22s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 44/116 [06:00<09:51,  8.22s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 44/116 [06:18<09:51,  8.22s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 48/116 [06:35<09:31,  8.40s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 48/116 [06:48<09:31,  8.40s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 52/116 [07:08<08:55,  8.36s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 52/116 [07:28<08:55,  8.36s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 56/116 [07:42<08:21,  8.35s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 56/116 [07:58<08:21,  8.35s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 60/116 [08:16<07:50,  8.41s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 60/116 [08:28<07:50,  8.41s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 64/116 [08:52<07:24,  8.56s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 64/116 [09:08<07:24,  8.56s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 68/116 [09:28<06:58,  8.73s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 68/116 [09:38<06:58,  8.73s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 72/116 [10:04<06:27,  8.82s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 72/116 [10:18<06:27,  8.82s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 76/116 [10:40<05:52,  8.82s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 76/116 [10:58<05:52,  8.82s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 80/116 [11:15<05:19,  8.87s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 80/116 [11:29<05:19,  8.87s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 84/116 [11:51<04:43,  8.87s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 84/116 [12:09<04:43,  8.87s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 88/116 [12:27<04:08,  8.88s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 88/116 [12:39<04:08,  8.88s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 92/116 [13:03<03:35,  8.97s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 92/116 [13:19<03:35,  8.97s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 96/116 [13:40<03:00,  9.04s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 96/116 [13:59<03:00,  9.04s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 100/116 [14:16<02:23,  8.99s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 100/116 [14:29<02:23,  8.99s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 104/116 [14:52<01:48,  9.01s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 104/116 [15:09<01:48,  9.01s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 108/116 [15:28<01:12,  9.01s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 108/116 [15:39<01:12,  9.01s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 112/116 [16:03<00:35,  8.98s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 112/116 [16:19<00:35,  8.98s/it]\u001b[A\n",
      "Epoch:  80%|████████  | 4/5 [1:05:44<16:21, 981.85s/it]s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/116 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03206114985193286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   3%|▎         | 4/116 [00:35<16:40,  8.94s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 4/116 [00:45<16:40,  8.94s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/116 [01:11<16:03,  8.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/116 [01:25<16:03,  8.92s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/116 [01:46<15:25,  8.90s/it]\u001b[A\n",
      "Iteration:  10%|█         | 12/116 [02:05<15:25,  8.90s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/116 [02:23<14:56,  8.96s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 16/116 [02:35<14:56,  8.96s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/116 [02:59<14:23,  8.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/116 [03:15<14:23,  8.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/116 [03:35<13:49,  9.02s/it]\u001b[A\n",
      "Iteration:  21%|██        | 24/116 [03:45<13:49,  9.02s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/116 [04:11<13:14,  9.03s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 28/116 [04:25<13:14,  9.03s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/116 [04:47<12:37,  9.02s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 32/116 [05:05<12:37,  9.02s/it]\u001b[A\n",
      "Iteration:  31%|███       | 36/116 [05:24<12:02,  9.03s/it]\u001b[A\n",
      "Iteration:  31%|███       | 36/116 [05:35<12:02,  9.03s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 40/116 [06:00<11:25,  9.02s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 40/116 [06:15<11:25,  9.02s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 44/116 [06:36<10:54,  9.09s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 44/116 [06:55<10:54,  9.09s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 48/116 [07:13<10:17,  9.08s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 48/116 [07:25<10:17,  9.08s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 52/116 [07:49<09:41,  9.08s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 52/116 [08:05<09:41,  9.08s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 56/116 [08:25<09:02,  9.05s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 56/116 [08:35<09:02,  9.05s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 60/116 [09:01<08:27,  9.06s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 60/116 [09:15<08:27,  9.06s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 64/116 [09:37<07:50,  9.05s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 64/116 [09:55<07:50,  9.05s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 68/116 [10:13<07:13,  9.03s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 68/116 [10:25<07:13,  9.03s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 72/116 [10:49<06:37,  9.03s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 72/116 [11:05<06:37,  9.03s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 76/116 [11:25<05:59,  9.00s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 76/116 [11:35<05:59,  9.00s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 80/116 [11:58<05:15,  8.77s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 80/116 [12:15<05:15,  8.77s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 84/116 [12:31<04:36,  8.64s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 84/116 [12:45<04:36,  8.64s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 88/116 [13:04<03:57,  8.49s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 88/116 [13:15<03:57,  8.49s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 92/116 [13:37<03:21,  8.42s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 92/116 [13:55<03:21,  8.42s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 96/116 [14:10<02:47,  8.36s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 96/116 [14:26<02:47,  8.36s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 100/116 [14:43<02:13,  8.33s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 100/116 [14:56<02:13,  8.33s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 104/116 [15:16<01:39,  8.29s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 104/116 [15:36<01:39,  8.29s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 108/116 [15:49<01:06,  8.30s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 108/116 [16:06<01:06,  8.30s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 112/116 [16:22<00:33,  8.28s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 112/116 [16:36<00:33,  8.28s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 5/5 [1:22:33<00:00, 990.00s/it]s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.023472348440855998\n",
      "Training time : 1.376 hrs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    token_classifier.fit(token_ids=train_token_ids, \n",
    "                         input_mask=train_input_mask, \n",
    "                         labels=train_label_ids,\n",
    "                         num_epochs=NUM_TRAIN_EPOCHS, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         learning_rate=LEARNING_RATE)\n",
    "print(\"Training time : {:.3f} hrs\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 115/115 [03:23<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 0.11518098939939038\n",
      "Prediction time : 0.056 hrs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    pred_label_ids = token_classifier.predict(token_ids=test_token_ids, \n",
    "                                              input_mask=test_input_mask, \n",
    "                                              labels=test_label_ids, \n",
    "                                              batch_size=BATCH_SIZE)\n",
    "print(\"Prediction time : {:.3f} hrs\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "The `predict` method of the token classifier outputs label ids for all tokens, including the padded tokens. `postprocess_token_labels` is a helper function that removes the predictions on padded tokens. If a `label_map` is provided, it maps the numerical label ids back to original token labels which are usually string type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "        X       0.96      0.97      0.96      1983\n",
      "      ORG       0.73      0.71      0.72       538\n",
      "      LOC       0.85      0.90      0.87       543\n",
      "      PER       0.95      0.93      0.94       550\n",
      "     MISC       0.61      0.84      0.71       396\n",
      "\n",
      "micro avg       0.87      0.91      0.89      4010\n",
      "macro avg       0.88      0.91      0.89      4010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_tags_no_padding = postprocess_token_labels(pred_label_ids, \n",
    "                                                test_input_mask, \n",
    "                                                label_map)\n",
    "true_tags_no_padding = postprocess_token_labels(test_label_ids, \n",
    "                                                test_input_mask, \n",
    "                                                label_map)\n",
    "print(classification_report(true_tags_no_padding, pred_tags_no_padding, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`postprocess_token_labels` also provides an option to remove the predictions on trailing word pieces, e.g. ##ize, so that the final predicted labels correspond to the original words in the input text. The `trailing_token_mask` is obtained from `tokenizer.tokenize_ner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      ORG       0.68      0.70      0.69       442\n",
      "      LOC       0.85      0.90      0.87       503\n",
      "      PER       0.94      0.92      0.93       455\n",
      "     MISC       0.60      0.82      0.69       344\n",
      "\n",
      "micro avg       0.75      0.84      0.79      1744\n",
      "macro avg       0.78      0.84      0.81      1744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_tags_no_padding_no_trailing = postprocess_token_labels(pred_label_ids, \n",
    "                                                            test_input_mask, \n",
    "                                                            label_map, \n",
    "                                                            remove_trailing_word_pieces=True, \n",
    "                                                            trailing_token_mask=test_trailing_token_mask)\n",
    "true_tags_no_padding_no_trailing = postprocess_token_labels(test_label_ids, \n",
    "                                                            test_input_mask, \n",
    "                                                            label_map, \n",
    "                                                            remove_trailing_word_pieces=True, \n",
    "                                                            trailing_token_mask=test_trailing_token_mask)\n",
    "print(classification_report(true_tags_no_padding_no_trailing, pred_tags_no_padding_no_trailing, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the metrics are worse after excluding trailing word pieces, because they are easy to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "By fine-tuning the pre-trained BERT model for token classification, we achieved significantly better results compared to the [original paper on the wikigold dataset](https://www.aclweb.org/anthology/W09-3302) with a much smaller training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:26:10.362535\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now() - startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_cpu",
   "language": "python",
   "name": "nlp_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
