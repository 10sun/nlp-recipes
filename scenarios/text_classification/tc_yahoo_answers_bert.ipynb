{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Text Classification of Yahoo Answers using BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import utils_nlp.dataset.yahoo_answers as ya_dataset\n",
    "from utils_nlp.eval.classification import eval_classification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../../../temp\"\n",
    "TRAIN_FILE = \"yahoo_answers_csv/train.csv\"\n",
    "TEST_FILE = \"yahoo_answers_csv/test.csv\"\n",
    "BERT_CACHE_DIR = \"../../../temp\"\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "UPDATE_EMBEDDINGS = False\n",
    "NUM_EPOCHS = 1\n",
    "NUM_ROWS_TRAIN = 100000  # number of training examples to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not os.path.isfile(os.path.join(DATA_FOLDER, TRAIN_FILE))) or (\n",
    "    not os.path.isfile(os.path.join(DATA_FOLDER, TEST_FILE))\n",
    "):\n",
    "    ya_dataset.download(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_train = ya_dataset.read_data(os.path.join(DATA_FOLDER, TRAIN_FILE), nrows=NUM_ROWS_TRAIN)\n",
    "df_test = ya_dataset.read_data(os.path.join(DATA_FOLDER, TEST_FILE), nrows=None)\n",
    "y_train = ya_dataset.get_labels(df_train)\n",
    "y_test = ya_dataset.get_labels(df_test)\n",
    "\n",
    "num_train_examples = df_train.shape[0]\n",
    "num_test_examples = df_test.shape[0]\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# clean/get text\n",
    "text_train = ya_dataset.clean_data(df_train)\n",
    "text_test = ya_dataset.clean_data(df_test)\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\", do_lower_case=True, cache_dir=BERT_CACHE_DIR\n",
    ")\n",
    "# tokenize and truncate\n",
    "tokens_train = [tokenizer.tokenize(x)[0 : MAX_LEN - 2] for x in text_train]\n",
    "tokens_test = [tokenizer.tokenize(x)[0 : MAX_LEN - 2] for x in text_test]\n",
    "\n",
    "# BERT format\n",
    "tokens_train = [[\"[CLS]\"] + x + [\"[SEP]\"] for x in tokens_train]\n",
    "tokens_test = [[\"[CLS]\"] + x + [\"[SEP]\"] for x in tokens_test]\n",
    "\n",
    "# convert tokens to ids\n",
    "tokens_train = [tokenizer.convert_tokens_to_ids(x) for x in tokens_train]\n",
    "tokens_test = [tokenizer.convert_tokens_to_ids(x) for x in tokens_test]\n",
    "\n",
    "# pad\n",
    "tokens_train = [x + [0] * (MAX_LEN - len(x)) for x in tokens_train]\n",
    "tokens_test = [x + [0] * (MAX_LEN - len(x)) for x in tokens_test]\n",
    "\n",
    "# create input mask\n",
    "input_mask_train = [[min(1, x) for x in y] for y in tokens_train]\n",
    "input_mask_test = [[min(1, x) for x in y] for y in tokens_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device_str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_str)\n",
    "print(\"using {} ...\".format(device_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", cache_dir=BERT_CACHE_DIR, num_labels=num_labels\n",
    ").to(device)\n",
    "\n",
    "# define loss function\n",
    "loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# define optimizer and model parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "opt = BertAdam(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether embedding layer is trainable\n",
    "if not UPDATE_EMBEDDINGS:\n",
    "    for p in model.bert.embeddings.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use multiple GPUs if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "print(\"using {} GPUs...\".format(torch.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model.train()\n",
    "num_batches = int(num_train_examples / BATCH_SIZE)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i in range(num_batches):\n",
    "        X_batch, mask_batch, y_batch = ya_dataset.get_batch_rnd(\n",
    "            tokens_train, input_mask_train, y_train, num_train_examples, BATCH_SIZE\n",
    "        )\n",
    "        X_batch = torch.tensor(X_batch, dtype=torch.long, device=device)\n",
    "        y_batch = torch.tensor(y_batch, dtype=torch.long, device=device)\n",
    "        mask_batch = torch.tensor(mask_batch, dtype=torch.long, device=device)\n",
    "        opt.zero_grad()\n",
    "        y_h = model(X_batch, None, mask_batch, labels=None)\n",
    "        loss = loss_func(y_h, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if i % int(0.01 * num_batches) == 0:\n",
    "            print(\n",
    "                \"epoch:{}/{}; batch:{}/{}; loss:{}\".format(\n",
    "                    epoch + 1, NUM_EPOCHS, i + 1, num_batches, loss.data\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "model.eval()\n",
    "preds = []\n",
    "for i in range(0, num_test_examples, BATCH_SIZE):\n",
    "    X_batch, mask_batch, y_batch = ya_dataset.get_batch_by_idx(\n",
    "        tokens_test, input_mask_test, y_test, i, BATCH_SIZE\n",
    "    )\n",
    "    X_batch = torch.tensor(X_batch, dtype=torch.long, device=device)\n",
    "    y_batch = torch.tensor(y_batch, dtype=torch.long, device=device)\n",
    "    mask_batch = torch.tensor(mask_batch, dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        p_batch = model(X_batch, None, mask_batch, labels=None)\n",
    "    preds.append(p_batch.cpu().data.numpy())\n",
    "\n",
    "preds = [x.argmax(1) for x in preds]\n",
    "preds = np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "eval_results = eval_classification(y_test, preds)\n",
    "print(\"accuracy: {}\".format(eval_results[\"accuracy\"]))\n",
    "print(\"precision: {}\".format(eval_results[\"precision\"]))\n",
    "print(\"recall: {}\".format(eval_results[\"recall\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
