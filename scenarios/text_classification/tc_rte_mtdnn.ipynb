{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Text Classification of RTE Sentences using MT-DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils_nlp.bert.common import Language\n",
    "from utils_nlp.dataset.glue import download_glue_data, load_pandas_df, build_data\n",
    "from utils_nlp.mtdnn.data_utils import label_map\n",
    "from utils_nlp.mtdnn.sequence_classification import MTDNNSequenceClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we fine-tune and evaluate a pretrained [MT-DNN](https://arxiv.org/abs/1901.11504) model on the [RTE](https://www.nyu.edu/projects/bowman/glue.pdf) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../../tmp\" #\"./temp\"\n",
    "BERT_CACHE_DIR = \"../../tmp\" #\"./temp\"\n",
    "LANGUAGE = Language.ENGLISH\n",
    "# TO_LOWER = True\n",
    "# MAX_LEN = 150\n",
    "# BATCH_SIZE = 32\n",
    "# NUM_GPUS = 2\n",
    "# NUM_EPOCHS = 1\n",
    "# TRAIN_SIZE = 0.6\n",
    "# LABEL_COL = \"genre\"\n",
    "# TEXT_COL = \"sentence1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "We start by loading a subset of the data. The following function also downloads and extracts the files, if they don't exist in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_glue_data(dest_path=DATA_FOLDER, tasks=\"RTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte_train_data  = load_pandas_df(local_cache_path=DATA_FOLDER, task=\"RTE\", file_split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte_train_data .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rte_dev_data = load_pandas_df(local_cache_path=DATA_FOLDER, task=\"RTE\", file_split=\"dev\")\n",
    "rte_test_data = load_pandas_df(local_cache_path=DATA_FOLDER, task=\"RTE\", file_split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we tokenize the text documents and convert them to lists of tokens. The following steps instantiate a BERT tokenizer given the language, and tokenize the text of the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = label_map.GLOBAL_MAP[\"rte\"]\n",
    "\n",
    "#rte_train_fout = os.path.join(DATA_FOLDER, \"RTE\", \"rte_train.json\")\n",
    "#rte_dev_fout = os.path.join(DATA_FOLDER, \"RTE\", \"rte_dev.json\")\n",
    "rte_test_fout = os.path.join(DATA_FOLDER, \"RTE\", \"rte_test.json\")\n",
    "#build_data(rte_train_data, rte_train_fout, label_dict)\n",
    "#build_data(rte_dev_data, rte_dev_fout, label_dict)\n",
    "build_data(rte_test_data, rte_test_fout, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict['entailment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "Next, we create a sequence classifier that loads a pre-trained MT-DNN model, given the language and number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "cache_dir = \".\"\n",
    "MODEL_FILE = \"mt_dnn_large.pt\"\n",
    "MTDNN_URL = \"https://mrc.blob.core.windows.net/mt-dnn-model/\" + MODEL_FILE\n",
    "model_path = os.path.join(cache_dir, \"mt_dnn_large.pt\")\n",
    "bash_command = \"wget\" + \" \" + MTDNN_URL + \" \" + \"-O\" + model_path\n",
    "subprocess.run(bash_command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MTDNNSequenceClassifier(\n",
    "    language=LANGUAGE, num_labels=2, cache_dir=BERT_CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/11/2019 04:14:10 0\n",
      "06/11/2019 04:14:10 0\n",
      "06/11/2019 04:14:10 0\n",
      "06/11/2019 04:14:10 0\n",
      "06/11/2019 04:14:10 0\n",
      "06/11/2019 04:14:10 0\n",
      "06/11/2019 04:14:10 0\n",
      "06/11/2019 04:14:10 Launching the MT-DNN training\n",
      "06/11/2019 04:14:10 Launching the MT-DNN training\n",
      "06/11/2019 04:14:10 Launching the MT-DNN training\n",
      "06/11/2019 04:14:10 Launching the MT-DNN training\n",
      "06/11/2019 04:14:10 Launching the MT-DNN training\n",
      "06/11/2019 04:14:10 Launching the MT-DNN training\n",
      "06/11/2019 04:14:10 Launching the MT-DNN training\n",
      "06/11/2019 04:14:10 Loading ../../tmp/RTE/rte_train.json as task 0\n",
      "06/11/2019 04:14:10 Loading ../../tmp/RTE/rte_train.json as task 0\n",
      "06/11/2019 04:14:10 Loading ../../tmp/RTE/rte_train.json as task 0\n",
      "06/11/2019 04:14:10 Loading ../../tmp/RTE/rte_train.json as task 0\n",
      "06/11/2019 04:14:10 Loading ../../tmp/RTE/rte_train.json as task 0\n",
      "06/11/2019 04:14:10 Loading ../../tmp/RTE/rte_train.json as task 0\n",
      "06/11/2019 04:14:10 Loading ../../tmp/RTE/rte_train.json as task 0\n",
      "Loaded 2489 samples out of 2489\n",
      "06/11/2019 04:14:10 2\n",
      "06/11/2019 04:14:10 2\n",
      "06/11/2019 04:14:10 2\n",
      "06/11/2019 04:14:10 2\n",
      "06/11/2019 04:14:10 2\n",
      "06/11/2019 04:14:10 2\n",
      "06/11/2019 04:14:10 2\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(os.path.join(DATA_FOLDER, \"RTE\"), \n",
    "               \"rte\",\n",
    "               \"0,1,2,3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mtdnn)",
   "language": "python",
   "name": "mtdnn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
