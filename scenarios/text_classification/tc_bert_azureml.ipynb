{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Text Classification of MultiNLI Sentences using BERT with Azure ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n",
      "Azure ML SDK Version: 1.0.48\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from utils_nlp.bert.common import Language, Tokenizer\n",
    "from utils_nlp.azureml import azureml_utils\n",
    "from utils_nlp.dataset.multinli import get_generator\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import azureml.core\n",
    "from azureml.core import Datastore, Experiment,  get_run\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.compute import ComputeTarget,  AmlCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.pipeline.steps import EstimatorStep\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Azure ML SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "In this notebook, we fine-tune and evaluate a pretrained [BERT](https://arxiv.org/abs/1810.04805) model on a subset of the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset using [AzureML](https://azure.microsoft.com/en-us/services/machine-learning-service/) Pipelines.\n",
    "\n",
    "We use a [distributed sequence classifier](../../utils_nlp/bert/sequence_classification_distributed.py) that wraps [Hugging Face's PyTorch implementation](https://github.com/huggingface/pytorch-pretrained-BERT) of Google's [BERT](https://github.com/google-research/bert).\n",
    "\n",
    "The notebooks acts as a template to,\n",
    "1. Process a massive dataset in parallel by dividing the dataset into chunks using [DASK](https://dask.org/) .\n",
    "2. Perform distributed training on AzureML compute on these processed chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = \"genre\"\n",
    "TEXT_COL = \"sentence1\"\n",
    "DATA_FOLDER = \"../../data/temp\"\n",
    "TRAIN_FOLDER = \"../../data/temp/train\"\n",
    "TEST_FOLDER = \"../../data/temp/test\"\n",
    "BERT_CACHE_DIR = \"../../data/temp\"\n",
    "LANGUAGE = Language.ENGLISH\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE = 32\n",
    "NUM_GPUS = 2\n",
    "NUM_EPOCHS = 1\n",
    "TRAIN_SIZE = 0.6\n",
    "TEXT_COL = \"sentence1\"\n",
    "ENCODED_LABEL_COL = \"label\"\n",
    "TOKEN_COL = \"tokens\"\n",
    "MASK_COL = \"mask\"\n",
    "NUM_PARTITIONS = None\n",
    "LABELS = ['telephone', 'government', 'travel', 'slate', 'fiction']\n",
    "PROJECT_FOLDER = \"../../\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will use AzureML pipelines to execute training pipelines. Each preprocessing step is included as a step in the pipeline. For a more detailed walkthrough of what pipelines are with a getting started guidelines check this [notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb). We start by doing some AzureML related setup below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Create a workspace\n",
    "\n",
    "First, go through the [Configuration](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`. This will create a config.json file containing the values needed below to create a workspace.\n",
    "\n",
    "**Note**: you do not need to fill in these values if you have a config.json in the same folder as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n",
      "WARNING - You have logged in. Now let us find all the subscriptions to which you have access...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive authentication successfully completed.\n"
     ]
    }
   ],
   "source": [
    "ws = azureml_utils.get_or_create_workspace(\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    workspace_region=\"<WORKSPACE_REGION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Create a compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 2, 'targetNodeCount': 2, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 2, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-07-30T23:55:09.394000+00:00', 'errors': None, 'creationTime': '2019-07-25T04:16:20.598768+00:00', 'modifiedTime': '2019-07-25T04:16:36.486727+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 2, 'maxNodeCount': 10, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC12'}\n"
     ]
    }
   ],
   "source": [
    "# choose your cluster\n",
    "cluster_name = \"pipelines-tc-12\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC12\", max_nodes=8\n",
    "    )\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute.\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "The pipeline is defined by a series of steps, the first being a PythonScriptStep which utilizes [DASK](https://dask.org/) to load dataframes in partitions allowing us to load and preprocess different sets of data in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_generator(DATA_FOLDER, \"train\", num_batches=NUM_PARTITIONS, batch_size=10e6)\n",
    "test_batches = get_generator(DATA_FOLDER, \"dev_matched\", num_batches=NUM_PARTITIONS, batch_size=10e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess and Tokenize\n",
    "\n",
    "In the classification task, we use the first sentence only as the text input, and the corresponding genre as the label. Select the examples corresponding to one of the entailment labels (*neutral* in this case) to avoid duplicate rows, as the sentences are not unique, whereas the sentence pairs are.\n",
    "\n",
    "Once filtered, we encode the labels. To do this, fit a label encoder with the known labels in a MNLI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(TRAIN_FOLDER, exist_ok=True)\n",
    "os.makedirs(TEST_FOLDER, exist_ok=True)\n",
    "\n",
    "labels = LABELS\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "num_train_partitions = 0\n",
    "for batch in train_batches:\n",
    "    batch = batch[batch[\"gold_label\"]==\"neutral\"]\n",
    "    batch[ENCODED_LABEL_COL] = label_encoder.transform(batch[LABEL_COL])\n",
    "    batch.to_csv(TRAIN_FOLDER+\"/batch{}.csv\".format(str(num_train_partitions)))\n",
    "    num_train_partitions += 1\n",
    "    \n",
    "num_test_partitions = 0\n",
    "for batch in test_batches:\n",
    "    batch = batch[batch[\"gold_label\"]==\"neutral\"]\n",
    "    batch[ENCODED_LABEL_COL] = label_encoder.transform(batch[LABEL_COL])\n",
    "    batch.to_csv(TEST_FOLDER+\"/batch{}.csv\".format(str(num_test_partitions)))\n",
    "    num_test_partitions += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the partitions of data ready they are uploaded to the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir=TRAIN_FOLDER, target_path=\"mnli_data/train\", overwrite=True, show_progress=False)\n",
    "ds.upload(src_dir=TEST_FOLDER, target_path=\"mnli_data/test\", overwrite=True, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TRAIN_FOLDER)\n",
    "shutil.rmtree(TEST_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now parallely operate on each batch to tokenize the data and preprocess the tokens. To do this, we create a PythonScript step below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils_nlp.bert.common import Language, Tokenizer\n",
    "\n",
    "LABEL_COL = \"genre\"\n",
    "TEXT_COL = \"sentence1\"\n",
    "LANGUAGE = Language.ENGLISH\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 150\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def tokenize(df):\n",
    "    \"\"\"Tokenize the text documents and convert them to lists of tokens using the BERT tokenizer.\n",
    "    Args:\n",
    "        df(pd.Dataframe): Dataframe with training or test samples\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        list: List of lists of tokens for train set.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(\n",
    "        LANGUAGE, to_lower=TO_LOWER)\n",
    "    tokens = tokenizer.tokenize(list(df[TEXT_COL]))\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def preprocess(tokens):\n",
    "    \"\"\" Preprocess method that does the following,\n",
    "            Convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary\n",
    "            Add the special tokens [CLS] and [SEP] to mark the beginning and end of a sentence\n",
    "            Pad or truncate the token lists to the specified max length\n",
    "            Return mask lists that indicate paddings' positions\n",
    "            Return token type id lists that indicate which sentence the tokens belong to (not needed\n",
    "            for one-sequence classification)\n",
    "\n",
    "    Args:\n",
    "        tokens(pd.Dataframe): Dataframe with tokens for train set.\n",
    "\n",
    "    Returns:\n",
    "        list: List of lists of tokens for train or test set with special tokens added.\n",
    "        list: Input mask.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(\n",
    "        LANGUAGE, to_lower=TO_LOWER)\n",
    "    tokens, mask, _ = tokenizer.preprocess_classification_tokens(\n",
    "        tokens, MAX_LEN\n",
    "    )\n",
    "\n",
    "    return tokens, mask\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str, help=\"input data\")\n",
    "parser.add_argument(\"--output_data\", type=str, help=\"Path to the output file.\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "input_data = args.input_data\n",
    "output_data = args.output_data\n",
    "output_dir = os.path.dirname(os.path.abspath(output_data))\n",
    "\n",
    "if output_dir is not None:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logger.info(\"%s created\" % output_dir)\n",
    "\n",
    "df = pd.read_csv(args.input_data)\n",
    "tokens_array = tokenize(df)\n",
    "tokens_array, mask_array = preprocess(tokens_array)\n",
    "\n",
    "df['tokens'] = tokens_array\n",
    "df['mask'] = mask_array\n",
    "\n",
    "# Filter columns\n",
    "cols = ['tokens', 'mask', 'label']\n",
    "df = df[cols]\n",
    "df.to_csv(output_data, header=False, index=False)\n",
    "logger.info(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../utils_nlp/bert/preprocess.py'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_file = os.path.join(PROJECT_FOLDER,'utils_nlp/bert/preprocess.py')\n",
    "shutil.move('preprocess.py',preprocess_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a conda environment for the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=[\n",
    "        \"numpy\",\n",
    "        \"scikit-learn\",\n",
    "        \"pandas\",\n",
    "    ],\n",
    "    pip_packages=[\"azureml-sdk==1.0.43.*\", \n",
    "                  \"torch==1.1\", \n",
    "                  \"tqdm==4.31.1\",\n",
    "                 \"pytorch-pretrained-bert>=0.6\"],\n",
    "    python_version=\"3.6.8\",\n",
    ")\n",
    "run_config = RunConfiguration(conda_dependencies=conda_dependencies)\n",
    "run_config.environment.docker.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create the list of steps that use the preprocess.py created above. We use the output of these steps as input to training in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_files = []\n",
    "processed_test_files = []\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "for i in range(num_train_partitions):\n",
    "        input_data = DataReference(datastore=ds, \n",
    "                                   data_reference_name='train_batch_{}'.format(str(i)), \n",
    "                                   path_on_datastore='mnli_data/train/batch{}.csv'.format(str(i)),\n",
    "                                   overwrite=False)\n",
    "\n",
    "        output_data = PipelineData(name=\"train{}\".format(str(i)), datastore=ds,\n",
    "                       output_path_on_compute='mnli_data/processed_train/batch{}.csv'.format(str(i)))\n",
    "\n",
    "        step = PythonScriptStep(\n",
    "            name='preprocess_step_train_{}'.format(str(i)),\n",
    "            arguments=[\"--input_data\", input_data, \"--output_data\", output_data],\n",
    "            script_name= 'utils_nlp/bert/preprocess.py',\n",
    "            inputs=[input_data],\n",
    "            outputs=[output_data],\n",
    "            source_directory=PROJECT_FOLDER,\n",
    "            compute_target=compute_target,\n",
    "            runconfig=run_config,\n",
    "            allow_reuse=False,\n",
    "        )\n",
    "        \n",
    "        processed_train_files.append(output_data)         \n",
    "            \n",
    "for i in range(num_test_partitions):\n",
    "            input_data = DataReference(datastore=ds, \n",
    "                                       data_reference_name='test_batch_{}'.format(str(i)), \n",
    "                                       path_on_datastore='mnli_data/test/batch{}.csv'.format(str(i)),\n",
    "                                       overwrite=False)\n",
    "        \n",
    "            output_data = PipelineData(name=\"test{}\".format(str(i)), datastore=ds,\n",
    "                        output_path_on_compute='mnli_data/processed_test/batch{}.csv'.format(str(i)))\n",
    "            \n",
    "            step = PythonScriptStep(\n",
    "                name='preprocess_step_test_{}'.format(str(i)),\n",
    "                arguments=[\"--input_data\", input_data, \"--output_data\", output_data],\n",
    "                script_name= 'utils_nlp/bert/preprocess.py',\n",
    "                inputs=[input_data],\n",
    "                outputs=[output_data],\n",
    "                source_directory=PROJECT_FOLDER,\n",
    "                compute_target=compute_target,\n",
    "                runconfig=run_config,\n",
    "                allow_reuse=False,\n",
    "            )\n",
    "            \n",
    "            processed_test_files.append(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train and Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is processed and available on datastore, we  train the classifier using the training examples. This involves fine-tuning the BERT Transformer and learning a linear classification layer on top of that. After training is complete we score the performance of the model on the test dataset\n",
    "\n",
    "The training is distributed and is done AzureML's capability to support distributed using MPI with horovod. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utils_nlp.bert.common import Language\n",
    "from utils_nlp.bert.sequence_classification_distributed import (\n",
    "    BERTSequenceDistClassifier,\n",
    ")\n",
    "from utils_nlp.common.timer import Timer\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_GPUS = 2\n",
    "NUM_EPOCHS = 1\n",
    "LABELS = [\"telephone\", \"government\", \"travel\", \"slate\", \"fiction\"]\n",
    "OUTPUT_DIR = \"./outputs/\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--train_files\",\n",
    "    nargs=\"+\",\n",
    "    default=[],\n",
    "    help=\"List of file paths to all the files in train dataset.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--test_files\",\n",
    "    nargs=\"+\",\n",
    "    default=[],\n",
    "    help=\"List of file paths to all the files in test dataset.\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "train_files = [file.strip() for file in args.train_files]\n",
    "test_files = [file.strip() for file in args.test_files]\n",
    "\n",
    "# Handle square brackets from train list\n",
    "train_files[0] = train_files[0][1:]\n",
    "train_files[len(train_files) - 1] = train_files[len(train_files) - 1][:-1]\n",
    "\n",
    "# Handle square brackets from test list\n",
    "test_files[0] = test_files[0][1:]\n",
    "test_files[len(test_files) - 1] = test_files[len(test_files) - 1][:-1]\n",
    "\n",
    "# Train\n",
    "classifier = BERTSequenceDistClassifier(\n",
    "    language=Language.ENGLISH, num_labels=len(LABELS)\n",
    ")\n",
    "with Timer() as t:\n",
    "    classifier.fit(\n",
    "        train_files,\n",
    "        num_gpus=NUM_GPUS,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "# Predict\n",
    "preds, labels_test = classifier.predict(\n",
    "    test_files, num_gpus=NUM_GPUS, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "results = classification_report(\n",
    "    labels_test, preds, target_names=LABELS, output_dict=True\n",
    ")\n",
    "\n",
    "# Write out results.\n",
    "result_file = os.path.join(OUTPUT_DIR, \"results.json\")\n",
    "with open(result_file, \"w+\") as fp:\n",
    "    json.dump(results, fp)\n",
    "\n",
    "# Save model\n",
    "model_file = os.path.join(OUTPUT_DIR, \"model.pt\")\n",
    "torch.save(classifier.model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../utils_nlp/bert/train.py'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = os.path.join(PROJECT_FOLDER,'utils_nlp/bert/train.py')\n",
    "shutil.move('train.py',train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create a Pytorch Estimator\n",
    "\n",
    "We create a Pytorch Estimator using AzureML SDK and additonally define an EstimatorStep to run it on AzureML pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.1.\n",
      "WARNING - 'process_count_per_node' parameter will be deprecated. Please use it as part of 'distributed_training' parameter.\n"
     ]
    }
   ],
   "source": [
    "estimator = PyTorch(source_directory=PROJECT_FOLDER,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='utils_nlp/bert/train.py',\n",
    "                    node_count=4,\n",
    "                    distributed_training=MpiConfiguration(),\n",
    "                    process_count_per_node=2,\n",
    "                    use_gpu=True,\n",
    "                    conda_packages=['scikit-learn=0.20.3', 'numpy>=1.16.0', 'pandas'],\n",
    "                    pip_packages=[\"tqdm==4.31.1\",\"pytorch-pretrained-bert>=0.6\"]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processed_train_files + processed_test_files\n",
    "\n",
    "est_step = EstimatorStep(name=\"Estimator-Train\", \n",
    "                         estimator=estimator, \n",
    "                         estimator_entry_script_arguments=[\n",
    "                             '--train_files',  str(processed_train_files),\n",
    "                             '--test_files', str(processed_test_files)],\n",
    "                         inputs = inputs,\n",
    "                         runconfig_pipeline_params=None, \n",
    "                         compute_target=compute_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Submit the pipeline\n",
    "\n",
    "The model is fine tuned on AML Compute and takes ~45 minutes to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Estimator-Train [473f7d30][ab68a3b4-3d2b-4a49-a4c6-1b0b791b1e88], (This step is eligible to reuse a previous run's output)\n",
      "Created step preprocess_step_train_0 [9fd37e6c][366a60a5-3853-4dac-a091-403789f8b34d], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_1 [74d43aed][b87722e9-b634-4369-b401-40a478dbc512], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_2 [0e17f205][2cda9c54-0172-4dad-8a11-bf2be1b8f1a6], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_3 [be4b9aeb][f000b8dd-3bb1-464a-bdd7-787939e8a1f7], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_4 [b3ca2f90][8341040b-bebf-4bcf-89a8-63242911e135], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_5 [38afb135][1e1929f9-986f-45e1-93d6-860d75f37858], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_6 [4e9145ef][d3914ba2-d13b-4d0c-b736-92a95fbd4c7c], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_7 [0a3d217b][9729eac8-cd87-4f66-803e-cce75c3e1ca5], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_8 [ff5c3cc0][ddce3abf-1dbf-4afb-9e26-a4a171db01d0], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_9 [5cd69c29][abda5fd2-8d4f-48fc-99a5-71e9a08e1d65], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_10 [7230bc90][0cec3291-2c32-4d96-9848-31d88410f0bf], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_11 [2662e629][ca6750ce-0520-4931-98d9-a71e0549aeda], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_12 [c139d6f0][33bb8844-d6da-4b82-a6d1-87d450c7fac0], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_13 [21aacaa7][69f38997-4998-4324-b6c1-686632589832], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_14 [653e1350][7bcd7b8c-0e98-4e3e-973c-5f921a580ac7], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_15 [dffdd014][248c47a9-658d-4547-8585-706953fa7f73], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_16 [ba0c9ccc][c119fac6-e2bf-4ec2-a43f-896f6e9d58c8], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_17 [25df8c61][03b96da9-01ba-47b7-a0ab-30d359a34184], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_18 [b131f496][20601968-a7d6-43c8-9d38-b44755bfea09], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_19 [ce6ffc03][27b9c003-35f1-4ba2-be72-ad50038865b4], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_20 [e5419f26][951a628b-8a72-4c04-bc30-da624ef6cb3d], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_21 [3c6ca188][4a75827f-1897-4136-84f0-2f58ba0e7556], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_22 [0164f1c1][ee96c0c4-9faf-4373-a699-c6c019bdd58f], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_23 [706519bc][a3f6f82f-95df-4977-9c1e-8a3878945f85], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_24 [9d403708][fd685760-09c4-45e9-94c3-8d7b0b3c2e94], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_25 [3f89c4d3][ae8f82e2-5eb7-45d5-8386-a28ff00d059f], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_26 [ccc334e1][7720da75-a593-4c9c-a350-6fb62abcc352], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_27 [7c51d6cd][2c8b2dd8-8413-4ecc-8edb-fdd8158b673d], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_28 [3a45e1de][d2999a4e-cd0f-4783-a020-7c0da71b6ec9], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_29 [f80927e6][109ab24a-f3d8-4532-9d26-756f6ef82212], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_30 [d7cdfa83][4c9105f9-afd4-415d-8330-1f9b5f528de4], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_31 [16fc7d84][e2cebd59-0de0-4c68-8f51-8ece6aad0eac], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_32 [1387d178][d3e53b07-f5b7-4d65-a382-27fb7292a569], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_33 [20260a9e][aee8f96d-54a4-42d4-a1fe-2977a0766c89], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_34 [adcd2c3c][73e540a9-de93-4c59-91ce-04f0f0c49f11], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_35 [0e03b611][546b6b12-1715-4574-bc56-398300f05ba2], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_36 [40bbf4f2][0e8e1937-895f-440e-a36f-c460c9a06fa3], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_37 [66e90793][dc3f6bd7-939f-4101-aa52-cce6c4e93f7a], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_38 [a338e023][f38246bd-c556-4627-96e7-25e15e545184], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_39 [fc8b7339][d2162575-f36a-46c6-9fea-d5ab8db1a337], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_40 [c7160935][ceb75754-9f3e-4272-b3f0-ceeff5693938], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_41 [9e98f6cf][e0a8c512-68cd-40d9-87b5-34f9e38395b0], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_42 [29ad1560][94cfb09c-f057-4ebc-91f5-337ba0ca5c25], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_43 [694711c7][d6d2e007-6ccb-4120-b9f4-d62184016678], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_44 [20139575][ddb09959-ee10-4906-ade8-22df5d77aff7], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_45 [9f4abb68][35abbc67-318d-484e-8e6d-b02d06f24b65], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_46 [a6b07d2b][ac615d7a-2f4a-4e9d-9236-d97f5e7ee866], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_47 [b3fe6de9][74231fb9-5af3-4b34-8c60-24ea40c2c4e7], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_48 [4a786091][041da1f5-df23-415d-9803-c8a0e5992d80], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_train_49 [d3bcbc7f][eba795f0-d88b-4394-b639-b35f6b2b39f7], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_test_0 [91fc910e][33c1f5df-2b75-4a0b-90d4-226112193b89], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_test_1 [cdd4ab0e][b504f6f8-8ae7-45c8-982f-38b6427b5a43], (This step will run and generate new outputs)\n",
      "Using data reference train_batch_0 for StepId [12fe0582][17f8e701-3e04-4b71-8bcf-c089ade07e53], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_1 for StepId [8cb4fd70][846dcc59-19b4-4f65-b586-90f6d04b7ce7], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_2 for StepId [4f07bb80][4db130ad-01fc-4deb-8461-66abd488c60f], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_3 for StepId [c7a6855b][b5bf4b06-9947-4406-bcf4-5e2fd4c6f87c], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_4 for StepId [d22118e2][99bbe1f0-a7ae-4edb-a12f-2210421bf8aa], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_5 for StepId [be672979][b305958a-6a60-4c3d-9db7-f86d0e68b7e1], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_6 for StepId [58ec956d][b9357ce8-e771-4829-b381-63f57094b9b5], (Consumers of this data are eligible to reuse prior runs.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data reference train_batch_7 for StepId [0d0b8edc][d2cb8ea4-4650-4143-85cc-3a50b2d92fb3], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_8 for StepId [8ead4f4d][0b8ae803-5f49-4361-9c27-156a4f69486b], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_9 for StepId [1f979fec][2d50bd14-2fb0-4742-8805-3bd1dc19c3c3], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_10 for StepId [4e505f04][cba68341-2c4a-432e-a4bc-38af276684a6], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_11 for StepId [47aafb86][f7250703-3bad-4db6-9c4a-e68c98565f0a], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_12 for StepId [c61464e0][1e833559-7b27-422d-ad18-c7b8421af99a], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_13 for StepId [fc80ef2f][c28e0d83-f13d-4bb4-9be5-d0da1d983202], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_14 for StepId [3320e153][65f357df-583d-4323-80cf-dafebf99cc07], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_15 for StepId [738f25c0][895c3744-0a30-4d4e-a977-d44b7f1f66c3], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_16 for StepId [50e05001][ccd4caec-2388-4f08-8de2-3339827d6235], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_17 for StepId [2fb57382][17a19068-51f7-4c41-91bb-7dad813ea472], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_18 for StepId [61a39a09][483e0c5f-6202-475e-b7f1-f793ac2156de], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_19 for StepId [e6a2b231][799ce7d8-6bc2-41b7-95a6-a03414eda1fc], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_20 for StepId [28924fe7][9dc2a80a-1a7d-49fc-9d33-9789bfaebd76], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_21 for StepId [73dc793a][19bf9363-deb0-4eb4-a589-bc08780061e9], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_22 for StepId [61bda11a][37d2249a-9f9f-4b6c-8c61-2b0262e7d5ef], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_23 for StepId [93cb129c][5b88784c-ea85-4b79-9fa8-374692a803d3], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_24 for StepId [918fb15b][26f61dda-448b-4cb3-b9c4-b72bcab4030b], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_25 for StepId [517a8b8d][5a903bbf-323e-4b18-b784-2d2e23c8e7d7], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_26 for StepId [14790aaf][bbe266bf-b6ef-4087-a66e-90bfdece293c], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_27 for StepId [12513a8a][92c6f4b1-44eb-430c-a659-9e87f4823b2b], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_28 for StepId [a84a2361][08581dd9-cf36-458e-9665-7bf798ffeeee], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_29 for StepId [7e5e45ca][2161f439-0e8a-47ee-9780-eae0ca99ae85], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_30 for StepId [007df351][c6c9d84d-d71c-46dd-88c9-b1fb44f642c3], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_31 for StepId [c1939309][db5ce984-359a-443b-a5e7-5ba398c3d68f], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_32 for StepId [e07a8618][a2d37ce8-0c10-415c-97a1-195c21d7361b], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_33 for StepId [bc7cddf4][e0572514-1e3d-4e8c-9542-6948768bf355], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_34 for StepId [9475cd3a][67585ef5-fa8a-4718-a2cf-637c62d12147], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_35 for StepId [ccbda991][fa9834be-405c-4252-b4b7-1eb9f908885e], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_36 for StepId [9cecf9bc][d54bf550-4da8-41f1-951a-d43e2b423d2a], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_37 for StepId [64d43db2][9567112e-7a8c-482d-9384-ef23f4c74c11], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_38 for StepId [5ee52774][5e878069-b66c-4fac-a883-d6df489deb42], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_39 for StepId [da22d0b8][2d2099f7-d261-4bd2-97a1-afb74eb461a6], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_40 for StepId [b279030b][18519919-2154-4167-93a9-1ce8d0e473fa], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_41 for StepId [78994859][46b6aff7-4c3f-4ada-8472-58126f7d0ea3], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_42 for StepId [0e1ae7bc][0ca8164e-ceb3-4404-a79d-583c87f38115], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_43 for StepId [21900fb0][475d6255-534b-44a5-bcb8-9e831a714656], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_44 for StepId [789e7132][c3ac2e29-647d-42b3-885d-c6eb1b0079a3], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_45 for StepId [303a1e2a][a13b63e2-bc7f-46d6-8424-01148e0c39ba], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_46 for StepId [b28c4dca][13ee5a14-7102-452d-8558-63d48dd9634c], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_47 for StepId [64db6cfa][a61f0148-28b6-4e14-b83c-5c7d76523638], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_48 for StepId [2acef880][ff3ed020-862b-4154-9562-564ae0b786cb], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference train_batch_49 for StepId [f8d50ad2][ff5b9b21-99ea-47e6-ae60-20e9bb417a85], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference test_batch_0 for StepId [948dc7dd][7893a2ac-3e3a-4222-bfba-448fcf808d69], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference test_batch_1 for StepId [f99efb8e][c00e2c25-bbb2-47e6-a2fb-c537f22b2780], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted pipeline run: a468370f-fa8f-48de-8661-ba1dc68a6d24\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[est_step])\n",
    "experiment = Experiment(ws, 'NLP-TC-BERT-distributed')\n",
    "pipeline_run = experiment.submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880a26037a984153aacf764cbf185da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Retrying (Retry(total=2, connect=2, read=3, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x00000202B5F08940>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',)': /history/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/nlprg/providers/Microsoft.MachineLearningServices/workspaces/MAIDAPTest/experiments/NLP-TC-BERT-distributed/runs/99531f83-70c6-41bb-96c9-20978c4df561\n",
      "WARNING - Retrying (Retry(total=2, connect=3, read=2, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='eastus2.aether.ms', port=443): Read timed out. (read timeout=100)\",)': /api/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/nlprg/providers/Microsoft.MachineLearningServices/workspaces/MAIDAPTest/PipelineRuns/a468370f-fa8f-48de-8661-ba1dc68a6d24\n"
     ]
    }
   ],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you would like to cancel the job for any reasons uncomment the code below.\n",
    "#pipeline_run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wait for the run to complete before continuing in the notebook\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Download and analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file outputs/results.json to ./outputs\\results.json...\n",
      "Downloading file outputs/model.pt to ./outputs\\model.pt...\n"
     ]
    }
   ],
   "source": [
    "step_run = pipeline_run.find_step_run(\"Estimator-Train\")[0]\n",
    "file_names = ['outputs/results.json', 'outputs/model.pt']\n",
    "azureml_utils.get_output_files(step_run, './outputs', file_names=file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              f1-score  precision    recall  support\n",
      "telephone     0.915902   0.882180  0.952305    629.0\n",
      "government    0.947368   0.948161  0.946578    599.0\n",
      "travel        0.841503   0.898778  0.791091    651.0\n",
      "slate         0.991093   0.991896  0.990291    618.0\n",
      "fiction       0.942097   0.920489  0.964744    624.0\n",
      "micro avg     0.927587   0.927587  0.927587   3121.0\n",
      "macro avg     0.927593   0.928301  0.929002   3121.0\n",
      "weighted avg  0.926549   0.927690  0.927587   3121.0\n"
     ]
    }
   ],
   "source": [
    "with open('outputs/results.json', 'r') as handle:\n",
    "    parsed = json.load(handle)\n",
    "    print(pd.DataFrame.from_dict(parsed).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally clean up any intermediate files we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(train_file)\n",
    "os.remove(preprocess_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nlp_cpu",
   "language": "python",
   "name": "ame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
