{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Text Classification of MultiNLI Sentences using BERT with Azure ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from utils_nlp.bert.common import Language, Tokenizer\n",
    "from utils_nlp.azureml import azureml_utils\n",
    "from utils_nlp.dataset.multinli import get_generator\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from azureml.core import Datastore, Experiment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.pipeline.steps import EstimatorStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "In this notebook, we fine-tune and evaluate a pretrained BERT model on a subset of the MultiNLI dataset by using Azure ML Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = \"genre\"\n",
    "TEXT_COL = \"sentence1\"\n",
    "DATA_FOLDER = \"../../data/temp\"\n",
    "TRAIN_FOLDER = \"../../data/temp/train\"\n",
    "TEST_FOLDER = \"../../data/temp/test\"\n",
    "BERT_CACHE_DIR = \"../../data/temp\"\n",
    "LANGUAGE = Language.ENGLISH\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE = 32\n",
    "NUM_GPUS = 2\n",
    "NUM_EPOCHS = 1\n",
    "TRAIN_SIZE = 0.6\n",
    "TEXT_COL = \"sentence1\"\n",
    "ENCODED_LABEL_COL = \"label\"\n",
    "TOKEN_COL = \"tokens\"\n",
    "MASK_COL = \"mask\"\n",
    "NUM_BATCHES = 100\n",
    "LABELS = ['telephone', 'government', 'travel', 'slate', 'fiction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will use AzureML pipelines to execute our training pipelines. Each preprocessing step is included as a step in the pipeline. For a more detailed walkthrough of what pipelines are with a getting started guidelines check this [notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb). We start by doing some AzureML related setup below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Create a workspace\n",
    "\n",
    "First, go through the [Configuration](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`. This will create a config.json file containing the values needed below to create a workspace.\n",
    "\n",
    "**Note**: you do not need to fill in these values if you have a config.json in the same folder as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n",
      "WARNING - You have logged in. Now let us find all the subscriptions to which you have access...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive authentication successfully completed.\n"
     ]
    }
   ],
   "source": [
    "ws = azureml_utils.get_or_create_workspace(\n",
    "    subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "    resource_group=\"<RESOURCE_GROUP>\",\n",
    "    workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    workspace_region=\"<WORKSPACE_REGION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Setup experiment and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a folder for the project\n",
    "project_folder = \"../../\"\n",
    "\n",
    "# Set up an experiment\n",
    "experiment_name = \"pipelines-tc\"\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "# Add logging to our experiment\n",
    "run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Create a compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 2, 'targetNodeCount': 2, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 2, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-07-15T23:14:54.400000+00:00', 'errors': None, 'creationTime': '2019-07-11T23:45:19.957009+00:00', 'modifiedTime': '2019-07-11T23:45:42.001270+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 2, 'maxNodeCount': 24, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC12'}\n"
     ]
    }
   ],
   "source": [
    "# choose your cluster\n",
    "cluster_name = \"pipelines-tc-12\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC12\", max_nodes=24\n",
    "    )\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute.\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "The pipeline is defined by a series of steps, the first being a PythonScriptStep which utilizes [DASK](https://dask.org/) to load dataframes in batches allowing us to load and preprocess different sets of data in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_generator(DATA_FOLDER, \"train\", num_batches=NUM_BATCHES, batch_size=10e7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess and Tokenize\n",
    "\n",
    "In the classification task, use the first sentence only as the text input, and the corresponding genre as the label. Select the examples corresponding to one of the entailment labels (*neutral* in this case) to avoid duplicate rows, as the sentences are not unique, whereas the sentence pairs are.\n",
    "\n",
    "Once filtered, we encode the labels. To do this, fit a label encoder with the know labels in a MNLI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(TRAIN_FOLDER):\n",
    "    os.makedirs(TRAIN_FOLDER)\n",
    "if not os.path.exists(TEST_FOLDER):\n",
    "    os.makedirs(TEST_FOLDER)\n",
    "\n",
    "labels = LABELS\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "i=0\n",
    "for batch in batches:\n",
    "    batch = batch[batch[\"gold_label\"]==\"neutral\"]\n",
    "    batch[ENCODED_LABEL_COL] = label_encoder.transform(batch[LABEL_COL])\n",
    "    \n",
    "    if i<0.8*NUM_BATCHES:\n",
    "        batch.to_csv(TRAIN_FOLDER+\"/batch{}.csv\".format(str(i)))\n",
    "    else:\n",
    "        batch.to_csv(TEST_FOLDER+\"/batch{}.csv\".format(str(i)))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have batches of data ready they are uploaded to the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.as_mount()\n",
    "ds.upload(src_dir=TRAIN_FOLDER, target_path=\"mnli_data/train\", overwrite=True, show_progress=False)\n",
    "ds.upload(src_dir=TEST_FOLDER, target_path=\"mnli_data/test\", overwrite=True, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shutil.rmtree(TRAIN_FOLDER)\n",
    "#shutil.rmtree(TEST_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now parallely operate on each batch to tokenize the data and preprocess the tokens. To do this, we create a PythonScript step below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../utils_nlp/bert/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../utils_nlp/bert/preprocess.py\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils_nlp.bert.common import Language, Tokenizer\n",
    "\n",
    "LABEL_COL = \"genre\"\n",
    "TEXT_COL = \"sentence1\"\n",
    "LANGUAGE = Language.ENGLISH\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 150\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def tokenize(df):\n",
    "    \"\"\"Tokenize the text documents and convert them to lists of tokens using the BERT tokenizer.\n",
    "    Args:\n",
    "        df(pd.Dataframe): Dataframe with training or test samples\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        list: List of lists of tokens for train set.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(\n",
    "        LANGUAGE, to_lower=TO_LOWER)\n",
    "    tokens = tokenizer.tokenize(list(df[TEXT_COL]))\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def preprocess(tokens):\n",
    "    \"\"\" Preprocess method that does the following,\n",
    "            Convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary\n",
    "            Add the special tokens [CLS] and [SEP] to mark the beginning and end of a sentence\n",
    "            Pad or truncate the token lists to the specified max length\n",
    "            Return mask lists that indicate paddings' positions\n",
    "            Return token type id lists that indicate which sentence the tokens belong to (not needed\n",
    "            for one-sequence classification)\n",
    "\n",
    "    Args:\n",
    "        tokens(pd.Dataframe): Dataframe with tokens for train set.\n",
    "\n",
    "    Returns:\n",
    "        list: List of lists of tokens for train or test set with special tokens added.\n",
    "        list: Input mask.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(\n",
    "        LANGUAGE, to_lower=TO_LOWER)\n",
    "    tokens, mask, _ = tokenizer.preprocess_classification_tokens(\n",
    "        tokens, MAX_LEN\n",
    "    )\n",
    "\n",
    "    return tokens, mask\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str, help=\"input data\")\n",
    "parser.add_argument(\"--output_data\", type=str, help=\"output data directory\")\n",
    "parser.add_argument(\"--output_filename\", type=str, help=\"output file name\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "input_data = args.input_data\n",
    "output_data = args.output_data\n",
    "\n",
    "if output_data is not None:\n",
    "    print(output_data)\n",
    "    os.makedirs(output_data, exist_ok=True)\n",
    "    logger.info(\"%s created\" % output_data)\n",
    "\n",
    "df = pd.read_csv(args.input_data)\n",
    "tokens_array = tokenize(df)\n",
    "tokens_array, mask_array = preprocess(tokens_array)\n",
    "\n",
    "df['tokens'] = tokens_array\n",
    "df['mask'] = mask_array\n",
    "\n",
    "# Filter columns\n",
    "cols = ['tokens', 'mask', 'label']\n",
    "df = df[cols]\n",
    "df.to_csv(os.path.join(args.output_data, \"output_filename\"))\n",
    "logger.info(\"Completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a conda environment for the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=[\n",
    "        \"numpy\",\n",
    "        \"scikit-learn\",\n",
    "        \"pandas\",\n",
    "    ],\n",
    "    pip_packages=[\"azureml-sdk==1.0.43.*\", \n",
    "                  \"torch==1.1\", \n",
    "                  \"tqdm==4.31.1\",\n",
    "                 \"pytorch-pretrained-bert>=0.6\"],\n",
    "    python_version=\"3.6.8\",\n",
    ")\n",
    "run_config = RunConfiguration(conda_dependencies=conda_dependencies)\n",
    "run_config.environment.docker.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create the list of steps that use the preprocess.py created above. Add these steps into a pipeline and validate it to ensure there are no errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reference batch_0 is ready to be created [96cfa6c7], (Consumers of this data will generate new runs.)\n",
      "Data reference batch_1 is ready to be created [7f84ffda], (Consumers of this data will generate new runs.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = []\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "train_dir = PipelineData(name=\"train_data\", datastore=ds,\n",
    "                           output_path_on_compute='mnli_data/processed_train')\n",
    "\n",
    "test_dir = PipelineData(name=\"test_dir\", datastore=ds,\n",
    "                        output_path_on_compute='mnli_data/processed_test')\n",
    "\n",
    "for i in range(2):\n",
    "        if i < 1:\n",
    "            input_data = DataReference(datastore=ds, \n",
    "                                       data_reference_name='batch_{}'.format(str(i)), \n",
    "                                       path_on_datastore='mnli_data/train/batch1.csv',\n",
    "                                       overwrite=False)\n",
    "            train_dir = PipelineData(name=\"train_data\", datastore=ds,\n",
    "                           output_path_on_compute='mnli_data/processed_train')\n",
    "            output_data = train_dir\n",
    "        else:\n",
    "            input_data = DataReference(datastore=ds, \n",
    "                                       data_reference_name='batch_{}'.format(str(i)), \n",
    "                                       path_on_datastore='mnli_data/test/batch82.csv',\n",
    "                                       overwrite=False)\n",
    "            test_dir = PipelineData(name=\"test_dir\", datastore=ds,\n",
    "                        output_path_on_compute='mnli_data/processed_test')\n",
    "            output_data = test_dir\n",
    "            \n",
    "        step = PythonScriptStep(\n",
    "            name='preprocess_step_{}'.format(str(i)),\n",
    "            arguments=[\"--input_data\", input_data, \n",
    "                       \"--output_data\", output_data, \n",
    "                       \"--output_filename\", 'batch{}.csv'.format(str(i))],\n",
    "            script_name= \"utils_nlp/bert/preprocess.py\",\n",
    "            inputs=[input_data],\n",
    "            outputs=[output_data],\n",
    "            source_directory=project_folder,\n",
    "            compute_target=compute_target,\n",
    "            runconfig=run_config,\n",
    "            allow_reuse=False,\n",
    "        )\n",
    "        \n",
    "        steps.append(step)\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step preprocess_step_0 [dd07459a][b183cc73-1fb1-42e7-b646-84cd56c333fe], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_1 [9adec2b1][c574ff86-8a63-4e42-96d7-5ccc1d27909b], (This step will run and generate new outputs)\n",
      "Created data reference batch_0 for StepId [96cfa6c7][a79c0d54-1eb9-43cc-9ca6-f7f4cf335311], (Consumers of this data will generate new runs.)\n",
      "Created data reference batch_1 for StepId [7f84ffda][6911f16a-5584-40a5-a278-1a8a82a68931], (Consumers of this data will generate new runs.)\n",
      "Submitted pipeline run: 5c872ce2-8c2a-4f55-ac20-5a11477d214b\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "pipeline_run1 = Experiment(ws, 'Preprocessing-MNLI').submit(pipeline, regenerate_outputs=False)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cea4b7579a4e879e1e199d9456d44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47eafb732f104ef59fe8a6f7edfbfc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Clean up local preprocess file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is processed and available on datastore, we  train the classifier using the training examples. This involves fine-tuning the BERT Transformer and learning a linear classification layer on top of that. \n",
    "\n",
    "The training is distributed and is done AzureML's capability to support distributed using MPI with horovod. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../utils_nlp/bert/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../utils_nlp/bert/train.py\n",
    "\n",
    "\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utils_nlp.bert.common import Language\n",
    "from utils_nlp.bert.sequence_classification_distributed import BERTSequenceDistClassifier\n",
    "from utils_nlp.common.timer import Timer\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_GPUS = 2\n",
    "NUM_EPOCHS = 1\n",
    "LABELS = [\"telephone\", \"government\", \"travel\", \"slate\", \"fiction\"]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_train_dir\", type=str, help=\"Training data\")\n",
    "parser.add_argument(\"--input_test_dir\", type=str, help=\"Test data\")\n",
    "parser.add_argument(\"--result_dir\", type=str, help=\"Results directory containing confidence report\")\n",
    "parser.add_argument(\"--result_file\",type=str, help=\"File name for confidence report\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "train_dir = args.input_train_dir\n",
    "test_dir = args.input_test_dir\n",
    "result_dir = args.result_dir\n",
    "result_file = args.result_file\n",
    "\n",
    "if result_dir is not None:\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    logger.info(\"%s created\" % result_dir)\n",
    "\n",
    "# Train\n",
    "classifier = BERTSequenceDistClassifier(\n",
    "    language=Language.ENGLISH, num_labels=len(LABELS)\n",
    ")\n",
    "with Timer() as t:\n",
    "    classifier.fit(\n",
    "        train_dir,\n",
    "        num_gpus=NUM_GPUS,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=True,\n",
    "    )\n",
    "logger.info(\"Training Time {}\".format(t.interval / 3600))\n",
    "\n",
    "# Predict\n",
    "preds, labels_test = classifier.predict(\n",
    "    test_dir, num_gpus=NUM_GPUS, batch_size=BATCH_SIZE\n",
    ")\n",
    "data = classification_report(labels_test, preds, target_names=LABELS)\n",
    "with open(os.path.join(result_dir, result_file), 'wb') as fp:\n",
    "    pickle.dump(data, fp, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create a Pytorch Estimator\n",
    "\n",
    "We create a Pytorch Estimator using AzureML SDK and additonally define an EstimatorStep to run it on AzureML pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "\n",
    "'''\n",
    "train_dir = DataReference(datastore=ds, \n",
    "                           data_reference_name='train_data', \n",
    "                                       path_on_datastore='mnli_data/processed_train',\n",
    "                                       overwrite=False)\n",
    "\n",
    "test_dir = DataReference(datastore=ds, \n",
    "                           data_reference_name='test_data', \n",
    "                                       path_on_datastore='mnli_data/processed_test',\n",
    "                                       overwrite=False)\n",
    "'''\n",
    "\n",
    "result_dir = PipelineData(name=\"results\", \n",
    "                 datastore=ds,\n",
    "                 output_path_on_compute='mnli_data/results')\n",
    "result_file = 'result.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.1.\n"
     ]
    }
   ],
   "source": [
    "script_params = {\n",
    "    '--input_train_dir': train_dir,\n",
    "    '--input_test_dir' : test_dir}\n",
    "\n",
    "estimator = PyTorch(source_directory=project_folder,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='utils_nlp/bert/train.py',\n",
    "                    node_count=2,\n",
    "                    process_count_per_node=1,\n",
    "                    distributed_training=MpiConfiguration(),\n",
    "                    use_gpu=True,\n",
    "                    conda_packages=['scikit-learn=0.20.3', 'numpy>=1.16.0', 'pandas'],\n",
    "                    pip_packages=[\"tqdm==4.31.1\",\"pytorch-pretrained-bert>=0.6\"]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_step = EstimatorStep(name=\"Estimator-Train\", \n",
    "                         estimator=estimator, \n",
    "                         estimator_entry_script_arguments=[\n",
    "                             '--input_train_dir', train_dir,\n",
    "                             '--input_test_dir' , test_dir,\n",
    "                             '--result_dir', result_dir,\n",
    "                             '--result_file', result_file],\n",
    "                         inputs =[train_dir, test_dir],\n",
    "                         outputs =[result_dir],\n",
    "                         runconfig_pipeline_params=None, \n",
    "                         compute_target=compute_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Estimator-Train is ready to be created [8129dbe1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[est_step])\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Estimator-Train [8129dbe1][f3b7072d-71ff-4270-8ff0-5dfa6a95c4c2], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_0 [fa5967c4][b183cc73-1fb1-42e7-b646-84cd56c333fe], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_1 [0470ec39][c574ff86-8a63-4e42-96d7-5ccc1d27909b], (This step will run and generate new outputs)\n",
      "Using data reference batch_0 for StepId [5d90d30f][a79c0d54-1eb9-43cc-9ca6-f7f4cf335311], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference batch_1 for StepId [ae824316][6911f16a-5584-40a5-a278-1a8a82a68931], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted pipeline run: 17e25a9d-7ba0-487d-84b3-f84a792ef997\n"
     ]
    }
   ],
   "source": [
    "pipeline_run = Experiment(ws, 'TC-Training-BERT').submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ebfdaacc5445ee864fea6182acf0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python NLP CPU",
   "language": "python",
   "name": "nlp_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
