{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Text Classification of MultiNLI Sentences using PyTorch Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1001 17:16:49.833771 139876639942464 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1001 17:16:49.869802 139876639942464 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import sys\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "from utils_nlp.models.transformers.sequence_classification import Processor, SequenceClassifier \n",
    "from utils_nlp.common.timer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "run_small_example"
    ]
   },
   "outputs": [],
   "source": [
    "# Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs\n",
    "QUICK_RUN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "## [TODO] - Modify for the final model\n",
    "This notebook fine-tunes and evaluates a pretrained [XLNet](https://arxiv.org/pdf/1906.08237.pdf) model on a subset of the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset.\n",
    "\n",
    "We use a [sequence classifier](../../utils_nlp/models/bert/sequence_classification.py) that wraps [Hugging Face's PyTorch implementation](https://github.com/huggingface/pytorch-pretrained-BERT) of Google's [BERT](https://github.com/google-research/bert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_FRACTION = 1\n",
    "TEST_DATA_FRACTION = 1\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "if QUICK_RUN:\n",
    "    TRAIN_DATA_FRACTION = 0.01\n",
    "    TEST_DATA_FRACTION = 0.01\n",
    "    NUM_EPOCHS = 1\n",
    "\n",
    "BATCH_SIZE = 32 if torch.cuda.is_available() else 8\n",
    "DATA_FOLDER = \"./temp\"\n",
    "MODEL_CACHE_DIR = \"./temp\"\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE_PRED = 512\n",
    "TRAIN_SIZE = 0.6\n",
    "LABEL_COL = \"genre\"\n",
    "TEXT_COL = \"sentence1\" \n",
    "TARGET_MODEL = \"roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TODO] - Remove Workflow overview\n",
    "\n",
    "```\n",
    "model_name = SequenceClassifier.list_supported_models()[0]\n",
    "num_labels = len(label_encoder.classes_)\n",
    "processor = Processor(model_name=model_name, cache_dir=temp_dir)\n",
    "ds = processor.preprocess(text_train, labels_train, max_len=max_len)\n",
    "classifier = SequenceClassifier(\n",
    "    model_name=model_name, num_labels=num_labels, cache_dir=temp_dir\n",
    ")\n",
    "classifier.fit(ds, device=\"cuda\", num_epochs=1, batch_size=32, num_gpus=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "\n",
    "Let's start by loading a subset of the data.  \n",
    "\n",
    "The following function downloads and extracts the files, if they don't already exists in the data folder.\n",
    "\n",
    "The MultiNLI dataset is mainly used for natural language inference (NLI) tasks, where the inputs are sentence pairs and the labels are entailment indicators. The sentence pairs are also classified into *genres* that allow for more coverage and better evaluation of NLI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "download_data"
    ]
   },
   "outputs": [],
   "source": [
    "df = load_pandas_df(DATA_FOLDER, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Analysis of Data  \n",
    "\n",
    "Let's observe our dataset to see what we are working with.  \n",
    "For our classification task, we use the first sentence only as the text input, and the corresponding genre as the label. We select the examples corresponding to one of the entailment labels (*neutral* in this case) to avoid duplicate rows, as the sentences are not unique, whereas the sentence pairs are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>sentence1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>telephone</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>travel</td>\n",
       "      <td>But a few Christian mosaics survive above the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>slate</td>\n",
       "      <td>It's not that the questions they asked weren't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>travel</td>\n",
       "      <td>Thebes held onto power until the 12th Dynasty,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         genre                                          sentence1\n",
       "0   government  Conceptually cream skimming has two basic dime...\n",
       "4    telephone  yeah i tell you what though if you go price so...\n",
       "6       travel  But a few Christian mosaics survive above the ...\n",
       "12       slate  It's not that the questions they asked weren't...\n",
       "13      travel  Thebes held onto power until the 12th Dynasty,..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"gold_label\"] == \"neutral\"] # Get unique sentences\n",
    "df[[LABEL_COL, TEXT_COL]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples in the dataset, shown below, are grouped into 5 genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Data Split \n",
    "Using SKlearns (model selection library), split the MNLI Dataset into training and testing. Based on the setting of the `QUICK_RUN` flag, we'll be sampling a fraction of the data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/useradmin/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, train_size = TRAIN_SIZE, random_state = 0)\n",
    "df_train = df_train.sample(frac=TRAIN_DATA_FRACTION).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=TEST_DATA_FRACTION).reset_index(drop=True)\n",
    "train_text = df_train[TEXT_COL]\n",
    "test_text = df_test[TEXT_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[LABEL_COL].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the labels into numeric values\n",
    "Label Encoder makes it easy to encode dataset labels, categorical features into numerical values, between `0` and `n_classes - 1`; where `n` is the number of distinct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_data_labels = label_encoder.fit_transform(df_train[LABEL_COL])\n",
    "test_data_labels = label_encoder.fit_transform(df_test[LABEL_COL])\n",
    "\n",
    "# Count unique encoded labels\n",
    "num_labels = len(np.unique(train_data_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 5\n",
      "Number of training examples: 785\n",
      "Number of testing examples: 524\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unique labels: {num_labels}\")\n",
    "print(f\"Number of training examples: {df_train.shape[0]}\")\n",
    "print(f\"Number of testing examples: {df_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = SequenceClassifier.list_supported_models()[0]\n",
    "# num_labels = len(label_encoder.classes_)\n",
    "# processor = Processor(model_name=model_name, cache_dir=temp_dir)\n",
    "# ds = processor.preprocess(text_train, labels_train, max_len=max_len)\n",
    "# classifier = SequenceClassifier(\n",
    "#     model_name=model_name, num_labels=num_labels, cache_dir=temp_dir\n",
    "# )\n",
    "# classifier.fit(ds, device=\"cuda\", num_epochs=1, batch_size=32, num_gpus=None)\n",
    "# SequenceClassifier.list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data For Training.  \n",
    "\n",
    "Before training a model, the text document needs to be tokenized and converted to a list of tokens. Do the following steps to:  \n",
    "1. Create a PyTorch Processor - Prepare and Tokenize data  \n",
    "1. Initialize a RoBERTa PyTorch Transformer Processor \n",
    "1. Create a Dataset using the initialized processor  \n",
    "1. Initialize a Sequence Classifier\n",
    "1. Fit the newly created classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1001 17:17:09.713756 139876639942464 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at ./temp/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I1001 17:17:09.714879 139876639942464 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at ./temp/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "supported_models = SequenceClassifier.list_supported_models()\n",
    "assert TARGET_MODEL in supported_models, f\"Unfortunately {TARGET_MODEL} is not currently supported\"\n",
    "processor = Processor(model_name=TARGET_MODEL, cache_dir=MODEL_CACHE_DIR)\n",
    "train_dataset = processor.preprocess(text=train_text, labels=train_data_labels, max_len=MAX_LEN)\n",
    "test_dataset = processor.preprocess(text=test_text, labels=test_data_labels, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "Now, we will create a sequence classifier that loads a pre-trained RoBERTa model and the number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1001 17:17:10.692902 139876639942464 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ./temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.9dad9043216064080cf9dd3711c53c0f11fe2b09313eaa66931057b4bdcaf068\n",
      "I1001 17:17:10.694562 139876639942464 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 5,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I1001 17:17:11.002182 139876639942464 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ./temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I1001 17:17:14.504448 139876639942464 modeling_utils.py:405] Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I1001 17:17:14.505632 139876639942464 modeling_utils.py:408] Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n"
     ]
    }
   ],
   "source": [
    "classifier = SequenceClassifier(model_name=TARGET_MODEL, num_labels=num_labels, cache_dir=MODEL_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "We train the classifier  using the training examples from MNLI. This involves fine-tunning the transformer and a linear classification layer on top of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A/home/useradmin/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "                                            \n",
      "Epoch:   0%|          | 0/1 [00:02<?, ?it/s]     \n",
      "Iteration:   0%|          | 0/50 [00:02<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.652686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   2%|▏         | 1/50 [00:02<02:23,  2.92s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 2/50 [00:03<01:49,  2.28s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 3/50 [00:04<01:27,  1.85s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 4/50 [00:05<01:11,  1.56s/it]\u001b[A\n",
      "Iteration:  10%|█         | 5/50 [00:06<01:00,  1.35s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 6/50 [00:07<00:53,  1.21s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 7/50 [00:08<00:47,  1.11s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 8/50 [00:08<00:43,  1.04s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 9/50 [00:09<00:40,  1.01it/s]\u001b[A\n",
      "                                            38,  1.05it/s]\u001b[A\n",
      "Epoch:   0%|          | 0/1 [00:11<?, ?it/s]              \n",
      "Iteration:  20%|██        | 10/50 [00:11<00:38,  1.05it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.381926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  22%|██▏       | 11/50 [00:11<00:36,  1.08it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 12/50 [00:12<00:34,  1.10it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 13/50 [00:13<00:33,  1.12it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 14/50 [00:14<00:31,  1.13it/s]\u001b[A\n",
      "Iteration:  30%|███       | 15/50 [00:14<00:30,  1.13it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 16/50 [00:15<00:29,  1.14it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 17/50 [00:16<00:29,  1.14it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 18/50 [00:17<00:28,  1.14it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 19/50 [00:18<00:27,  1.14it/s]\u001b[A\n",
      "                                            26,  1.14it/s]\u001b[A\n",
      "Epoch:   0%|          | 0/1 [00:19<?, ?it/s]              \n",
      "Iteration:  40%|████      | 20/50 [00:19<00:26,  1.14it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.561228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  42%|████▏     | 21/50 [00:20<00:25,  1.14it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 22/50 [00:21<00:24,  1.14it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 23/50 [00:21<00:23,  1.14it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 24/50 [00:22<00:22,  1.14it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 25/50 [00:23<00:21,  1.14it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 26/50 [00:24<00:20,  1.15it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 27/50 [00:25<00:20,  1.14it/s]\u001b[A\n",
      "Iteration:  56%|█████▌    | 28/50 [00:26<00:19,  1.15it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 29/50 [00:27<00:18,  1.15it/s]\u001b[A\n",
      "                                            17,  1.15it/s]\u001b[A\n",
      "Epoch:   0%|          | 0/1 [00:28<?, ?it/s]              \n",
      "Iteration:  60%|██████    | 30/50 [00:28<00:17,  1.15it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.158241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  62%|██████▏   | 31/50 [00:28<00:16,  1.15it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 32/50 [00:29<00:15,  1.15it/s]\u001b[A\n",
      "Iteration:  66%|██████▌   | 33/50 [00:30<00:14,  1.15it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 34/50 [00:31<00:13,  1.15it/s]\u001b[A\n",
      "Iteration:  70%|███████   | 35/50 [00:32<00:13,  1.14it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 36/50 [00:33<00:12,  1.15it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 37/50 [00:34<00:11,  1.15it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 38/50 [00:35<00:10,  1.14it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 39/50 [00:35<00:09,  1.15it/s]\u001b[A\n",
      "                                            08,  1.15it/s]\u001b[A\n",
      "Epoch:   0%|          | 0/1 [00:37<?, ?it/s]              \n",
      "Iteration:  80%|████████  | 40/50 [00:37<00:08,  1.15it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.924342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  82%|████████▏ | 41/50 [00:37<00:07,  1.15it/s]\u001b[A\n",
      "Iteration:  84%|████████▍ | 42/50 [00:38<00:06,  1.15it/s]\u001b[A\n",
      "Iteration:  86%|████████▌ | 43/50 [00:39<00:06,  1.14it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 44/50 [00:40<00:05,  1.14it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 45/50 [00:41<00:04,  1.14it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 46/50 [00:42<00:03,  1.14it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 47/50 [00:42<00:02,  1.15it/s]\u001b[A\n",
      "Iteration:  96%|█████████▌| 48/50 [00:43<00:01,  1.15it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 49/50 [00:44<00:00,  1.15it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [00:45<00:00, 45.26s/it]7it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training time: 0.013 hrs]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    classifier.fit(train_dataset)\n",
    "print(\"[Training time: {:.3f} hrs]\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score\n",
    "\n",
    "We score the test set against the trained sequence classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 33/33 [00:09<00:00,  3.72it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = classifier.predict(test_dataset, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6793893129770993\n",
      "{\n",
      "    \"fiction\": {\n",
      "        \"f1-score\": 0.7094017094017093,\n",
      "        \"precision\": 0.6335877862595419,\n",
      "        \"recall\": 0.8058252427184466,\n",
      "        \"support\": 103\n",
      "    },\n",
      "    \"government\": {\n",
      "        \"f1-score\": 0.6229508196721312,\n",
      "        \"precision\": 0.4797979797979798,\n",
      "        \"recall\": 0.8878504672897196,\n",
      "        \"support\": 107\n",
      "    },\n",
      "    \"macro avg\": {\n",
      "        \"f1-score\": 0.6578039596633319,\n",
      "        \"precision\": 0.7356734902078413,\n",
      "        \"recall\": 0.6867853156090762,\n",
      "        \"support\": 524\n",
      "    },\n",
      "    \"micro avg\": {\n",
      "        \"f1-score\": 0.6793893129770993,\n",
      "        \"precision\": 0.6793893129770993,\n",
      "        \"recall\": 0.6793893129770993,\n",
      "        \"support\": 524\n",
      "    },\n",
      "    \"slate\": {\n",
      "        \"f1-score\": 0.24460431654676257,\n",
      "        \"precision\": 0.68,\n",
      "        \"recall\": 0.14912280701754385,\n",
      "        \"support\": 114\n",
      "    },\n",
      "    \"telephone\": {\n",
      "        \"f1-score\": 0.957345971563981,\n",
      "        \"precision\": 0.9619047619047619,\n",
      "        \"recall\": 0.9528301886792453,\n",
      "        \"support\": 106\n",
      "    },\n",
      "    \"travel\": {\n",
      "        \"f1-score\": 0.7547169811320755,\n",
      "        \"precision\": 0.9230769230769231,\n",
      "        \"recall\": 0.6382978723404256,\n",
      "        \"support\": 94\n",
      "    },\n",
      "    \"weighted avg\": {\n",
      "        \"f1-score\": 0.6489142654042407,\n",
      "        \"precision\": 0.7306279796836873,\n",
      "        \"recall\": 0.6793893129770993,\n",
      "        \"support\": 524\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(test_data_labels, preds, target_names=label_encoder.classes_, output_dict=True) \n",
    "accuracy = accuracy_score(test_data_labels, preds )\n",
    "print(\"accuracy: {}\".format(accuracy))\n",
    "print(json.dumps(report, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (nlp_gpu)",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
