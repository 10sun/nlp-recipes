{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Pipelines with Azure Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we fine-tune and evaluate a number of pretrained models on a subset of the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset using [Azure Machine Learning Pipelines](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines). Pipelines allow us to create sequential steps for preprocessing and training workflows, in addition to parallel steps that run independenly on a cluster of nodes. We demonstrate how one can submit model training jobs for multiple models, each consisting of multiple steps.\n",
    "\n",
    "We use a [sequence classifier](../../../utils_nlp/models/transformers/sequence_classification.py) that wraps [Hugging Face's PyTorch implementation](https://github.com/huggingface/transformers) of different transformers, like [BERT](https://github.com/google-research/bert), [XLNet](https://github.com/zihangdai/xlnet), and [RoBERTa](https://github.com/pytorch/fairseq).\n",
    "\n",
    "Below is a general illustration of the pipeline and its preprocessing and training steps.\n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/tc_pipeline_graph.PNG\" width=\"500\">\n",
    "\n",
    "The pipeline steps we chose are generic [Python script steps](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py) of the Azure ML SDK. This allows us to run parametrized Python scripts on a remote target. For this example, we will create pipeline steps that execute the preprocessing and training scripts provided in the [scripts](scripts) folder, with different arguments for different model types.\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "- [Define Parameters](#Define-Parameters)\n",
    "- [Create AML Workspace and Compute Target](#Create-AML-Workspace-and-Compute-Target)\n",
    "- [Upload Training Data to Workspace](#Upload-Training-Data-to-Workspace)\n",
    "- [Setup Execution Environment](#Setup-Execution-Environment)\n",
    "- [Define Pipeline Graph](#Define-Pipeline-Graph)\n",
    "- [Run Pipeline](#Run-Pipeline)\n",
    "- [Retrieve a Trained Model from Pipeline](#Retrieve-a-Trained-Model-from-Pipeline)\n",
    "- [Test Model](#Test-Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from azureml.core import Datastore, Environment, Experiment\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_GPU_IMAGE, RunConfiguration\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineRun\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from utils_nlp.azureml import azureml_utils\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "from utils_nlp.models.transformers.sequence_classification import Processor, SequenceClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_ID = \"\"\n",
    "RESOURCE_GROUP = \"\"\n",
    "WORKSPACE_NAME = \"\"\n",
    "WORKSPACE_REGION = \"\"  # \"southcentralus\", \"eastus\", etc\n",
    "\n",
    "# remote target\n",
    "CLUSTER_NAME = \"\"  # 2-16 chars\n",
    "VM_SIZE = \"STANDARD_NC12\"\n",
    "MIN_NODES = 0\n",
    "MAX_NODES = 2\n",
    "\n",
    "# local data\n",
    "TEMP_DIR = \"temp\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "# remote data\n",
    "REMOTE_DATA_CONTAINER = \"data\"\n",
    "\n",
    "# remote env config\n",
    "PIP_PACKAGES = [\"azureml-sdk==1.0.69\", \"torch==1.3\", \"tqdm==4.31.1\", \"transformers==2.1.1\"]\n",
    "CONDA_PACKAGES = [\"numpy\", \"scikit-learn\", \"pandas\"]\n",
    "UTILS_NLP_WHL_DIR = \"../../../dist\"\n",
    "PYTHON_VERSION = \"3.6.8\"\n",
    "USE_GPU = True\n",
    "\n",
    "# pipeline scripts\n",
    "SCRIPTS_DIR = \"scripts\"\n",
    "PREPROCESS_SCRIPT = \"preprocess.py\"\n",
    "TRAIN_SCRIPT = \"train.py\"\n",
    "TRAINED_MODEL_SUFFIX = \"_clf\"\n",
    "LABEL_ENCODER_SUFFIX = \"_le\"\n",
    "\n",
    "# pretrained models\n",
    "MODEL_NAMES = [\"bert-base-uncased\", \"xlnet-base-cased\"]\n",
    "\n",
    "# experiment\n",
    "EXPERIMENT_NAME = \"tc_pipeline_exp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML Workspace and Compute Target\n",
    "\n",
    "The following code block creates or retrieves an existing Azure ML workspace and a corresponding Azure ML compute target. For deep learning tasks, it is recommended that your compute nodes are GPU-enabled. Here, we're using a scalable cluster of size *(min_nodes, max_nodes)*. Setting *min_nodes* to zero ensures that the nodes are shutdown when not in use. Azure ML will allocate nodes as needed, up to *max_nodes*, and based on the jobs submitted to the compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/get AML workspace\n",
    "ws = azureml_utils.get_or_create_workspace(\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group=RESOURCE_GROUP,\n",
    "    workspace_name=WORKSPACE_NAME,\n",
    "    workspace_region=WORKSPACE_REGION,\n",
    ")\n",
    "\n",
    "# create/get compute target\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=CLUSTER_NAME)\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=VM_SIZE, min_nodes=MIN_NODES, max_nodes=MAX_NODES, vm_priority=\"lowpriority\"\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(\n",
    "        workspace=ws, name=CLUSTER_NAME, provisioning_configuration=compute_config\n",
    "    )\n",
    "    compute_target.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Training Data to Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use a subset of the MultiNLI dataset for fine-tuning the specified pre-trained models. The dataset contains a column of sentences (*sentence1*) which we will use as text input, and a *genre* column which we use as class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Therefore, they were not required to prepare a...</td>\n",
       "      <td>government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It could fall any time, said the Kal.</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A risk-neutral person is one who is indifferen...</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Click for Chatterbox's take on Snitchensgate.</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wow yeah and then it then it puts a lot of res...</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Therefore, they were not required to prepare a...  government\n",
       "1              It could fall any time, said the Kal.     fiction\n",
       "2  A risk-neutral person is one who is indifferen...       slate\n",
       "3     (Click for Chatterbox's take on Snitchensgate.       slate\n",
       "4  wow yeah and then it then it puts a lot of res...   telephone"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training data sample\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "df = load_pandas_df(TEMP_DIR)\n",
    "df[TEXT_COL] = df[\"sentence1\"]\n",
    "df[LABEL_COL] = df[\"genre\"]\n",
    "df[[TEXT_COL, LABEL_COL]].to_csv(\n",
    "    os.path.join(TEMP_DIR, TRAIN_FILE), header=True, index=None, quoting=1\n",
    ")\n",
    "# inspect dataset\n",
    "df[[TEXT_COL, LABEL_COL]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Azure ML workspace comes with a default datastore that is linked to an Azure Blob storage in the same resource group. We will use this datastore to upload the CSV data file. We will also use it for the intermediate output of the pipeline steps, as well as for the final output of the training step. In practice, one can create other datastores and link them to existing Blob Storage containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading temp/train.csv\n",
      "Uploaded temp/train.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_35a15e0fc32e4ff2bd8fa3c5a42e2426"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload data to datastore\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload_files(\n",
    "    files=[os.path.join(TEMP_DIR, TRAIN_FILE)],\n",
    "    target_path=REMOTE_DATA_CONTAINER,\n",
    "    overwrite=True,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Execution Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the *pip* and *conda* dependencies listed in the parameters section, we would need to include the packaged utils_nlp wheel file (this can be created by running *python3 setup.py bdist_wheel* from the root dir). The utils_nlp folder of this repo includes the transformer procesor and the classifier that we will fine-tune on the remote target. The *preprocess.py* and *train.py* [scripts](scripts) import the *utils_nlp* package, as they call the preprocessing and classification functions of its wrapper classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate utils_nlp whl file\n",
    "utils_nlp_whl_file = [x for x in os.listdir(UTILS_NLP_WHL_DIR) if x.endswith(\".whl\")][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env setup\n",
    "conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=CONDA_PACKAGES,\n",
    "    pip_packages=PIP_PACKAGES,\n",
    "    python_version=PYTHON_VERSION,\n",
    ")\n",
    "nlp_repo_whl = Environment.add_private_pip_wheel(\n",
    "    workspace=ws,\n",
    "    file_path=os.path.join(UTILS_NLP_WHL_DIR, utils_nlp_whl_file),\n",
    "    exist_ok=True,\n",
    ")\n",
    "conda_dependencies.add_pip_package(nlp_repo_whl)\n",
    "run_config = RunConfiguration(conda_dependencies=conda_dependencies)\n",
    "run_config.environment.docker.enabled = True\n",
    "if USE_GPU:\n",
    "    run_config.environment.docker.base_image = DEFAULT_GPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pipeline Graph\n",
    "\n",
    "As shown in the diagram earlier, the pipeline can be represented as a graph, where nodes represent execution steps. In this example we create a pipeline with two steps for each pretrained model we want to fine-tune. The processing and fine-tuning steps need to be executed in order. However, each sequence of these two steps can be executed in parallel for many types of models on multiple nodes of the compute cluster.\n",
    "\n",
    "For text classification, a number of pretrained-models are available from [Hugging Face's transformers package](https://github.com/huggingface/transformers), which is used within *utils_nlp*. Here, we include preprocessing and training steps for the *MODEL_NAMES* defined in the parameters section. You can list the supported pretrained models using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert-base-uncased', 'bert-large-uncased', 'bert-base-cased', 'bert-large-cased', 'bert-base-multilingual-uncased', 'bert-base-multilingual-cased', 'bert-base-chinese', 'bert-base-german-cased', 'bert-large-uncased-whole-word-masking', 'bert-large-cased-whole-word-masking', 'bert-large-uncased-whole-word-masking-finetuned-squad', 'bert-large-cased-whole-word-masking-finetuned-squad', 'bert-base-cased-finetuned-mrpc', 'bert-base-german-dbmdz-cased', 'bert-base-german-dbmdz-uncased', 'roberta-base', 'roberta-large', 'roberta-large-mnli', 'xlnet-base-cased', 'xlnet-large-cased', 'distilbert-base-uncased', 'distilbert-base-uncased-distilled-squad']\n"
     ]
    }
   ],
   "source": [
    "print(SequenceClassifier.list_supported_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline input\n",
    "input_dir = DataReference(\n",
    "    datastore=ds,\n",
    "    data_reference_name=\"input_dir\",\n",
    "    path_on_datastore=REMOTE_DATA_CONTAINER,\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "# create 2 pipeline steps (preprocessing & training) for each model\n",
    "all_steps = []\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "\n",
    "    # intermediate output\n",
    "    preprocess_dir = PipelineData(\n",
    "        name=\"preprocessed\",\n",
    "        datastore=ds,\n",
    "        output_path_on_compute=REMOTE_DATA_CONTAINER + \"/\" + \"preprocessed_\" + model_name,\n",
    "    )\n",
    "    # final output\n",
    "    output_dir = PipelineData(\n",
    "        name=\"trained\",\n",
    "        datastore=ds,\n",
    "        output_path_on_compute=REMOTE_DATA_CONTAINER + \"/\" + \"trained_\" + model_name,\n",
    "    )\n",
    "\n",
    "    preprocess_step = PythonScriptStep(\n",
    "        name=\"preprocess_step_{}\".format(model_name),\n",
    "        arguments=[input_dir, TRAIN_FILE, preprocess_dir, TEXT_COL, LABEL_COL, model_name],\n",
    "        script_name=PREPROCESS_SCRIPT,\n",
    "        inputs=[input_dir],\n",
    "        outputs=[preprocess_dir],\n",
    "        source_directory=SCRIPTS_DIR,\n",
    "        compute_target=compute_target,\n",
    "        runconfig=run_config,\n",
    "        allow_reuse=False,\n",
    "    )\n",
    "\n",
    "    train_step = PythonScriptStep(\n",
    "        name=\"train_step_{}\".format(model_name),\n",
    "        arguments=[preprocess_dir, output_dir, model_name, MAX_NODES],\n",
    "        script_name=TRAIN_SCRIPT,\n",
    "        inputs=[preprocess_dir],\n",
    "        outputs=[output_dir],\n",
    "        source_directory=SCRIPTS_DIR,\n",
    "        compute_target=compute_target,\n",
    "        runconfig=run_config,\n",
    "        allow_reuse=False,\n",
    "    )\n",
    "\n",
    "    # set execution order of steps\n",
    "    train_step.run_after(preprocess_step)\n",
    "\n",
    "    all_steps.append(preprocess_step)\n",
    "    all_steps.append(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image is an example of how the pipeline graph generated by Azure ML looks like. This particular graph example represents a pipeline submitted for 2 models with a total of 4 steps.\n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/pipeline_graph_example.PNG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline\n",
    "\n",
    "Once the pipeline and its steps are defined, we can create an experiment in the Azure ML workspace and submit a pipeline run as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=[all_steps])\n",
    "experiment_name = EXPERIMENT_NAME + datetime.now().strftime(\"%H%M%S\")\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "pipeline_run = experiment.submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=False)\n",
    "pipeline_run_id = pipeline_run.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve a Trained Model from Pipeline\n",
    "\n",
    "The Azure ML SDK allows retrieving the pipeline runs and steps using the run id and step name. The following example downloads the output of the training step of the first model in *MODEL_NAMES*, which includes the fine-tuned classifier and the label_encoder used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve an existing training step & download corresponding model\n",
    "# (from an existing experiment and pipeline run)\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "pipeline_run = PipelineRun(experiment, pipeline_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the training step for the first model\n",
    "train_step_run = pipeline_run.find_step_run(\"train_step_{}\".format(MODEL_NAMES[0]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_step_run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b6e7c53d3a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# download its output (a traind model & a label encoder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_step_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEMP_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load classifier and label encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrained_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./temp/azureml/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_step_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_step_run' is not defined"
     ]
    }
   ],
   "source": [
    "# download its output (a traind model & a label encoder)\n",
    "train_step_run.get_output_data(output_dir.name).download(local_path=TEMP_DIR)\n",
    "\n",
    "# load classifier and label encoder\n",
    "trained_dir = \"./temp/azureml/\" + train_step_run.id + \"/\" + output_dir.name\n",
    "classifier = pickle.load(\n",
    "    open(trained_dir + \"/\" + MODEL_NAMES[0] + TRAINED_MODEL_SUFFIX, \"rb\")\n",
    ")\n",
    "label_encoder = pickle.load(\n",
    "    open(trained_dir + \"/\" + MODEL_NAMES[0] + LABEL_ENCODER_SUFFIX, \"rb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "Finally, we can test the model by scoring some text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['fiction'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "test_input = [\"Let's go to Orlando. I've heard it's a nice place\"]\n",
    "processor = Processor(model_name=MODEL_NAMES[0], cache_dir=TEMP_DIR)\n",
    "test_ds = processor.preprocess(test_input, max_len=150)\n",
    "pred = classifier.predict(test_ds, device=\"cpu\")\n",
    "label_encoder.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "nlp_gpu",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
