{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Pipelines with Azure Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we fine-tune and evaluate a number of pretrained models on a subset of the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset using [Azure Machine Learning Pipelines](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines). Pipelines allow us to create sequential steps for preprocessing and training workflows, in addition to parallel steps that run independenly on a cluster of nodes. We demonstrate how one can submit model training jobs for multiple models, each consisting of multiple steps.\n",
    "\n",
    "We use a [sequence classifier](../../../utils_nlp/models/transformers/sequence_classification.py) that wraps [Hugging Face's PyTorch implementation](https://github.com/huggingface/transformers) of different transformers, like [BERT](https://github.com/google-research/bert), [XLNet](https://github.com/zihangdai/xlnet), and [RoBERTa](https://github.com/pytorch/fairseq).\n",
    "\n",
    "Below is a general illustration of the pipeline and its preprocessing and training steps.\n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/tc_pipeline_graph.PNG\" width=500>\n",
    "\n",
    "The pipeline steps we chose are generic [Python script steps](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py) of the Azure ML SDK. This allows us to run parametrized Python scripts on a remote target. For this example, we will create pipeline steps that execute the preprocessing and training scripts provided in the [scripts](scripts) folder, with different arguments for different model types.\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "- [Define Parameters](#Define-Parameters)\n",
    "- [Create AML Workspace and Compute Target](#Create-AML-Workspace-and-Compute-Target)\n",
    "- [Upload Training Data to Workspace](#Upload-Training-Data-to-Workspace)\n",
    "- [Setup Execution Environment](#Setup-Execution-Environment)\n",
    "- [Define Pipeline Graph](#Define-Pipeline-Graph)\n",
    "- [Run Pipeline](#Run-Pipeline)\n",
    "- [Retrieve a Trained Model from Pipeline](#Retrieve-a-Trained-Model-from-Pipeline)\n",
    "- [Test Model](#Test-Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from azureml.core import Datastore, Environment, Experiment\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineRun\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.widgets import RunDetails\n",
    "from utils_nlp.azureml import azureml_utils\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "from utils_nlp.models.transformers.sequence_classification import Processor, SequenceClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_ID = \"\"\n",
    "RESOURCE_GROUP = \"ignite-demo\"\n",
    "WORKSPACE_NAME = \"ignite-nlp-amlws\"\n",
    "WORKSPACE_REGION = \"eastus\"\n",
    "\n",
    "# remote target\n",
    "CLUSTER_NAME = \"ignite-nlp-clstr\"  # 2-16 chars\n",
    "VM_SIZE = \"STANDARD_NC12\"\n",
    "MIN_NODES = 0\n",
    "MAX_NODES = 2\n",
    "\n",
    "# local data\n",
    "TEMP_DIR = \"temp\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "TRAIN_SAMPLE_SIZE = 10000\n",
    "# remote data\n",
    "REMOTE_DATA_CONTAINER = \"data\"\n",
    "\n",
    "# remote env config\n",
    "PIP_PACKAGES = [\"azureml-sdk==1.0.65\", \"torch==1.1\", \"tqdm==4.31.1\", \"transformers==2.1.1\"]\n",
    "CONDA_PACKAGES = [\"numpy\", \"scikit-learn\", \"pandas\"]\n",
    "UTILS_NLP_WHL_DIR = \"../../../dist\"\n",
    "PYTHON_VERSION = \"3.6.8\"\n",
    "USE_GPU = True\n",
    "\n",
    "# pipeline scripts\n",
    "SCRIPTS_DIR = \"scripts\"\n",
    "PREPROCESS_SCRIPT = \"preprocess.py\"\n",
    "TRAIN_SCRIPT = \"train.py\"\n",
    "\n",
    "# pretrained models\n",
    "MODEL_NAMES = [\"bert-base-uncased\", \"xlnet-base-cased\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML Workspace and Compute Target\n",
    "\n",
    "The following code block creates or retrieves an existing Azure ML workspace and a corresponding Azure ML compute target. For deep learning tasks, it is recommended that your compute nodes are GPU-enabled. Here, we're using a scalable cluster of size *(min_nodes, max_nodes)*. Setting *min_nodes* to zero ensures that the nodes are shutdown when not in use. Azure ML will allocate nodes as needed, up to *max_nodes*, and based on the jobs submitted to the compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/get AML workspace\n",
    "ws = azureml_utils.get_or_create_workspace(\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group=RESOURCE_GROUP,\n",
    "    workspace_name=WORKSPACE_NAME,\n",
    "    workspace_region=WORKSPACE_REGION,\n",
    ")\n",
    "\n",
    "# create/get compute target\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=CLUSTER_NAME)\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=VM_SIZE, min_nodes=MIN_NODES, max_nodes=MAX_NODES, vm_priority=\"lowpriority\"\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(\n",
    "        workspace=ws, name=CLUSTER_NAME, provisioning_configuration=compute_config\n",
    "    )\n",
    "    compute_target.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Training Data to Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use a subset of the MultiNLI dataset for fine-tuning the specified pre-trained models. The dataset contains a column of sentences (*sentence1*) which we will use as text input, and a *genre* column which we use as class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31802</th>\n",
       "      <td>As Buchanan put it, the boys in the War Room h...</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380426</th>\n",
       "      <td>But if we cited your telephone number instead,...</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321741</th>\n",
       "      <td>And while some, such as Goodwin, argue that th...</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58393</th>\n",
       "      <td>Many Poles, especially among the intellectual ...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138205</th>\n",
       "      <td>The small but significant Craft and Folk Art M...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text   label\n",
       "31802   As Buchanan put it, the boys in the War Room h...   slate\n",
       "380426  But if we cited your telephone number instead,...   slate\n",
       "321741  And while some, such as Goodwin, argue that th...   slate\n",
       "58393   Many Poles, especially among the intellectual ...  travel\n",
       "138205  The small but significant Craft and Folk Art M...  travel"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training data sample\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "df = load_pandas_df(TEMP_DIR, \"train\")\n",
    "df = df[df[\"gold_label\"] == \"neutral\"] # filter duplicate sentences\n",
    "df = df.sample(TRAIN_SAMPLE_SIZE)\n",
    "df[TEXT_COL] = df[\"sentence1\"]\n",
    "df[LABEL_COL] = df[\"genre\"]\n",
    "df[[TEXT_COL, LABEL_COL]].to_csv(\n",
    "    os.path.join(TEMP_DIR, TRAIN_FILE), header=True, index=None, quoting=1\n",
    ")\n",
    "# inspect dataset\n",
    "df[[TEXT_COL, LABEL_COL]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Azure ML workspace comes with a default datastore that is linked to an Azure Blob storage in the same resource group. We will use this datastore to upload the CSV data file. We will also use it for the intermediate output of the pipeline steps, as well as for the final output of the training step. In practice, one can create other datastores and link them to existing Blob Storage containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading temp/train.csv\n",
      "Uploaded temp/train.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_6c40ab853af64e0b968e85f0cd51d0e5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload data to datastore\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload_files(\n",
    "    files=[os.path.join(TEMP_DIR, TRAIN_FILE)],\n",
    "    target_path=REMOTE_DATA_CONTAINER,\n",
    "    overwrite=True,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Execution Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the *pip* and *conda* dependencies listed in the parameters section, we would need to include the packaged utils_nlp wheel file. The utils_nlp folder of this repo includes the transformer procesor and the classifier that we will fine-tune on the remote target. The *preprocess.py* and *train.py* [scripts](scripts) import the *utils_nlp* package, as they call the preprocessing and classification functions of its wrapper classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate utils_nlp whl file\n",
    "utils_nlp_whl_file = [x for x in os.listdir(UTILS_NLP_WHL_DIR) if x.endswith(\".whl\")][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env setup\n",
    "conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=CONDA_PACKAGES,\n",
    "    pip_packages=PIP_PACKAGES,\n",
    "    python_version=PYTHON_VERSION,\n",
    ")\n",
    "nlp_repo_whl = Environment.add_private_pip_wheel(\n",
    "    workspace=ws,\n",
    "    file_path=os.path.join(UTILS_NLP_WHL_DIR, utils_nlp_whl_file),\n",
    "    exist_ok=True,\n",
    ")\n",
    "conda_dependencies.add_pip_package(nlp_repo_whl)\n",
    "run_config = RunConfiguration(conda_dependencies=conda_dependencies)\n",
    "run_config.environment.docker.enabled = True\n",
    "if USE_GPU:\n",
    "    run_config.environment.docker.base_image = azureml.core.runconfig.DEFAULT_GPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pipeline Graph\n",
    "\n",
    "As shown in the diagram earlier, the pipeline can be represented as a graph, where nodes represent execution steps. In this example we create a pipeline with two steps for each pretrained model we want to fine-tune. The processing and fine-tuning steps need to be executed in order. However, each sequence of these two steps can be executed in parallel for many types of models on multiple nodes of the compute cluster.\n",
    "\n",
    "For text classification, a number of pretrained-models are available from [Hugging Face's transformers package](https://github.com/huggingface/transformers), which is used within *utils_nlp*. Here, we include preprocessing and training steps for the *MODEL_NAMES* defined in the parameters section. You can list the supported pretrained models using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert-base-uncased', 'bert-large-uncased', 'bert-base-cased', 'bert-large-cased', 'bert-base-multilingual-uncased', 'bert-base-multilingual-cased', 'bert-base-chinese', 'bert-base-german-cased', 'bert-large-uncased-whole-word-masking', 'bert-large-cased-whole-word-masking', 'bert-large-uncased-whole-word-masking-finetuned-squad', 'bert-large-cased-whole-word-masking-finetuned-squad', 'bert-base-cased-finetuned-mrpc', 'bert-base-german-dbmdz-cased', 'bert-base-german-dbmdz-uncased', 'roberta-base', 'roberta-large', 'roberta-large-mnli', 'xlnet-base-cased', 'xlnet-large-cased', 'distilbert-base-uncased', 'distilbert-base-uncased-distilled-squad']\n"
     ]
    }
   ],
   "source": [
    "print(SequenceClassifier.list_supported_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = DataReference(\n",
    "    datastore=ds,\n",
    "    data_reference_name=\"input_dir\",\n",
    "    path_on_datastore=REMOTE_DATA_CONTAINER,\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "# create pipeline steps\n",
    "all_steps = []\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "\n",
    "    preprocess_dir = PipelineData(\n",
    "        name=\"preprocessed\",\n",
    "        datastore=ds,\n",
    "        output_path_on_compute=REMOTE_DATA_CONTAINER + \"/\" + \"preprocessed_\" + model_name,\n",
    "    )\n",
    "\n",
    "    output_dir = PipelineData(\n",
    "        name=\"trained\",\n",
    "        datastore=ds,\n",
    "        output_path_on_compute=REMOTE_DATA_CONTAINER + \"/\" + \"trained_\" + model_name,\n",
    "    )\n",
    "\n",
    "    preprocess_step = PythonScriptStep(\n",
    "        name=\"preprocess_step_{}\".format(model_name),\n",
    "        arguments=[input_dir, TRAIN_FILE, preprocess_dir, TEXT_COL, LABEL_COL, model_name],\n",
    "        script_name=PREPROCESS_SCRIPT,\n",
    "        inputs=[input_dir],\n",
    "        outputs=[preprocess_dir],\n",
    "        source_directory=SCRIPTS_DIR,\n",
    "        compute_target=compute_target,\n",
    "        runconfig=run_config,\n",
    "        allow_reuse=False,\n",
    "    )\n",
    "\n",
    "    train_step = PythonScriptStep(\n",
    "        name=\"train_step_{}\".format(model_name),\n",
    "        arguments=[preprocess_dir, output_dir, model_name, MAX_NODES],\n",
    "        script_name=TRAIN_SCRIPT,\n",
    "        inputs=[preprocess_dir],\n",
    "        outputs=[output_dir],\n",
    "        source_directory=SCRIPTS_DIR,\n",
    "        compute_target=compute_target,\n",
    "        runconfig=run_config,\n",
    "        allow_reuse=False,\n",
    "    )\n",
    "\n",
    "    train_step.run_after(preprocess_step)\n",
    "\n",
    "    all_steps.append(preprocess_step)\n",
    "    all_steps.append(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline\n",
    "\n",
    "Once the pipeline and its steps are defined, we can create an experiment in the Azure ML workspace and submit a pipeline run as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step preprocess_step_bert-base-uncased [9e0b4f42][d967ea09-c7a8-45e7-adba-44c9a3ccc8a9], (This step will run and generate new outputs)\n",
      "Created step train_step_bert-base-uncased [4eb92d51][5ebdf951-fecf-4384-a497-ec2be1878246], (This step will run and generate new outputs)\n",
      "Created step preprocess_step_xlnet-base-cased [2f85a9f4][df2b656c-04c0-4246-b1a0-a9dd479656a7], (This step will run and generate new outputs)\n",
      "Created step train_step_xlnet-base-cased [28ddb914][82bfe567-ddb1-4b6a-ac5b-09a89bbba91d], (This step will run and generate new outputs)\n",
      "Using data reference input_dir for StepId [142359a7][c1b309f5-1049-4014-8fd7-794c0b242c46], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference input_dir for StepId [326d0b6b][c1b309f5-1049-4014-8fd7-794c0b242c46], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted pipeline run: a8c5b459-3dd0-4cea-9f8f-003df5fd3309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8974208ac3274370b036b0d5da6e6150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=[all_steps])\n",
    "experiment_name = \"nlpatIgnite_\" + datetime.now().strftime(\"%H%M%S\")\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "pipeline_run = experiment.submit(pipeline)\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run_id = pipeline_run.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve a Trained Model from Pipeline\n",
    "\n",
    "The Azure ML SDK allows retrieving the pipeline runs and steps using the run id and step name. The following example downloads the output of the training step of the first model in *MODEL_NAMES*, which includes the fine-tuned classifier and the label_encoder used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve an existing training step & download corresponding model\n",
    "# (from an existing experiment and pipeline run)\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "pipeline_run = PipelineRun(experiment, pipeline_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>nlpatIgnite_214131</td><td>7784250a-bbf1-47d3-85df-942b98c9b244</td><td>azureml.StepRun</td><td>Completed</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/9086b59a-02d7-4687-b3fd-e39fa5e0fd9b/resourceGroups/ignite-demo/providers/Microsoft.MachineLearningServices/workspaces/ignite-nlp-amlws/experiments/nlpatIgnite_214131/runs/7784250a-bbf1-47d3-85df-942b98c9b244\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: nlpatIgnite_214131,\n",
       "Id: 7784250a-bbf1-47d3-85df-942b98c9b244,\n",
       "Type: azureml.StepRun,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step_run = pipeline_run.find_step_run(\"train_step_{}\".format(MODEL_NAMES[0]))[0]\n",
    "train_step_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/bleik2/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/sklearn/base.py:253: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.21.3 when using version 0.20.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# download\n",
    "train_step_run.get_output_data(output_dir.name).download(local_path=TEMP_DIR)\n",
    "\n",
    "# load classifier and label encoder\n",
    "trained_dir = (\n",
    "    \"./temp/azureml/\" + train_step_run.id + \"/\" + output_dir.name \n",
    ")\n",
    "classifier = pickle.load(open(trained_dir + \"/\" + MODEL_NAMES[0] + \"_clf\", \"rb\"))\n",
    "label_encoder = pickle.load(open(trained_dir + \"/\" + MODEL_NAMES[0] + \"_le\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "Finally, we can test the model by scoring some text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['fiction'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "test_input = [\"Let's go to Orlando. I've heard it's a nice place\"]\n",
    "processor = Processor(model_name=MODEL_NAMES[0], cache_dir=TEMP_DIR)\n",
    "test_ds = processor.preprocess(test_input, max_len=150)\n",
    "pred = classifier.predict(test_ds, device=\"cpu\")\n",
    "label_encoder.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:nlp_gpu]",
   "language": "python",
   "name": "conda-env-nlp_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
