{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Using BERT\n",
    "## Summary\n",
    "This notebook demonstrates how to fine tune [pretrained BERT model](https://github.com/huggingface/pytorch-pretrained-BERT) for token level named entity recognition (NER) task. A few utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, and model evaluation. \n",
    "\n",
    "[BERT (Bidirectional Transformers forLanguage Understanding)](https://arxiv.org/pdf/1810.04805.pdf) is a powerful pre-trained lanaguage model that can be used for multiple NLP tasks, including text classification, question answering, named entity recognition. It's able to achieve state of the art performance with only a few epochs of fine tuning.  \n",
    "The figure below illustrates how BERT can be fine tuned for NER tasks. The input data is a list of tokens representing a sentence. In the training data, each token has an entity label. After fine tuning, the model predicts an entity label for each token of a given sentence in the testing data. \n",
    "\n",
    "![](bert_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages\n",
    "* pytorch\n",
    "* pytorch-pretrained-bert\n",
    "* pandas\n",
    "* seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import random\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "bert_utils_path = os.path.abspath('../../utils_nlp/bert')\n",
    "if bert_utils_path not in sys.path:\n",
    "    sys.path.insert(0, bert_utils_path)\n",
    "\n",
    "from bert_data_utils import KaggleNERProcessor\n",
    "from bert_utils import BertTokenClassifier, postprocess_token_labels\n",
    "\n",
    "from common_ner import Tokenizer, create_data_loader, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f27da7229f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data_dir = \"./data/NER/ner_dataset.csv\"\n",
    "cache_dir=\".\"\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configurations\n",
    "language = Language.ENGLISH\n",
    "do_lower_case = True\n",
    "max_seq_length = 75\n",
    "\n",
    "# training configurations\n",
    "device=\"gpu\"\n",
    "batch_size = 32\n",
    "num_train_epochs = 2\n",
    "\n",
    "# optimizer configurations\n",
    "learning_rate = 3e-5\n",
    "clip_gradient = True\n",
    "max_gradient_norm = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and validation examples\n",
    "`KaggleNERProcessor` is a dataset specific class that splits the whole dataset into training and validation datasets according to `dev_percentage`. The `get_train_examples` and `get_dev_examples` return the training and validation datasets respectively. The `get_labels` method returns a list of all unique labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kaggle_ner_processor = KaggleNERProcessor(data_dir=ner_data_dir, dev_percentage = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-art', 'I-art', 'I-org', 'B-per', 'B-geo', 'B-tim', 'I-per', 'B-gpe', 'I-tim', 'B-nat', 'I-nat', 'I-geo', 'B-eve', 'B-org', 'O', 'I-eve', 'I-gpe', 'X']\n"
     ]
    }
   ],
   "source": [
    "train_text, train_labels = kaggle_ner_processor.get_train_examples()\n",
    "dev_text, dev_labels = kaggle_ner_processor.get_dev_examples()\n",
    "label_list = kaggle_ner_processor.get_labels()\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KaggleNERProcessor` generates training and evaluation examples in `BertInputData` type. `BertInputData` is a `namedtuple` with the following three fields:\n",
    "* text_a: text string of the first sentence.\n",
    "* text_b: text string of the second setence. This is only required for two-sentence tasks.\n",
    "* label: required for training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence: \n",
      "Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "Sample sentence labels: \n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Sample sentence: \\n{}\\n'.format(train_text[0]))\n",
    "print('Sample sentence labels: \\n{}\\n'.format(train_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw input to numerical features\n",
    "The `preprocess_ner_tokens` of the tokenizer preprocess converts raw string data to numerical features, involving the following steps:\n",
    "1. Tokenization.\n",
    "2. Convert tokens and labels to numerical values, i.e. token ids and label ids.\n",
    "3. Sequence padding or truncation according to the `max_seq_length` configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dictionary that maps labels to numerical values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(language=language, \n",
    "                      to_lower=do_lower_case, \n",
    "                      cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create numerical features**  \n",
    "Note there is an argument called `trailing_piece_tag`. BERT uses a WordPiece tokenizer which breaks down some words into multiple tokens, e.g. \"playing\" is tokenized into \"play\" and \"##ing\". Since the input data only come with one token label for \"playing\", within `create_token_feature_dataset`, the original token label is assigned to the first token \"play\" and the second token \"##ing\" is labeled as \"X\". By default, `trailing_piece_tag` is set to \"X\". If your \"X\" already exists in your data, you can set `trailing_piece_tag` to another value that doesn't exist in your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ids, train_input_mask, train_label_ids = tokenizer.preprocess_ner_tokens(text=train_text,\n",
    "                                                                                      label_map=label_map,\n",
    "                                                                                      max_seq_length=max_seq_length,\n",
    "                                                                                      labels=train_labels,\n",
    "                                                                                      trailing_piece_tag=\"X\")\n",
    "dev_token_ids, dev_input_mask, dev_label_ids = tokenizer.preprocess_ner_tokens(text=dev_text,\n",
    "                                                                               label_map=label_map,\n",
    "                                                                               max_seq_length=max_seq_length,\n",
    "                                                                               labels=dev_labels,\n",
    "                                                                               trailing_piece_tag=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokenizer.preprocess_ner_tokens` outputs three lists of numerical features: \n",
    "1. token ids: numerical values each corresponds to a token.\n",
    "2. attention mask: 1 for input tokens and 0 for padded tokens, so that padded tokens are not attended to. \n",
    "4. label ids: numerical values each corresponds to an entity label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample token id:\n",
      "[5190, 1997, 28337, 2031, 9847, 2083, 2414, 2000, 6186, 1996, 2162, 1999, 5712, 1998, 5157, 1996, 10534, 1997, 2329, 3629, 2013, 2008, 2406, 1012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample attention mask:\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample label ids:\n",
      "[14, 14, 14, 14, 14, 14, 4, 14, 14, 14, 14, 14, 4, 14, 14, 14, 14, 14, 7, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample token id:\\n{}\\n\".format(train_token_ids[0]))\n",
    "print(\"Sample attention mask:\\n{}\\n\".format(train_input_mask[0]))\n",
    "print(\"Sample label ids:\\n{}\\n\".format(train_label_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Token Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_classifier = BertTokenClassifier(language=Language.ENGLISH,\n",
    "                                       num_labels=len(label_list),\n",
    "                                       cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/1349 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 27/1349 [00:30<24:52,  1.13s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 27/1349 [00:49<24:52,  1.13s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 55/1349 [01:01<24:05,  1.12s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 55/1349 [01:20<24:05,  1.12s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 83/1349 [01:31<23:29,  1.11s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 83/1349 [01:50<23:29,  1.11s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 111/1349 [02:02<22:49,  1.11s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 111/1349 [02:20<22:49,  1.11s/it]\u001b[A\n",
      "Iteration:  10%|█         | 139/1349 [02:33<22:14,  1.10s/it]\u001b[A\n",
      "Iteration:  10%|█         | 139/1349 [02:50<22:14,  1.10s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 167/1349 [03:03<21:42,  1.10s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 167/1349 [03:20<21:42,  1.10s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 195/1349 [03:34<21:07,  1.10s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 195/1349 [03:50<21:07,  1.10s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 223/1349 [04:05<20:36,  1.10s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 223/1349 [04:20<20:36,  1.10s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 251/1349 [04:35<20:06,  1.10s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 251/1349 [04:50<20:06,  1.10s/it]\u001b[A\n",
      "Iteration:  21%|██        | 279/1349 [05:06<19:37,  1.10s/it]\u001b[A\n",
      "Iteration:  21%|██        | 279/1349 [05:20<19:37,  1.10s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 307/1349 [05:37<19:06,  1.10s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 307/1349 [05:50<19:06,  1.10s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 335/1349 [06:08<18:36,  1.10s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 335/1349 [06:20<18:36,  1.10s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 363/1349 [06:38<18:00,  1.10s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 363/1349 [06:50<18:00,  1.10s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 391/1349 [07:09<17:27,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 391/1349 [07:20<17:27,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|███       | 419/1349 [07:39<16:54,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|███       | 419/1349 [07:50<16:54,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 447/1349 [08:10<16:24,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 447/1349 [08:30<16:24,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 475/1349 [08:40<15:53,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 475/1349 [09:00<15:53,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 503/1349 [09:11<15:25,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 503/1349 [09:30<15:25,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 531/1349 [09:42<14:55,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 531/1349 [10:00<14:55,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 559/1349 [10:12<14:21,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 559/1349 [10:30<14:21,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 587/1349 [10:43<13:50,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 587/1349 [11:00<13:50,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 615/1349 [11:13<13:20,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 615/1349 [11:30<13:20,  1.09s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 643/1349 [11:44<12:50,  1.09s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 643/1349 [12:00<12:50,  1.09s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 671/1349 [12:14<12:18,  1.09s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 671/1349 [12:30<12:18,  1.09s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 699/1349 [12:45<11:49,  1.09s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 699/1349 [13:00<11:49,  1.09s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 727/1349 [13:15<11:16,  1.09s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 727/1349 [13:30<11:16,  1.09s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 755/1349 [13:45<10:45,  1.09s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 755/1349 [14:00<10:45,  1.09s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 783/1349 [14:16<10:16,  1.09s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 783/1349 [14:30<10:16,  1.09s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 811/1349 [14:47<09:47,  1.09s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 811/1349 [15:00<09:47,  1.09s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 839/1349 [15:17<09:17,  1.09s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 839/1349 [15:30<09:17,  1.09s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 867/1349 [15:48<08:45,  1.09s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 867/1349 [16:00<08:45,  1.09s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 895/1349 [16:18<08:13,  1.09s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 895/1349 [16:30<08:13,  1.09s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 923/1349 [16:49<07:44,  1.09s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 923/1349 [17:00<07:44,  1.09s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 951/1349 [17:19<07:13,  1.09s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 951/1349 [17:30<07:13,  1.09s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 979/1349 [17:50<06:43,  1.09s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 979/1349 [18:10<06:43,  1.09s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1007/1349 [18:21<06:13,  1.09s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1007/1349 [18:40<06:13,  1.09s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1035/1349 [18:51<05:44,  1.10s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1035/1349 [19:10<05:44,  1.10s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1063/1349 [19:22<05:13,  1.09s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1063/1349 [19:40<05:13,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1091/1349 [19:53<04:43,  1.10s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1091/1349 [20:10<04:43,  1.10s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1119/1349 [20:24<04:12,  1.10s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1119/1349 [20:40<04:12,  1.10s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1147/1349 [20:54<03:40,  1.09s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1147/1349 [21:10<03:40,  1.09s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 1175/1349 [21:25<03:10,  1.10s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 1175/1349 [21:40<03:10,  1.10s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 1203/1349 [21:56<02:40,  1.10s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 1203/1349 [22:10<02:40,  1.10s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 1230/1349 [22:26<02:11,  1.10s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 1230/1349 [22:40<02:11,  1.10s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 1258/1349 [22:57<01:40,  1.10s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 1258/1349 [23:10<01:40,  1.10s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 1286/1349 [23:28<01:09,  1.10s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 1286/1349 [23:40<01:09,  1.10s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1314/1349 [23:58<00:38,  1.10s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1314/1349 [24:10<00:38,  1.10s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 1342/1349 [24:29<00:07,  1.10s/it]\u001b[A\n",
      "Epoch:  50%|█████     | 1/2 [24:36<24:36, 1476.47s/it].09s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/1349 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.13670625791969876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   2%|▏         | 28/1349 [00:30<24:13,  1.10s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 28/1349 [00:43<24:13,  1.10s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 56/1349 [01:01<23:38,  1.10s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 56/1349 [01:13<23:38,  1.10s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 84/1349 [01:31<23:03,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 84/1349 [01:43<23:03,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 112/1349 [02:01<22:27,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 112/1349 [02:13<22:27,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|█         | 140/1349 [02:32<21:56,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|█         | 140/1349 [02:43<21:56,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 168/1349 [03:02<21:22,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 168/1349 [03:13<21:22,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 196/1349 [03:33<20:53,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 196/1349 [03:43<20:53,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 224/1349 [04:03<20:25,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 224/1349 [04:23<20:25,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 252/1349 [04:34<19:52,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 252/1349 [04:53<19:52,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██        | 280/1349 [05:04<19:24,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██        | 280/1349 [05:23<19:24,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 308/1349 [05:35<18:54,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 308/1349 [05:53<18:54,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 336/1349 [06:05<18:21,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 336/1349 [06:23<18:21,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 364/1349 [06:36<17:54,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 364/1349 [06:53<17:54,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 392/1349 [07:06<17:22,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 392/1349 [07:23<17:22,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|███       | 420/1349 [07:37<16:51,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|███       | 420/1349 [07:53<16:51,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 448/1349 [08:08<16:24,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 448/1349 [08:23<16:24,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 476/1349 [08:38<15:51,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 476/1349 [08:53<15:51,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 504/1349 [09:09<15:23,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 504/1349 [09:23<15:23,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 532/1349 [09:39<14:52,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 532/1349 [09:53<14:52,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 560/1349 [10:09<14:18,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 560/1349 [10:23<14:18,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 587/1349 [10:40<13:56,  1.10s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 587/1349 [10:53<13:56,  1.10s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 615/1349 [11:10<13:25,  1.10s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 615/1349 [11:23<13:25,  1.10s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 643/1349 [11:41<12:54,  1.10s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 643/1349 [11:53<12:54,  1.10s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 671/1349 [12:11<12:21,  1.09s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 671/1349 [12:23<12:21,  1.09s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 699/1349 [12:42<11:51,  1.10s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 699/1349 [12:53<11:51,  1.10s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 727/1349 [13:13<11:20,  1.09s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 727/1349 [13:23<11:20,  1.09s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 755/1349 [13:43<10:50,  1.10s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 755/1349 [14:03<10:50,  1.10s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 783/1349 [14:14<10:18,  1.09s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 783/1349 [14:33<10:18,  1.09s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 811/1349 [14:44<09:47,  1.09s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 811/1349 [15:03<09:47,  1.09s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 839/1349 [15:15<09:18,  1.10s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 839/1349 [15:33<09:18,  1.10s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 867/1349 [15:46<08:47,  1.10s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 867/1349 [16:03<08:47,  1.10s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 895/1349 [16:17<08:16,  1.09s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 895/1349 [16:33<08:16,  1.09s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 923/1349 [16:47<07:45,  1.09s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 923/1349 [17:03<07:45,  1.09s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 951/1349 [17:18<07:14,  1.09s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 951/1349 [17:33<07:14,  1.09s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 979/1349 [17:48<06:43,  1.09s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 979/1349 [18:03<06:43,  1.09s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1007/1349 [18:18<06:12,  1.09s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1007/1349 [18:33<06:12,  1.09s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1035/1349 [18:49<05:42,  1.09s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1035/1349 [19:03<05:42,  1.09s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1063/1349 [19:20<05:11,  1.09s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1063/1349 [19:33<05:11,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1091/1349 [19:50<04:41,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1091/1349 [20:03<04:41,  1.09s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1119/1349 [20:20<04:10,  1.09s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1119/1349 [20:33<04:10,  1.09s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1147/1349 [20:51<03:39,  1.09s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1147/1349 [21:03<03:39,  1.09s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 1175/1349 [21:21<03:09,  1.09s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 1175/1349 [21:33<03:09,  1.09s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 1203/1349 [21:51<02:38,  1.09s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 1203/1349 [22:03<02:38,  1.09s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 1231/1349 [22:22<02:08,  1.09s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 1231/1349 [22:33<02:08,  1.09s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 1259/1349 [22:53<01:38,  1.09s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 1259/1349 [23:03<01:38,  1.09s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 1287/1349 [23:23<01:07,  1.09s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 1287/1349 [23:33<01:07,  1.09s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1315/1349 [23:53<00:36,  1.09s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1315/1349 [24:13<00:36,  1.09s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 1343/1349 [24:24<00:06,  1.09s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 2/2 [49:07<00:00, 1474.84s/it].09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07746745187449534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "token_classifier.fit(token_ids=train_token_ids, \n",
    "                     input_mask=train_input_mask, \n",
    "                     labels=train_label_ids,\n",
    "                     device=device,\n",
    "                     num_epochs=num_train_epochs, \n",
    "                     batch_size=batch_size, \n",
    "                     learning_rate=learning_rate,\n",
    "                     clip_gradient=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_label_ids = token_classifier.predict(token_ids=dev_token_ids, \n",
    "                                          input_mask=dev_input_mask, \n",
    "                                          label_map=label_map,\n",
    "                                          labels=dev_label_ids, \n",
    "                                          batch_size=batch_size,\n",
    "                                          device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "The `predict` method of the token classifier outputs label ids for all tokens, including the padded tokens. `postprocess_token_labels` is a helper function that removes the predictions on padded tokens. If a `label_map` is provided, it maps the numerical label ids back to original token labels which are usually string type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_no_padding = postprocess_token_labels(pred_label_ids, dev_input_mask, label_map)\n",
    "true_tags_no_padding =  postprocess_token_labels(dev_label_ids, dev_input_mask, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8966012568936771\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score: {}\".format(f1_score(true_tags_no_padding, pred_tags_no_padding)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
