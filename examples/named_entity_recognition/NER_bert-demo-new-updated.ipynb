{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Using BERT\n",
    "## Summary\n",
    "This notebook demonstrates how to fine tune [pretrained BERT model](https://github.com/huggingface/pytorch-pretrained-BERT) for token level named entity recognition (NER) task. A few utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, and model evaluation. \n",
    "\n",
    "[BERT (Bidirectional Transformers forLanguage Understanding)](https://arxiv.org/pdf/1810.04805.pdf) is a powerful pre-trained lanaguage model that can be used for multiple NLP tasks, including text classification, question answering, named entity recognition. It's able to achieve state of the art performance with only a few epochs of fine tuning.  \n",
    "The figure below illustrates how BERT can be fine tuned for NER tasks. The input data is a list of tokens representing a sentence. In the training data, each token has an entity label. After fine tuning, the model predicts an entity label for each token of a given sentence in the testing data. \n",
    "\n",
    "![](bert_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages\n",
    "* pytorch\n",
    "* pytorch-pretrained-bert\n",
    "* pandas\n",
    "* seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import random\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "bert_utils_path = os.path.abspath('../../utils_nlp/bert')\n",
    "if bert_utils_path not in sys.path:\n",
    "    sys.path.insert(0, bert_utils_path)\n",
    "\n",
    "from bert_data_utils import KaggleNERProcessor\n",
    "from token_classification import BertTokenClassifier, postprocess_token_labels\n",
    "\n",
    "from common_ner import create_data_loader, Language, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9d9c1489f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data_dir = \"./data/NER/ner_dataset.csv\"\n",
    "cache_dir=\".\"\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configurations\n",
    "language = Language.ENGLISH\n",
    "do_lower_case = True\n",
    "max_seq_length = 75\n",
    "\n",
    "# training configurations\n",
    "device=\"gpu\"\n",
    "batch_size = 32\n",
    "num_train_epochs = 2\n",
    "\n",
    "# optimizer configurations\n",
    "learning_rate = 3e-5\n",
    "clip_gradient = True\n",
    "max_gradient_norm = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and validation examples\n",
    "`KaggleNERProcessor` is a dataset specific class that splits the whole dataset into training and validation datasets according to `dev_percentage`. The `get_train_examples` and `get_dev_examples` return the training and validation datasets respectively. The `get_labels` method returns a list of all unique labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kaggle_ner_processor = KaggleNERProcessor(data_dir=ner_data_dir, dev_percentage = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-nat', 'I-per', 'B-art', 'B-gpe', 'I-art', 'I-eve', 'B-eve', 'I-gpe', 'I-tim', 'B-per', 'I-nat', 'B-geo', 'I-org', 'B-org', 'O', 'I-geo', 'B-tim', 'X']\n"
     ]
    }
   ],
   "source": [
    "train_text, train_labels = kaggle_ner_processor.get_train_examples()\n",
    "dev_text, dev_labels = kaggle_ner_processor.get_dev_examples()\n",
    "label_list = kaggle_ner_processor.get_labels()\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KaggleNERProcessor` generates training and evaluation examples in `BertInputData` type. `BertInputData` is a `namedtuple` with the following three fields:\n",
    "* text_a: text string of the first sentence.\n",
    "* text_b: text string of the second setence. This is only required for two-sentence tasks.\n",
    "* label: required for training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence: \n",
      "Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "Sample sentence labels: \n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Sample sentence: \\n{}\\n'.format(train_text[0]))\n",
    "print('Sample sentence labels: \\n{}\\n'.format(train_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw input to numerical features\n",
    "The `preprocess_ner_tokens` of the tokenizer preprocess converts raw string data to numerical features, involving the following steps:\n",
    "1. Tokenization.\n",
    "2. Convert tokens and labels to numerical values, i.e. token ids and label ids.\n",
    "3. Sequence padding or truncation according to the `max_seq_length` configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dictionary that maps labels to numerical values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(language=language, \n",
    "                      to_lower=do_lower_case, \n",
    "                      cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create numerical features**  \n",
    "Note there is an argument called `trailing_piece_tag`. BERT uses a WordPiece tokenizer which breaks down some words into multiple tokens, e.g. \"playing\" is tokenized into \"play\" and \"##ing\". Since the input data only come with one token label for \"playing\", within `create_token_feature_dataset`, the original token label is assigned to the first token \"play\" and the second token \"##ing\" is labeled as \"X\". By default, `trailing_piece_tag` is set to \"X\". If your \"X\" already exists in your data, you can set `trailing_piece_tag` to another value that doesn't exist in your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ids, train_input_mask, train_trailing_token_mask, train_label_ids = \\\n",
    "    tokenizer.preprocess_ner_tokens(text=train_text,\n",
    "                                    label_map=label_map,\n",
    "                                    max_seq_length=max_seq_length,\n",
    "                                    labels=train_labels,\n",
    "                                    trailing_piece_tag=\"X\")\n",
    "dev_token_ids, dev_input_mask, dev_trailing_token_mask, dev_label_ids = \\\n",
    "    tokenizer.preprocess_ner_tokens(text=dev_text,\n",
    "                                    label_map=label_map,\n",
    "                                    max_seq_length=max_seq_length,\n",
    "                                    labels=dev_labels,\n",
    "                                    trailing_piece_tag=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokenizer.preprocess_ner_tokens` outputs three lists of numerical features: \n",
    "1. token ids: list of numerical values each corresponds to a token.\n",
    "2. attention mask: list of 1s and 0s, 1 for input tokens and 0 for padded tokens, so that padded tokens are not attended to. \n",
    "3. trailing word piece mask: boolean list, True for the first word piece of each original word, False for the trailing word pieces, e.g. ##ing. This mask is useful for removing the predictions on trailing word pieces, so that each original word in the input text has a unique predicted label. \n",
    "4. label ids: list of numerical values each corresponds to an entity label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample token ids:\n",
      "[5190, 1997, 28337, 2031, 9847, 2083, 2414, 2000, 6186, 1996, 2162, 1999, 5712, 1998, 5157, 1996, 10534, 1997, 2329, 3629, 2013, 2008, 2406, 1012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample attention mask:\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Sample trailing token mask:\n",
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "\n",
      "Sample label ids:\n",
      "[14, 14, 14, 14, 14, 14, 11, 14, 14, 14, 14, 14, 11, 14, 14, 14, 14, 14, 3, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample token ids:\\n{}\\n\".format(train_token_ids[0]))\n",
    "print(\"Sample attention mask:\\n{}\\n\".format(train_input_mask[0]))\n",
    "print(\"Sample trailing token mask:\\n{}\\n\".format(train_trailing_token_mask[0]))\n",
    "print(\"Sample label ids:\\n{}\\n\".format(train_label_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Token Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "token_classifier = BertTokenClassifier(language=Language.ENGLISH,\n",
    "                                       num_labels=len(label_list),\n",
    "                                       cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/1349 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 28/1349 [00:30<23:52,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 28/1349 [00:49<23:52,  1.08s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 56/1349 [01:00<23:25,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 56/1349 [01:20<23:25,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 84/1349 [01:31<22:58,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 84/1349 [01:50<22:58,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 112/1349 [02:02<22:30,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 112/1349 [02:20<22:30,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|█         | 140/1349 [02:33<22:04,  1.10s/it]\u001b[A\n",
      "Iteration:  10%|█         | 140/1349 [02:50<22:04,  1.10s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 168/1349 [03:04<21:37,  1.10s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 168/1349 [03:20<21:37,  1.10s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 195/1349 [03:34<21:13,  1.10s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 195/1349 [03:50<21:13,  1.10s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 223/1349 [04:04<20:38,  1.10s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 223/1349 [04:20<20:38,  1.10s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 251/1349 [04:35<20:06,  1.10s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 251/1349 [04:50<20:06,  1.10s/it]\u001b[A\n",
      "Iteration:  21%|██        | 278/1349 [05:05<19:43,  1.10s/it]\u001b[A\n",
      "Iteration:  21%|██        | 278/1349 [05:20<19:43,  1.10s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 306/1349 [05:36<19:06,  1.10s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 306/1349 [05:50<19:06,  1.10s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 334/1349 [06:07<18:36,  1.10s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 334/1349 [06:20<18:36,  1.10s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 362/1349 [06:37<18:06,  1.10s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 362/1349 [06:50<18:06,  1.10s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 390/1349 [07:08<17:37,  1.10s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 390/1349 [07:20<17:37,  1.10s/it]\u001b[A\n",
      "Iteration:  31%|███       | 418/1349 [07:39<17:05,  1.10s/it]\u001b[A\n",
      "Iteration:  31%|███       | 418/1349 [07:50<17:05,  1.10s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 446/1349 [08:10<16:34,  1.10s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 446/1349 [08:30<16:34,  1.10s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 474/1349 [08:41<16:04,  1.10s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 474/1349 [09:00<16:04,  1.10s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 502/1349 [09:12<15:36,  1.11s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 502/1349 [09:30<15:36,  1.11s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 530/1349 [09:43<15:04,  1.10s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 530/1349 [10:00<15:04,  1.10s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 558/1349 [10:14<14:32,  1.10s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 558/1349 [10:30<14:32,  1.10s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 586/1349 [10:45<14:00,  1.10s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 586/1349 [11:00<14:00,  1.10s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 614/1349 [11:15<13:29,  1.10s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 614/1349 [11:30<13:29,  1.10s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 642/1349 [11:46<12:58,  1.10s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 642/1349 [12:00<12:58,  1.10s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 670/1349 [12:17<12:26,  1.10s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 670/1349 [12:30<12:26,  1.10s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 698/1349 [12:48<11:55,  1.10s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 698/1349 [13:00<11:55,  1.10s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 726/1349 [13:18<11:24,  1.10s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 726/1349 [13:30<11:24,  1.10s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 754/1349 [13:49<10:55,  1.10s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 754/1349 [14:00<10:55,  1.10s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 782/1349 [14:20<10:22,  1.10s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 782/1349 [14:40<10:22,  1.10s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 810/1349 [14:50<09:50,  1.10s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 810/1349 [15:10<09:50,  1.10s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 838/1349 [15:21<09:20,  1.10s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 838/1349 [15:40<09:20,  1.10s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 866/1349 [15:52<08:50,  1.10s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 866/1349 [16:10<08:50,  1.10s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 894/1349 [16:23<08:20,  1.10s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 894/1349 [16:40<08:20,  1.10s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 922/1349 [16:54<07:49,  1.10s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 922/1349 [17:10<07:49,  1.10s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 950/1349 [17:24<07:17,  1.10s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 950/1349 [17:40<07:17,  1.10s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 978/1349 [17:55<06:47,  1.10s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 978/1349 [18:10<06:47,  1.10s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1006/1349 [18:26<06:16,  1.10s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1006/1349 [18:40<06:16,  1.10s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1034/1349 [18:57<05:45,  1.10s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1034/1349 [19:10<05:45,  1.10s/it]\u001b[A\n",
      "Iteration:  79%|███████▊  | 1062/1349 [19:27<05:13,  1.09s/it]\u001b[A\n",
      "Iteration:  79%|███████▊  | 1062/1349 [19:40<05:13,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1090/1349 [19:57<04:42,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1090/1349 [20:10<04:42,  1.09s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1118/1349 [20:28<04:12,  1.09s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1118/1349 [20:40<04:12,  1.09s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1146/1349 [20:58<03:41,  1.09s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1146/1349 [21:10<03:41,  1.09s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 1174/1349 [21:29<03:10,  1.09s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 1174/1349 [21:40<03:10,  1.09s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 1202/1349 [22:00<02:40,  1.09s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 1202/1349 [22:20<02:40,  1.09s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 1230/1349 [22:30<02:09,  1.09s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 1230/1349 [22:50<02:09,  1.09s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 1258/1349 [23:01<01:39,  1.09s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 1258/1349 [23:20<01:39,  1.09s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 1286/1349 [23:32<01:08,  1.09s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 1286/1349 [23:50<01:08,  1.09s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1314/1349 [24:02<00:38,  1.10s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1314/1349 [24:20<00:38,  1.10s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 1341/1349 [24:32<00:08,  1.10s/it]\u001b[A\n",
      "Epoch:  50%|█████     | 1/2 [24:41<24:41, 1481.69s/it].10s/it]\u001b[A\n",
      "Iteration:   0%|          | 0/1349 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1369325082205109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   2%|▏         | 28/1349 [00:30<23:59,  1.09s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 28/1349 [00:48<23:59,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 56/1349 [01:01<23:31,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 56/1349 [01:18<23:31,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 84/1349 [01:31<23:00,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 84/1349 [01:48<23:00,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 112/1349 [02:02<22:36,  1.10s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 112/1349 [02:18<22:36,  1.10s/it]\u001b[A\n",
      "Iteration:  10%|█         | 140/1349 [02:33<22:05,  1.10s/it]\u001b[A\n",
      "Iteration:  10%|█         | 140/1349 [02:48<22:05,  1.10s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 168/1349 [03:04<21:34,  1.10s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 168/1349 [03:18<21:34,  1.10s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 196/1349 [03:34<21:03,  1.10s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 196/1349 [03:48<21:03,  1.10s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 224/1349 [04:05<20:32,  1.10s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 224/1349 [04:18<20:32,  1.10s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 252/1349 [04:35<19:58,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 252/1349 [04:48<19:58,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██        | 280/1349 [05:06<19:31,  1.10s/it]\u001b[A\n",
      "Iteration:  21%|██        | 280/1349 [05:18<19:31,  1.10s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 308/1349 [05:37<18:59,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 308/1349 [05:48<18:59,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 336/1349 [06:07<18:27,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 336/1349 [06:18<18:27,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 364/1349 [06:38<17:56,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 364/1349 [06:48<17:56,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 392/1349 [07:08<17:24,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 392/1349 [07:28<17:24,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|███       | 420/1349 [07:39<16:54,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|███       | 420/1349 [07:58<16:54,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 448/1349 [08:10<16:23,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 448/1349 [08:28<16:23,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 476/1349 [08:40<15:53,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 476/1349 [08:58<15:53,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 503/1349 [09:10<15:30,  1.10s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 503/1349 [09:28<15:30,  1.10s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 531/1349 [09:41<14:56,  1.10s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 531/1349 [09:58<14:56,  1.10s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 559/1349 [10:12<14:28,  1.10s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 559/1349 [10:28<14:28,  1.10s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 587/1349 [10:42<13:57,  1.10s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 587/1349 [10:58<13:57,  1.10s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 615/1349 [11:13<13:26,  1.10s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 615/1349 [11:28<13:26,  1.10s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 643/1349 [11:44<12:53,  1.10s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 643/1349 [11:58<12:53,  1.10s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 671/1349 [12:14<12:22,  1.09s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 671/1349 [12:28<12:22,  1.09s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 699/1349 [12:45<11:52,  1.10s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 699/1349 [12:58<11:52,  1.10s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 727/1349 [13:16<11:22,  1.10s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 727/1349 [13:28<11:22,  1.10s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 755/1349 [13:47<10:53,  1.10s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 755/1349 [13:58<10:53,  1.10s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 783/1349 [14:17<10:20,  1.10s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 783/1349 [14:28<10:20,  1.10s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 811/1349 [14:48<09:48,  1.09s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 811/1349 [14:58<09:48,  1.09s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 839/1349 [15:19<09:20,  1.10s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 839/1349 [15:38<09:20,  1.10s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 867/1349 [15:49<08:48,  1.10s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 867/1349 [16:08<08:48,  1.10s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 895/1349 [16:20<08:19,  1.10s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 895/1349 [16:38<08:19,  1.10s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 923/1349 [16:51<07:48,  1.10s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 923/1349 [17:08<07:48,  1.10s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 951/1349 [17:22<07:17,  1.10s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 951/1349 [17:38<07:17,  1.10s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 979/1349 [17:53<06:45,  1.10s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 979/1349 [18:08<06:45,  1.10s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1007/1349 [18:23<06:15,  1.10s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1007/1349 [18:38<06:15,  1.10s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1035/1349 [18:54<05:45,  1.10s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1035/1349 [19:08<05:45,  1.10s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1063/1349 [19:25<05:14,  1.10s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1063/1349 [19:38<05:14,  1.10s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1091/1349 [19:55<04:42,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1091/1349 [20:08<04:42,  1.09s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1119/1349 [20:26<04:12,  1.10s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1119/1349 [20:38<04:12,  1.10s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1147/1349 [20:57<03:40,  1.09s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1147/1349 [21:08<03:40,  1.09s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 1175/1349 [21:27<03:09,  1.09s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 1175/1349 [21:38<03:09,  1.09s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 1203/1349 [21:58<02:39,  1.09s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 1203/1349 [22:08<02:39,  1.09s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 1231/1349 [22:28<02:09,  1.09s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 1231/1349 [22:48<02:09,  1.09s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 1259/1349 [22:59<01:38,  1.09s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 1259/1349 [23:18<01:38,  1.09s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 1287/1349 [23:29<01:07,  1.09s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 1287/1349 [23:48<01:07,  1.09s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1315/1349 [24:00<00:37,  1.09s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1315/1349 [24:18<00:37,  1.09s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 1343/1349 [24:31<00:06,  1.10s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 2/2 [49:19<00:00, 1480.55s/it].10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07744894394467414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "token_classifier.fit(token_ids=train_token_ids, \n",
    "                     input_mask=train_input_mask, \n",
    "                     labels=train_label_ids,\n",
    "                     use_gpu=True,\n",
    "                     num_epochs=num_train_epochs, \n",
    "                     batch_size=batch_size, \n",
    "                     learning_rate=learning_rate,\n",
    "                     clip_gradient=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 150/150 [00:50<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 0.0871064007282257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_label_ids = token_classifier.predict(token_ids=dev_token_ids, \n",
    "                                          input_mask=dev_input_mask, \n",
    "                                          labels=dev_label_ids, \n",
    "                                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "The `predict` method of the token classifier outputs label ids for all tokens, including the padded tokens. `postprocess_token_labels` is a helper function that removes the predictions on padded tokens. If a `label_map` is provided, it maps the numerical label ids back to original token labels which are usually string type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_no_padding = postprocess_token_labels(pred_label_ids, \n",
    "                                                dev_input_mask, \n",
    "                                                label_map)\n",
    "true_tags_no_padding =  postprocess_token_labels(dev_label_ids, \n",
    "                                                 dev_input_mask, \n",
    "                                                 label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.89765070581303\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score: {}\".format(f1_score(true_tags_no_padding, pred_tags_no_padding)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`postprocess_token_labels` also provides an option to remove the predictions on trailing word pieces, e.g. ##ing, so that the final predicted labels correspond to the original words in the input text. The `trailing_token_mask` is obtained from `tokenizer.preprocess_ner_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_no_padding_no_trailing = postprocess_token_labels(pred_label_ids, \n",
    "                                                            dev_input_mask, \n",
    "                                                            label_map, \n",
    "                                                            remove_trailing_word_pieces=True, \n",
    "                                                            trailing_token_mask=dev_trailing_token_mask)\n",
    "true_tags_no_padding_no_trailing = postprocess_token_labels(dev_label_ids, \n",
    "                                                            dev_input_mask, \n",
    "                                                            label_map, \n",
    "                                                            remove_trailing_word_pieces=True, \n",
    "                                                            trailing_token_mask=dev_trailing_token_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8250952274254986\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score: {}\".format(f1_score(true_tags_no_padding_no_trailing, pred_tags_no_padding_no_trailing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the F1 score is worse after exluding trailing word pieces, because they are easy to predict. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
