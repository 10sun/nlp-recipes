{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Using BERT\n",
    "## Summary\n",
    "This notebook demonstrates how to fine tune [pretrained BERT model](https://github.com/huggingface/pytorch-pretrained-BERT) for token level named entity recognition (NER) task. A few utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, and model evaluation. \n",
    "\n",
    "[BERT (Bidirectional Transformers forLanguage Understanding)](https://arxiv.org/pdf/1810.04805.pdf) is a powerful pre-trained lanaguage model that can be used for multiple NLP tasks, including text classification, question answering, named entity recognition. It's able to achieve state of the art performance with only a few epochs of fine tuning.  \n",
    "The figure below illustrates how BERT can be fine tuned for NER tasks. The input data is a list of tokens representing a sentence. In the training data, each token has an entity label. After fine tuning, the model predicts an entity label for each token of a given sentence in the testing data. \n",
    "\n",
    "![](bert_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required packages\n",
    "* pytorch\n",
    "* pytorch-pretrained-bert\n",
    "* pandas\n",
    "* seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pprint\n",
    "import random\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "bert_utils_path = os.path.abspath('../../utils_nlp/bert')\n",
    "if bert_utils_path not in sys.path:\n",
    "    sys.path.insert(0, bert_utils_path)\n",
    "\n",
    "from configs import BERTFineTuneConfig\n",
    "from bert_data_utils import KaggleNERProcessor\n",
    "from bert_utils import (BertTokenClassifier, \n",
    "                        create_token_feature_dataset, \n",
    "                        get_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbc340879f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = \"config.yaml\"\n",
    "ner_data_dir = \"./data/NER/ner_dataset.csv\"\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelConfig': {'bert_model': 'bert-base-uncased',\n",
      "                 'do_lower_case': True,\n",
      "                 'max_seq_length': 75},\n",
      " 'OptimizerConfig': {'clip_gradient': True,\n",
      "                     'learning_rate': 3e-05,\n",
      "                     'max_gradient_norm': 1.0,\n",
      "                     'no_decay_params': ['bias', 'gamma', 'beta'],\n",
      "                     'optimizer_name': 'Adam',\n",
      "                     'params_weight_decay': 0.01},\n",
      " 'TrainConfig': {'batch_size': 32, 'num_train_epochs': 2}}\n"
     ]
    }
   ],
   "source": [
    "with open(config_file, 'r') as ymlfile:\n",
    "    config_dict = yaml.safe_load(ymlfile)\n",
    "\n",
    "pprint.pprint(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BERTFineTuneConfig(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and validation examples\n",
    "`KaggleNERProcessor` is a dataset specific class that splits the whole dataset into training and validation datasets according to `dev_percentage`. The `get_train_examples` and `get_dev_examples` return the training and validation datasets respectively. The `get_labels` method returns a list of all unique labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kaggle_ner_processor = KaggleNERProcessor(data_dir=ner_data_dir, dev_percentage = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-gpe', 'B-art', 'B-eve', 'I-geo', 'B-tim', 'I-art', 'I-eve', 'B-nat', 'B-org', 'I-org', 'O', 'B-geo', 'B-gpe', 'I-tim', 'I-per', 'I-nat', 'B-per', 'X']\n"
     ]
    }
   ],
   "source": [
    "train_examples = kaggle_ner_processor.get_train_examples()\n",
    "dev_examples = kaggle_ner_processor.get_dev_examples()\n",
    "label_list = kaggle_ner_processor.get_labels()\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KaggleNERProcessor` generates training and evaluation examples in `BertInputData` type. `BertInputData` is a `namedtuple` with the following three fields:\n",
    "* text_a: text string of the first sentence.\n",
    "* text_b: text string of the second setence. This is only required for two-sentence tasks.\n",
    "* label: required for training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence: \n",
      "Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "Sample sentence labels: \n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Sample sentence: \\n{}\\n'.format(train_examples[0].text_a))\n",
    "print('Sample sentence labels: \\n{}\\n'.format(train_examples[0].label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw input to feature dataset.\n",
    "The function `create_token_feature_dataset` converts raw string data to PyTorch `TensorDataset` containing numerical features, involving the following steps:\n",
    "1. Tokenization.\n",
    "2. Convert tokens and labels to numerical values, i.e. token ids and label ids.\n",
    "3. Sequence padding or truncation according to the `max_seq_length` configuration.\n",
    "4. Convert numpy arrays to Pytorch `TensorDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dictionary that maps labels to numerical values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(config.bert_model,\n",
    "                                          do_lower_case=config.do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create feature TensorDataset**  \n",
    "Note there is an argument called `trailing_piece_tag`. BERT uses a WordPiece tokenizer which breaks down some words into multiple tokens, e.g. \"playing\" is tokenized into \"play\" and \"##ing\". Since the input data only come with one token label for \"playing\", within `create_token_feature_dataset`, the original token label is assigned to the first token \"play\" and the second token \"##ing\" is labeled as \"X\". By default, `trailing_piece_tag` is set to \"X\". If your \"X\" already exists in your data, you can set `trailing_piece_tag` to another value that doesn't exist in your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_token_feature_dataset(data=train_examples,\n",
    "                                             tokenizer=tokenizer,\n",
    "                                             label_map=label_map,\n",
    "                                             true_label_available=True,\n",
    "                                             max_seq_length=config.max_seq_length, \n",
    "                                             trailing_piece_tag=\"X\")\n",
    "dev_dataset = create_token_feature_dataset(data=dev_examples,\n",
    "                                           tokenizer=tokenizer,\n",
    "                                           label_map=label_map, \n",
    "                                           true_label_available=True,\n",
    "                                           max_seq_length=config.max_seq_length, \n",
    "                                           trailing_piece_tag=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_token_feature_dataset` outputs a `TensorDataset` with four tensors:  \n",
    "1. token ids: numerical values each corresponds to a token.\n",
    "2. attention mask: 1 for input tokens and 0 for padded tokens, so that padded tokens are not attended to. \n",
    "3. segment ids: 0 for the first sentence and 1 for the second sentence, only used in two sentence tasks, not used in NER.\n",
    "4. label ids: numerical values each corresponds to an entity label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample token id:\n",
      "tensor([ 5190,  1997, 28337,  2031,  9847,  2083,  2414,  2000,  6186,  1996,\n",
      "         2162,  1999,  5712,  1998,  5157,  1996, 10534,  1997,  2329,  3629,\n",
      "         2013,  2008,  2406,  1012,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0])\n",
      "\n",
      "Sample attention mask:\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "\n",
      "Sample label ids:\n",
      "tensor([10, 10, 10, 10, 10, 10, 11, 10, 10, 10, 10, 10, 11, 10, 10, 10, 10, 10,\n",
      "        12, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample token id:\\n{}\\n\".format(train_dataset[0][0]))\n",
    "print(\"Sample attention mask:\\n{}\\n\".format(train_dataset[0][1]))\n",
    "print(\"Sample label ids:\\n{}\\n\".format(train_dataset[0][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Token Classifier\n",
    "The `get_device` is helper function which detects if GPU is avalaible and the number of GPUs available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT fine tune configurations:\n",
      "batch_size=32\n",
      "num_train_epochs=2\n",
      "bert_model=bert-base-uncased\n",
      "max_seq_length=75\n",
      "do_lower_case=True\n",
      "optimizer_name=Adam\n",
      "learning_rate=3e-05\n",
      "no_decay_params=['bias', 'gamma', 'beta']\n",
      "params_weight_decay=0.01\n",
      "clip_gradient=True\n",
      "max_gradient_norm=1.0\n"
     ]
    }
   ],
   "source": [
    "device, n_gpu = get_device()\n",
    "token_classifier = BertTokenClassifier(config=config, \n",
    "                                       label_map=label_map, \n",
    "                                       device=device, \n",
    "                                       n_gpu=n_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/1349 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 33/1349 [00:30<20:09,  1.09it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 33/1349 [00:49<20:09,  1.09it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 66/1349 [01:00<19:40,  1.09it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 66/1349 [01:20<19:40,  1.09it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 99/1349 [01:31<19:11,  1.09it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 99/1349 [01:50<19:11,  1.09it/s]\u001b[A\n",
      "Iteration:  10%|▉         | 132/1349 [02:01<18:43,  1.08it/s]\u001b[A\n",
      "Iteration:  10%|▉         | 132/1349 [02:20<18:43,  1.08it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 165/1349 [02:32<18:13,  1.08it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 165/1349 [02:50<18:13,  1.08it/s]\u001b[A\n",
      "Iteration:  15%|█▍        | 198/1349 [03:03<17:44,  1.08it/s]\u001b[A\n",
      "Iteration:  15%|█▍        | 198/1349 [03:20<17:44,  1.08it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 231/1349 [03:33<17:15,  1.08it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 231/1349 [03:50<17:15,  1.08it/s]\u001b[A\n",
      "Iteration:  20%|█▉        | 264/1349 [04:04<16:45,  1.08it/s]\u001b[A\n",
      "Iteration:  20%|█▉        | 264/1349 [04:20<16:45,  1.08it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 297/1349 [04:35<16:16,  1.08it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 297/1349 [04:50<16:16,  1.08it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 330/1349 [05:05<15:45,  1.08it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 330/1349 [05:20<15:45,  1.08it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 363/1349 [05:36<15:15,  1.08it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 363/1349 [05:50<15:15,  1.08it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 396/1349 [06:07<14:45,  1.08it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 396/1349 [06:20<14:45,  1.08it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 429/1349 [06:37<14:15,  1.08it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 429/1349 [06:50<14:15,  1.08it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 462/1349 [07:08<13:45,  1.07it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 462/1349 [07:20<13:45,  1.07it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 495/1349 [07:39<13:16,  1.07it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 495/1349 [07:50<13:16,  1.07it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 528/1349 [08:10<12:45,  1.07it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 528/1349 [08:30<12:45,  1.07it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 561/1349 [08:41<12:15,  1.07it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 561/1349 [09:00<12:15,  1.07it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 594/1349 [09:11<11:44,  1.07it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 594/1349 [09:30<11:44,  1.07it/s]\u001b[A\n",
      "Iteration:  46%|████▋     | 627/1349 [09:42<11:13,  1.07it/s]\u001b[A\n",
      "Iteration:  46%|████▋     | 627/1349 [10:00<11:13,  1.07it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 660/1349 [10:13<10:42,  1.07it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 660/1349 [10:30<10:42,  1.07it/s]\u001b[A\n",
      "Iteration:  51%|█████▏    | 693/1349 [10:44<10:12,  1.07it/s]\u001b[A\n",
      "Iteration:  51%|█████▏    | 693/1349 [11:00<10:12,  1.07it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 726/1349 [11:15<09:42,  1.07it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 726/1349 [11:30<09:42,  1.07it/s]\u001b[A\n",
      "Iteration:  56%|█████▋    | 759/1349 [11:46<09:11,  1.07it/s]\u001b[A\n",
      "Iteration:  56%|█████▋    | 759/1349 [12:00<09:11,  1.07it/s]\u001b[A\n",
      "Iteration:  59%|█████▊    | 792/1349 [12:17<08:40,  1.07it/s]\u001b[A\n",
      "Iteration:  59%|█████▊    | 792/1349 [12:30<08:40,  1.07it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 825/1349 [12:47<08:10,  1.07it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 825/1349 [13:00<08:10,  1.07it/s]\u001b[A\n",
      "Iteration:  64%|██████▎   | 857/1349 [13:17<07:40,  1.07it/s]\u001b[A\n",
      "Iteration:  64%|██████▎   | 857/1349 [13:30<07:40,  1.07it/s]\u001b[A\n",
      "Iteration:  66%|██████▌   | 889/1349 [13:47<07:10,  1.07it/s]\u001b[A\n",
      "Iteration:  66%|██████▌   | 889/1349 [14:00<07:10,  1.07it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 921/1349 [14:17<06:41,  1.07it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 921/1349 [14:30<06:41,  1.07it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 953/1349 [14:48<06:11,  1.07it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 953/1349 [15:00<06:11,  1.07it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 985/1349 [15:18<05:41,  1.07it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 985/1349 [15:30<05:41,  1.07it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 1017/1349 [15:48<05:11,  1.07it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 1017/1349 [16:00<05:11,  1.07it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1049/1349 [16:18<04:41,  1.07it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1049/1349 [16:30<04:41,  1.07it/s]\u001b[A\n",
      "Iteration:  80%|████████  | 1081/1349 [16:48<04:11,  1.07it/s]\u001b[A\n",
      "Iteration:  80%|████████  | 1081/1349 [17:00<04:11,  1.07it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1113/1349 [17:18<03:41,  1.07it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1113/1349 [17:30<03:41,  1.07it/s]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1145/1349 [17:48<03:11,  1.06it/s]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1145/1349 [18:00<03:11,  1.06it/s]\u001b[A\n",
      "Iteration:  87%|████████▋ | 1177/1349 [18:18<02:41,  1.06it/s]\u001b[A\n",
      "Iteration:  87%|████████▋ | 1177/1349 [18:30<02:41,  1.06it/s]\u001b[A\n",
      "Iteration:  90%|████████▉ | 1209/1349 [18:48<02:11,  1.06it/s]\u001b[A\n",
      "Iteration:  90%|████████▉ | 1209/1349 [19:00<02:11,  1.06it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 1241/1349 [19:18<01:41,  1.06it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 1241/1349 [19:30<01:41,  1.06it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 1273/1349 [19:48<01:11,  1.06it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 1273/1349 [20:00<01:11,  1.06it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1305/1349 [20:18<00:41,  1.07it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1305/1349 [20:30<00:41,  1.07it/s]\u001b[A\n",
      "Iteration:  99%|█████████▉| 1337/1349 [20:48<00:11,  1.06it/s]\u001b[A\n",
      "Epoch:  50%|█████     | 1/2 [20:59<20:59, 1259.91s/it].07it/s]\u001b[A\n",
      "Iteration:   0%|          | 0/1349 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2427288124778255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   2%|▏         | 32/1349 [00:30<20:35,  1.07it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 32/1349 [00:40<20:35,  1.07it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 64/1349 [01:00<20:06,  1.07it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 64/1349 [01:10<20:06,  1.07it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 96/1349 [01:30<19:37,  1.06it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 96/1349 [01:50<19:37,  1.06it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 128/1349 [02:00<19:07,  1.06it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 128/1349 [02:20<19:07,  1.06it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 160/1349 [02:30<18:37,  1.06it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 160/1349 [02:50<18:37,  1.06it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 192/1349 [03:00<18:06,  1.06it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 192/1349 [03:20<18:06,  1.06it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 224/1349 [03:30<17:36,  1.06it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 224/1349 [03:50<17:36,  1.06it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 256/1349 [04:00<17:07,  1.06it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 256/1349 [04:20<17:07,  1.06it/s]\u001b[A\n",
      "Iteration:  21%|██▏       | 288/1349 [04:30<16:37,  1.06it/s]\u001b[A\n",
      "Iteration:  21%|██▏       | 288/1349 [04:50<16:37,  1.06it/s]\u001b[A\n",
      "Iteration:  24%|██▎       | 320/1349 [05:00<16:06,  1.06it/s]\u001b[A\n",
      "Iteration:  24%|██▎       | 320/1349 [05:20<16:06,  1.06it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 352/1349 [05:30<15:37,  1.06it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 352/1349 [05:50<15:37,  1.06it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 384/1349 [06:00<15:07,  1.06it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 384/1349 [06:20<15:07,  1.06it/s]\u001b[A\n",
      "Iteration:  31%|███       | 416/1349 [06:31<14:37,  1.06it/s]\u001b[A\n",
      "Iteration:  31%|███       | 416/1349 [06:50<14:37,  1.06it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 448/1349 [07:01<14:07,  1.06it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 448/1349 [07:20<14:07,  1.06it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 480/1349 [07:31<13:37,  1.06it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 480/1349 [07:50<13:37,  1.06it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 512/1349 [08:01<13:08,  1.06it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 512/1349 [08:20<13:08,  1.06it/s]\u001b[A\n",
      "Iteration:  40%|████      | 544/1349 [08:31<12:38,  1.06it/s]\u001b[A\n",
      "Iteration:  40%|████      | 544/1349 [08:50<12:38,  1.06it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 576/1349 [09:01<12:07,  1.06it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 576/1349 [09:20<12:07,  1.06it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 608/1349 [09:31<11:37,  1.06it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 608/1349 [09:50<11:37,  1.06it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 640/1349 [10:01<11:06,  1.06it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 640/1349 [10:20<11:06,  1.06it/s]\u001b[A\n",
      "Iteration:  50%|████▉     | 672/1349 [10:31<10:36,  1.06it/s]\u001b[A\n",
      "Iteration:  50%|████▉     | 672/1349 [10:50<10:36,  1.06it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 704/1349 [11:01<10:06,  1.06it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 704/1349 [11:20<10:06,  1.06it/s]\u001b[A\n",
      "Iteration:  55%|█████▍    | 736/1349 [11:32<09:35,  1.06it/s]\u001b[A\n",
      "Iteration:  55%|█████▍    | 736/1349 [11:50<09:35,  1.06it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 769/1349 [12:02<09:04,  1.07it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 769/1349 [12:20<09:04,  1.07it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 801/1349 [12:33<08:34,  1.06it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 801/1349 [12:50<08:34,  1.06it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 833/1349 [13:03<08:04,  1.07it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 833/1349 [13:20<08:04,  1.07it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 865/1349 [13:33<07:34,  1.06it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 865/1349 [13:50<07:34,  1.06it/s]\u001b[A\n",
      "Iteration:  66%|██████▋   | 897/1349 [14:03<07:04,  1.06it/s]\u001b[A\n",
      "Iteration:  66%|██████▋   | 897/1349 [14:20<07:04,  1.06it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 929/1349 [14:33<06:34,  1.06it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 929/1349 [14:50<06:34,  1.06it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 961/1349 [15:03<06:04,  1.06it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 961/1349 [15:20<06:04,  1.06it/s]\u001b[A\n",
      "Iteration:  74%|███████▎  | 994/1349 [15:34<05:33,  1.07it/s]\u001b[A\n",
      "Iteration:  74%|███████▎  | 994/1349 [15:50<05:33,  1.07it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1026/1349 [16:04<05:03,  1.07it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1026/1349 [16:20<05:03,  1.07it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1058/1349 [16:34<04:33,  1.06it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1058/1349 [16:50<04:33,  1.06it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 1090/1349 [17:04<04:03,  1.06it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 1090/1349 [17:20<04:03,  1.06it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1122/1349 [17:34<03:33,  1.06it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1122/1349 [17:50<03:33,  1.06it/s]\u001b[A\n",
      "Iteration:  86%|████████▌ | 1154/1349 [18:04<03:03,  1.06it/s]\u001b[A\n",
      "Iteration:  86%|████████▌ | 1154/1349 [18:20<03:03,  1.06it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 1186/1349 [18:34<02:33,  1.06it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 1186/1349 [18:50<02:33,  1.06it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 1218/1349 [19:05<02:03,  1.06it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 1218/1349 [19:20<02:03,  1.06it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 1250/1349 [19:35<01:33,  1.06it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 1250/1349 [19:50<01:33,  1.06it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 1282/1349 [20:05<01:03,  1.06it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 1282/1349 [20:20<01:03,  1.06it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1314/1349 [20:35<00:32,  1.06it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 1314/1349 [20:50<00:32,  1.06it/s]\u001b[A\n",
      "Iteration: 100%|█████████▉| 1346/1349 [21:05<00:02,  1.06it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 2/2 [42:08<00:00, 1262.37s/it].06it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.23164681046427932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "token_classifier.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 150/150 [01:31<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.21716053078571956\n",
      "F1 Score: 0.794088923156001\n"
     ]
    }
   ],
   "source": [
    "pred_tags, true_tags = token_classifier.predict(dev_dataset)\n",
    "print(\"F1 Score: {}\".format(f1_score(pred_tags, true_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model performance on the first token of each word by exluding \"X\" labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score without 'X' label: 0.8329393223010244\n"
     ]
    }
   ],
   "source": [
    "pred_tags_no_X = []\n",
    "true_tags_no_X = []\n",
    "for p, t in zip(pred_tags, true_tags):\n",
    "    p = p[t != \"X\"]\n",
    "    t = t[t != \"X\"]\n",
    "    pred_tags_no_X.append(p)\n",
    "    true_tags_no_X.append(t)\n",
    "print(\"F1 Score without 'X' label: {}\".format(f1_score(pred_tags_no_X, true_tags_no_X)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
