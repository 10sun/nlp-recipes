{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Text Summerization on CNN/DM Dataset using BertSum\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook demonstrates how to fine tune BERT for extractive text summerization. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, result postprocessing, and model evaluation.\n",
    "\n",
    "BertSum refers to  [Fine-tune BERT for Extractive Summarization (https://arxiv.org/pdf/1903.10318.pdf) with [published example](https://github.com/nlpyang/BertSum/). Extractive summarization are usually used in document summarization where each input document consists of mutiple sentences. The preprocessing of the input training data involves assigning label 0 or 1 to the document sentences based on the give summary. The summarization problem is also simplfied to classifying whether each document sentence should be included in the summary. \n",
    "\n",
    "The figure below illustrates how BERTSum can be fine tuned for extractive summarization task. Each sentence is inserted with [CLS] token at the beginning and  [SEP] at the end. Interval segment embedding and positional embedding are added upon the token embedding before input the BERT model. The [CLS] token representation is used as sentence embedding and only the [CLS] tokens are used as input for the summarization model. The summarization layer predicts whether the probability of each each sentence token should be included in the summary or not. Techniques like trigram blocking can be used to improve model accuarcy.   \n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/BertSum.PNG\">\n",
    "\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "The running time shown in this notebook is on a Standard_NC24s_v3 Azure Deep Learning Virtual Machine with 4 NVIDIA Tesla V100 GPUs. \n",
    "> **Tip**: If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`** to run the notebook on a small subset of the data and a smaller number of epochs. \n",
    "\n",
    "The table below provides some reference running time on different machine configurations.  \n",
    "\n",
    "|QUICK_RUN|USE_PREPROCESSED_DATA|encoder|Machine Configurations|Running time|\n",
    "|:---------|:---------|:---------|:----------------------|:------------|\n",
    "|True|True|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 20 minutes |\n",
    "|False|True|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 60 minutes |\n",
    "|True|False|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 20 minutes |\n",
    "|True|True|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 80 minutes |\n",
    "|False|True|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 6.5hours |\n",
    "|True|False|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 80 minutes |\n",
    "|False|False|any| 1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| > 24 hours|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = True\n",
    "USE_PREPROCESSED_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Before we start the notebook, we should set the environment variable to make sure you can access the GPUs on your machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to clone a modified version of BertSum so that it works for prediction cases and can run on any GPU device ID on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-15 17:28:02--  https://raw.githubusercontent.com/nlpyang/BertSum/master/bert_config_uncased_base.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.124.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.124.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 313 [text/plain]\n",
      "Saving to: ‘bert_config_uncased_base.json’\n",
      "\n",
      "bert_config_uncased 100%[===================>]     313  --.-KB/s    in 0s      \n",
      "\n",
      "2019-10-15 17:28:02 (59.7 MB/s) - ‘bert_config_uncased_base.json’ saved [313/313]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/nlpyang/BertSum/master/bert_config_uncased_base.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_CONFIG_PATH=\"./bert_config_uncased_base.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "nlp_path = os.path.abspath('../../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "sys.path.insert(0, \"./\")\n",
    "sys.path.insert(0, \"./BertSum\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to install the dependencies for pyrouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://azure.archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:2 http://azure.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Ign:3 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:4 http://azure.archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \n",
      "Hit:6 https://packages.microsoft.com/repos/microsoft-ubuntu-xenial-prod xenial InRelease\n",
      "Hit:7 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Fetched 252 kB in 1s (350 kB/s)                    \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "expat is already the newest version (2.2.5-3ubuntu0.2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  linux-azure-cloud-tools-5.0.0-1018 linux-azure-headers-5.0.0-1018\n",
      "  linux-azure-tools-5.0.0-1018\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Note, selecting 'libexpat1-dev' instead of 'libexpat-dev'\n",
      "libexpat1-dev is already the newest version (2.2.5-3ubuntu0.2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  linux-azure-cloud-tools-5.0.0-1018 linux-azure-headers-5.0.0-1018\n",
      "  linux-azure-tools-5.0.0-1018\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# dependencies for ROUGE-1.5.5.pl\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install expat\n",
    "!sudo apt-get install libexpat-dev -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in your terminal to install pre-requiste for using pyrouge.\n",
    "1. sudo cpan install XML::Parser\n",
    "1. sudo cpan install XML::Parser::PerlSAX\n",
    "1. sudo cpan install XML::DOM\n",
    "\n",
    "Download ROUGE-1.5.5 from https://github.com/andersjo/pyrouge/tree/master/tools/ROUGE-1.5.5.\n",
    "Run the following command in your terminal.\n",
    "* pyrouge_set_rouge_path $ABSOLUTE_DIRECTORY_TO_ROUGE-1.5.5.pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprossing\n",
    "\n",
    "The dataset we used for this notebook is CNN/DM dataset which contains the documents and accompanying questions from the news articles of CNN and Daily mail. The highlights in each article are used as summary. The dataset consits of ~289K training examples, ~11K valiation and ~11K test dataset.  You can choose to use the preprocessed version at [BERTSum published example](https://github.com/nlpyang/BertSum/) or use the following section to preprocess the data. Since it takes up to 28 hours to preprocess the training data  to run on 10  Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz, we suggest you continue with set as True first and experiment with data preprocessing  with QUICKRUN set as True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Preprocessed Data\n",
    "Please go to  [BERTSum published example](https://github.com/nlpyang/BertSum/) to download preprocessed data and unzip it to the folder \"./bert_data\" at the current path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PREPROCESSED_DATA =  True\n",
    "if USE_PREPROCESSED_DATA:\n",
    "    BERT_DATA_PATH=\"./bert_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you choose to use preprocessed data, continue to section Model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Data Preprocessing\n",
    "To continue with the data preprocessing, run the following command to download from https://github.com/harvardnlp/sent-summary and unzip the data to folder ./harvardnlp_cnndm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-15 17:28:19--  https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.238.221\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.238.221|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 500375629 (477M) [application/x-gzip]\n",
      "Saving to: ‘cnndm.tar.gz’\n",
      "\n",
      "cnndm.tar.gz        100%[===================>] 477.20M  91.7MB/s    in 5.2s    \n",
      "\n",
      "2019-10-15 17:28:24 (91.4 MB/s) - ‘cnndm.tar.gz’ saved [500375629/500375629]\n",
      "\n",
      "test.txt.src\n",
      "test.txt.tgt.tagged\n",
      "train.txt.src\n",
      "train.txt.tgt.tagged\n",
      "val.txt.src\n",
      "val.txt.tgt.tagged\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz &&\\\n",
    "    mkdir -p harvardnlp_cnndm &&\\\n",
    "    mv cnndm.tar.gz ./harvardnlp_cnndm && cd ./harvardnlp_cnndm &&\\\n",
    "    tar -xvf cnndm.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Details of Data Preprocessing\n",
    "\n",
    "The purpose of preprocessing is to process the input articles to the format that BertSum takes.  Functions defined specific in harvardnlp_cnndm_preprocess function are unique to CNN/DM dataset that's processed by harvardnlp. However, it provides a skeleton of how to preprocessing data into the format that BertSum takes. Assuming you have all articles and target summery each in a file, line-breaker seperated, the steps to preprocess the data are:\n",
    "1. sentence tokenization\n",
    "2. word tokenization\n",
    "3. label the sentences in the article with 1 meaning the sentence is selected and 0 meaning the sentence is not selected. The options for the selection algorithms are \"greedy\" and \"combination\"\n",
    "3. convert each example to  BertSum format\n",
    "    - filter the sentences in the example based on the min_src_ntokens argument. If the lefted total sentence number is less than min_nsents, the example is discarded.\n",
    "    - truncate the sentences in the example if the length is greater than max_src_ntokens\n",
    "    - truncate the sentences in the example and the labels if the totle number of sentences is greater than max_nsents\n",
    "    - [CLS] and [SEP] are inserted before and after each sentence\n",
    "    - wordPiece tokenization\n",
    "    - truncate the example to 512 tokens\n",
    "    - convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary.\n",
    "    - segment ids are generated\n",
    "    - [CLS] token positions are logged\n",
    "    - [CLS] token labels are truncated if it's greater than 512, which is the maximum input length that can be taken by the BERT model.\n",
    "    \n",
    "    \n",
    "Note that the original BERTSum paper use Stanford CoreNLP for data proprocessing, here we'll first how to use NLTK version, and then we also provide instruction of how to set up Stanford NLP and code examples of how to use Standford CoreNLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "I1021 15:21:13.066047 139626020988736 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1021 15:21:13.100040 139626020988736 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.dataset.harvardnlp_cnndm import harvardnlp_cnndm_preprocess\n",
    "from utils_nlp.models.bert.extractive_text_summarization import bertsum_formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 3 µs, total: 15 µs\n",
      "Wall time: 32.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_train_job_number = -1\n",
    "max_test_job_number = -1\n",
    "if QUICK_RUN:\n",
    "    max_train_job_number = 100\n",
    "    max_test_job_number = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of training data: 100\n",
      "CPU times: user 2.53 s, sys: 2.37 s, total: 4.91 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TRAIN_SRC_FILE = \"./harvardnlp_cnndm/train.txt.src\"\n",
    "TRAIN_TGT_FILE = \"./harvardnlp_cnndm/train.txt.tgt.tagged\"\n",
    "PROCESSED_TRAIN_FILE = f\"./harvardnlp_cnndm/train.bertdata_{QUICK_RUN}\" \n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "n_cpus = multiprocessing.cpu_count() - 1\n",
    "jobs = harvardnlp_cnndm_preprocess(n_cpus, TRAIN_SRC_FILE, TRAIN_TGT_FILE, max_train_job_number)\n",
    "print(\"total length of training data:\", len(jobs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1018 18:33:13.232276 140259011639104 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp559sv0bl\n",
      "100%|██████████| 231508/231508 [00:00<00:00, 3077653.07B/s]\n",
      "I1018 18:33:13.472229 140259011639104 file_utils.py:309] copying /tmp/tmp559sv0bl to cache at /home/daden/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1018 18:33:13.473265 140259011639104 file_utils.py:313] creating metadata file for /home/daden/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1018 18:33:13.474657 140259011639104 file_utils.py:322] removing temp file /tmp/tmp559sv0bl\n",
      "I1018 18:33:13.475498 140259011639104 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/daden/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_distilbert import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertsum.prepro.data_builder import TransformerData\n",
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "default_preprocessing_parameters =  {\"max_nsents\": 200, \"max_src_ntokens\": 2000, \"min_nsents\": 3, \"min_src_ntokens\": 5, \"use_interval\": True}\n",
    "args=Bunch(default_preprocessing_parameters)\n",
    "transformerdata = TransformerData(args, tokenizer)\n",
    "bertsum_formatting(n_cpus, transformerdata,\"combination\", jobs[0:max_train_job_number], PROCESSED_TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of training data: 100\n",
      "CPU times: user 686 ms, sys: 1.2 s, total: 1.88 s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEST_SRC_FILE = \"./harvardnlp_cnndm/test.txt.src\"\n",
    "TEST_TGT_FILE = \"./harvardnlp_cnndm/test.txt.tgt.tagged\"\n",
    "PROCESSED_TEST_FILE = f\"./harvardnlp_cnndm/test.bertdata_{QUICK_RUN}\" \n",
    "\n",
    "import multiprocessing\n",
    "n_cpus = multiprocessing.cpu_count() - 1\n",
    "jobs = harvardnlp_cnndm_preprocess(n_cpus, TEST_SRC_FILE, TEST_TGT_FILE, max_test_job_number)\n",
    "print(\"total length of training data:\", len(jobs))\n",
    "from bertsum.prepro.data_builder import TransformerData\n",
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "default_preprocessing_parameters =  {\"max_nsents\": 200, \"max_src_ntokens\": 2000, \"min_nsents\": 3, \"min_src_ntokens\": 5, \"use_interval\": True}\n",
    "args=Bunch(default_preprocessing_parameters)\n",
    "transformerdata = TransformerData(args, tokenizer)\n",
    "bertsum_formatting(n_cpus, transformerdata,\"combination\", jobs[0:max_test_job_number], PROCESSED_TEST_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': [['marseille',\n",
       "   ',',\n",
       "   'france',\n",
       "   '(',\n",
       "   'cnn',\n",
       "   ')',\n",
       "   'the',\n",
       "   'french',\n",
       "   'prosecutor',\n",
       "   'leading',\n",
       "   'an',\n",
       "   'investigation',\n",
       "   'into',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'of',\n",
       "   'germanwings',\n",
       "   'flight',\n",
       "   '9525',\n",
       "   'insisted',\n",
       "   'wednesday',\n",
       "   'that',\n",
       "   'he',\n",
       "   'was',\n",
       "   'not',\n",
       "   'aware',\n",
       "   'of',\n",
       "   'any',\n",
       "   'video',\n",
       "   'footage',\n",
       "   'from',\n",
       "   'on',\n",
       "   'board',\n",
       "   'the',\n",
       "   'plane',\n",
       "   '.'],\n",
       "  ['marseille',\n",
       "   'prosecutor',\n",
       "   'brice',\n",
       "   'robin',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'that',\n",
       "   '``',\n",
       "   'so',\n",
       "   'far',\n",
       "   'no',\n",
       "   'videos',\n",
       "   'were',\n",
       "   'used',\n",
       "   'in',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'investigation',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['he',\n",
       "   'added',\n",
       "   ',',\n",
       "   '``',\n",
       "   'a',\n",
       "   'person',\n",
       "   'who',\n",
       "   'has',\n",
       "   'such',\n",
       "   'a',\n",
       "   'video',\n",
       "   'needs',\n",
       "   'to',\n",
       "   'immediately',\n",
       "   'give',\n",
       "   'it',\n",
       "   'to',\n",
       "   'the',\n",
       "   'investigators',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['robin',\n",
       "   \"'s\",\n",
       "   'comments',\n",
       "   'follow',\n",
       "   'claims',\n",
       "   'by',\n",
       "   'two',\n",
       "   'magazines',\n",
       "   ',',\n",
       "   'german',\n",
       "   'daily',\n",
       "   'bild',\n",
       "   'and',\n",
       "   'french',\n",
       "   'paris',\n",
       "   'match',\n",
       "   ',',\n",
       "   'of',\n",
       "   'a',\n",
       "   'cell',\n",
       "   'phone',\n",
       "   'video',\n",
       "   'showing',\n",
       "   'the',\n",
       "   'harrowing',\n",
       "   'final',\n",
       "   'seconds',\n",
       "   'from',\n",
       "   'on',\n",
       "   'board',\n",
       "   'germanwings',\n",
       "   'flight',\n",
       "   '9525',\n",
       "   'as',\n",
       "   'it',\n",
       "   'crashed',\n",
       "   'into',\n",
       "   'the',\n",
       "   'french',\n",
       "   'alps',\n",
       "   '.'],\n",
       "  ['all', '150', 'on', 'board', 'were', 'killed', '.'],\n",
       "  ['paris',\n",
       "   'match',\n",
       "   'and',\n",
       "   'bild',\n",
       "   'reported',\n",
       "   'that',\n",
       "   'the',\n",
       "   'video',\n",
       "   'was',\n",
       "   'recovered',\n",
       "   'from',\n",
       "   'a',\n",
       "   'phone',\n",
       "   'at',\n",
       "   'the',\n",
       "   'wreckage',\n",
       "   'site',\n",
       "   '.'],\n",
       "  ['the',\n",
       "   'two',\n",
       "   'publications',\n",
       "   'described',\n",
       "   'the',\n",
       "   'supposed',\n",
       "   'video',\n",
       "   ',',\n",
       "   'but',\n",
       "   'did',\n",
       "   'not',\n",
       "   'post',\n",
       "   'it',\n",
       "   'on',\n",
       "   'their',\n",
       "   'websites',\n",
       "   '.'],\n",
       "  ['the',\n",
       "   'publications',\n",
       "   'said',\n",
       "   'that',\n",
       "   'they',\n",
       "   'watched',\n",
       "   'the',\n",
       "   'video',\n",
       "   ',',\n",
       "   'which',\n",
       "   'was',\n",
       "   'found',\n",
       "   'by',\n",
       "   'a',\n",
       "   'source',\n",
       "   'close',\n",
       "   'to',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['one',\n",
       "   'can',\n",
       "   'hear',\n",
       "   'cries',\n",
       "   'of',\n",
       "   '`',\n",
       "   'my',\n",
       "   'god',\n",
       "   \"'\",\n",
       "   'in',\n",
       "   'several',\n",
       "   'languages',\n",
       "   ',',\n",
       "   '``',\n",
       "   'paris',\n",
       "   'match',\n",
       "   'reported',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['metallic',\n",
       "   'banging',\n",
       "   'can',\n",
       "   'also',\n",
       "   'be',\n",
       "   'heard',\n",
       "   'more',\n",
       "   'than',\n",
       "   'three',\n",
       "   'times',\n",
       "   ',',\n",
       "   'perhaps',\n",
       "   'of',\n",
       "   'the',\n",
       "   'pilot',\n",
       "   'trying',\n",
       "   'to',\n",
       "   'open',\n",
       "   'the',\n",
       "   'cockpit',\n",
       "   'door',\n",
       "   'with',\n",
       "   'a',\n",
       "   'heavy',\n",
       "   'object',\n",
       "   '.'],\n",
       "  ['towards',\n",
       "   'the',\n",
       "   'end',\n",
       "   ',',\n",
       "   'after',\n",
       "   'a',\n",
       "   'heavy',\n",
       "   'shake',\n",
       "   ',',\n",
       "   'stronger',\n",
       "   'than',\n",
       "   'the',\n",
       "   'others',\n",
       "   ',',\n",
       "   'the',\n",
       "   'screaming',\n",
       "   'intensifies',\n",
       "   '.'],\n",
       "  ['then', 'nothing', '.', '``'],\n",
       "  ['``',\n",
       "   'it',\n",
       "   'is',\n",
       "   'a',\n",
       "   'very',\n",
       "   'disturbing',\n",
       "   'scene',\n",
       "   ',',\n",
       "   '``',\n",
       "   'said',\n",
       "   'julian',\n",
       "   'reichelt',\n",
       "   ',',\n",
       "   'editor-in-chief',\n",
       "   'of',\n",
       "   'bild',\n",
       "   'online',\n",
       "   '.'],\n",
       "  ['an',\n",
       "   'official',\n",
       "   'with',\n",
       "   'france',\n",
       "   \"'s\",\n",
       "   'accident',\n",
       "   'investigation',\n",
       "   'agency',\n",
       "   ',',\n",
       "   'the',\n",
       "   'bea',\n",
       "   ',',\n",
       "   'said',\n",
       "   'the',\n",
       "   'agency',\n",
       "   'is',\n",
       "   'not',\n",
       "   'aware',\n",
       "   'of',\n",
       "   'any',\n",
       "   'such',\n",
       "   'video',\n",
       "   '.'],\n",
       "  ['lt.',\n",
       "   'col.',\n",
       "   'jean-marc',\n",
       "   'menichini',\n",
       "   ',',\n",
       "   'a',\n",
       "   'french',\n",
       "   'gendarmerie',\n",
       "   'spokesman',\n",
       "   'in',\n",
       "   'charge',\n",
       "   'of',\n",
       "   'communications',\n",
       "   'on',\n",
       "   'rescue',\n",
       "   'efforts',\n",
       "   'around',\n",
       "   'the',\n",
       "   'germanwings',\n",
       "   'crash',\n",
       "   'site',\n",
       "   ',',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'that',\n",
       "   'the',\n",
       "   'reports',\n",
       "   'were',\n",
       "   '``',\n",
       "   'completely',\n",
       "   'wrong',\n",
       "   '``',\n",
       "   'and',\n",
       "   '``',\n",
       "   'unwarranted',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['cell',\n",
       "   'phones',\n",
       "   'have',\n",
       "   'been',\n",
       "   'collected',\n",
       "   'at',\n",
       "   'the',\n",
       "   'site',\n",
       "   ',',\n",
       "   'he',\n",
       "   'said',\n",
       "   ',',\n",
       "   'but',\n",
       "   'that',\n",
       "   'they',\n",
       "   '``',\n",
       "   'had',\n",
       "   \"n't\",\n",
       "   'been',\n",
       "   'exploited',\n",
       "   'yet',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['menichini',\n",
       "   'said',\n",
       "   'he',\n",
       "   'believed',\n",
       "   'the',\n",
       "   'cell',\n",
       "   'phones',\n",
       "   'would',\n",
       "   'need',\n",
       "   'to',\n",
       "   'be',\n",
       "   'sent',\n",
       "   'to',\n",
       "   'the',\n",
       "   'criminal',\n",
       "   'research',\n",
       "   'institute',\n",
       "   'in',\n",
       "   'rosny',\n",
       "   'sous-bois',\n",
       "   ',',\n",
       "   'near',\n",
       "   'paris',\n",
       "   ',',\n",
       "   'in',\n",
       "   'order',\n",
       "   'to',\n",
       "   'be',\n",
       "   'analyzed',\n",
       "   'by',\n",
       "   'specialized',\n",
       "   'technicians',\n",
       "   'working',\n",
       "   'hand-in-hand',\n",
       "   'with',\n",
       "   'investigators',\n",
       "   '.'],\n",
       "  ['but',\n",
       "   'none',\n",
       "   'of',\n",
       "   'the',\n",
       "   'cell',\n",
       "   'phones',\n",
       "   'found',\n",
       "   'so',\n",
       "   'far',\n",
       "   'have',\n",
       "   'been',\n",
       "   'sent',\n",
       "   'to',\n",
       "   'the',\n",
       "   'institute',\n",
       "   ',',\n",
       "   'menichini',\n",
       "   'said',\n",
       "   '.'],\n",
       "  ['asked',\n",
       "   'whether',\n",
       "   'staff',\n",
       "   'involved',\n",
       "   'in',\n",
       "   'the',\n",
       "   'search',\n",
       "   'could',\n",
       "   'have',\n",
       "   'leaked',\n",
       "   'a',\n",
       "   'memory',\n",
       "   'card',\n",
       "   'to',\n",
       "   'the',\n",
       "   'media',\n",
       "   ',',\n",
       "   'menichini',\n",
       "   'answered',\n",
       "   'with',\n",
       "   'a',\n",
       "   'categorical',\n",
       "   '``',\n",
       "   'no',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['reichelt',\n",
       "   'told',\n",
       "   '``',\n",
       "   'erin',\n",
       "   'burnett',\n",
       "   ':',\n",
       "   'outfront',\n",
       "   '``',\n",
       "   'that',\n",
       "   'he',\n",
       "   'had',\n",
       "   'watched',\n",
       "   'the',\n",
       "   'video',\n",
       "   'and',\n",
       "   'stood',\n",
       "   'by',\n",
       "   'the',\n",
       "   'report',\n",
       "   ',',\n",
       "   'saying',\n",
       "   'bild',\n",
       "   'and',\n",
       "   'paris',\n",
       "   'match',\n",
       "   'are',\n",
       "   '``',\n",
       "   'very',\n",
       "   'confident',\n",
       "   '``',\n",
       "   'that',\n",
       "   'the',\n",
       "   'clip',\n",
       "   'is',\n",
       "   'real',\n",
       "   '.'],\n",
       "  ['he',\n",
       "   'noted',\n",
       "   'that',\n",
       "   'investigators',\n",
       "   'only',\n",
       "   'revealed',\n",
       "   'they',\n",
       "   \"'d\",\n",
       "   'recovered',\n",
       "   'cell',\n",
       "   'phones',\n",
       "   'from',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'site',\n",
       "   'after',\n",
       "   'bild',\n",
       "   'and',\n",
       "   'paris',\n",
       "   'match',\n",
       "   'published',\n",
       "   'their',\n",
       "   'reports',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['that', 'is', 'something', 'we', 'did', 'not', 'know', 'before', '.'],\n",
       "  ['...',\n",
       "   'overall',\n",
       "   'we',\n",
       "   'can',\n",
       "   'say',\n",
       "   'many',\n",
       "   'things',\n",
       "   'of',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   'were',\n",
       "   \"n't\",\n",
       "   'revealed',\n",
       "   'by',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   'at',\n",
       "   'the',\n",
       "   'beginning',\n",
       "   ',',\n",
       "   '``',\n",
       "   'he',\n",
       "   'said',\n",
       "   '.'],\n",
       "  ['what', 'was', 'mental', 'state', 'of', 'germanwings', 'co-pilot', '?'],\n",
       "  ['german',\n",
       "   'airline',\n",
       "   'lufthansa',\n",
       "   'confirmed',\n",
       "   'tuesday',\n",
       "   'that',\n",
       "   'co-pilot',\n",
       "   'andreas',\n",
       "   'lubitz',\n",
       "   'had',\n",
       "   'battled',\n",
       "   'depression',\n",
       "   'years',\n",
       "   'before',\n",
       "   'he',\n",
       "   'took',\n",
       "   'the',\n",
       "   'controls',\n",
       "   'of',\n",
       "   'germanwings',\n",
       "   'flight',\n",
       "   '9525',\n",
       "   ',',\n",
       "   'which',\n",
       "   'he',\n",
       "   \"'s\",\n",
       "   'accused',\n",
       "   'of',\n",
       "   'deliberately',\n",
       "   'crashing',\n",
       "   'last',\n",
       "   'week',\n",
       "   'in',\n",
       "   'the',\n",
       "   'french',\n",
       "   'alps',\n",
       "   '.'],\n",
       "  ['lubitz',\n",
       "   'told',\n",
       "   'his',\n",
       "   'lufthansa',\n",
       "   'flight',\n",
       "   'training',\n",
       "   'school',\n",
       "   'in',\n",
       "   '2009',\n",
       "   'that',\n",
       "   'he',\n",
       "   'had',\n",
       "   'a',\n",
       "   '``',\n",
       "   'previous',\n",
       "   'episode',\n",
       "   'of',\n",
       "   'severe',\n",
       "   'depression',\n",
       "   ',',\n",
       "   '``',\n",
       "   'the',\n",
       "   'airline',\n",
       "   'said',\n",
       "   'tuesday',\n",
       "   '.'],\n",
       "  ['email',\n",
       "   'correspondence',\n",
       "   'between',\n",
       "   'lubitz',\n",
       "   'and',\n",
       "   'the',\n",
       "   'school',\n",
       "   'discovered',\n",
       "   'in',\n",
       "   'an',\n",
       "   'internal',\n",
       "   'investigation',\n",
       "   ',',\n",
       "   'lufthansa',\n",
       "   'said',\n",
       "   ',',\n",
       "   'included',\n",
       "   'medical',\n",
       "   'documents',\n",
       "   'he',\n",
       "   'submitted',\n",
       "   'in',\n",
       "   'connection',\n",
       "   'with',\n",
       "   'resuming',\n",
       "   'his',\n",
       "   'flight',\n",
       "   'training',\n",
       "   '.'],\n",
       "  ['the',\n",
       "   'announcement',\n",
       "   'indicates',\n",
       "   'that',\n",
       "   'lufthansa',\n",
       "   ',',\n",
       "   'the',\n",
       "   'parent',\n",
       "   'company',\n",
       "   'of',\n",
       "   'germanwings',\n",
       "   ',',\n",
       "   'knew',\n",
       "   'of',\n",
       "   'lubitz',\n",
       "   \"'s\",\n",
       "   'battle',\n",
       "   'with',\n",
       "   'depression',\n",
       "   ',',\n",
       "   'allowed',\n",
       "   'him',\n",
       "   'to',\n",
       "   'continue',\n",
       "   'training',\n",
       "   'and',\n",
       "   'ultimately',\n",
       "   'put',\n",
       "   'him',\n",
       "   'in',\n",
       "   'the',\n",
       "   'cockpit',\n",
       "   '.'],\n",
       "  ['lufthansa',\n",
       "   ',',\n",
       "   'whose',\n",
       "   'ceo',\n",
       "   'carsten',\n",
       "   'spohr',\n",
       "   'previously',\n",
       "   'said',\n",
       "   'lubitz',\n",
       "   'was',\n",
       "   '100',\n",
       "   '%',\n",
       "   'fit',\n",
       "   'to',\n",
       "   'fly',\n",
       "   ',',\n",
       "   'described',\n",
       "   'its',\n",
       "   'statement',\n",
       "   'tuesday',\n",
       "   'as',\n",
       "   'a',\n",
       "   '``',\n",
       "   'swift',\n",
       "   'and',\n",
       "   'seamless',\n",
       "   'clarification',\n",
       "   '``',\n",
       "   'and',\n",
       "   'said',\n",
       "   'it',\n",
       "   'was',\n",
       "   'sharing',\n",
       "   'the',\n",
       "   'information',\n",
       "   'and',\n",
       "   'documents',\n",
       "   '--',\n",
       "   'including',\n",
       "   'training',\n",
       "   'and',\n",
       "   'medical',\n",
       "   'records',\n",
       "   '--',\n",
       "   'with',\n",
       "   'public',\n",
       "   'prosecutors',\n",
       "   '.'],\n",
       "  ['spohr',\n",
       "   'traveled',\n",
       "   'to',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'site',\n",
       "   'wednesday',\n",
       "   ',',\n",
       "   'where',\n",
       "   'recovery',\n",
       "   'teams',\n",
       "   'have',\n",
       "   'been',\n",
       "   'working',\n",
       "   'for',\n",
       "   'the',\n",
       "   'past',\n",
       "   'week',\n",
       "   'to',\n",
       "   'recover',\n",
       "   'human',\n",
       "   'remains',\n",
       "   'and',\n",
       "   'plane',\n",
       "   'debris',\n",
       "   'scattered',\n",
       "   'across',\n",
       "   'a',\n",
       "   'steep',\n",
       "   'mountainside',\n",
       "   '.'],\n",
       "  ['he',\n",
       "   'saw',\n",
       "   'the',\n",
       "   'crisis',\n",
       "   'center',\n",
       "   'set',\n",
       "   'up',\n",
       "   'in',\n",
       "   'seyne-les-alpes',\n",
       "   ',',\n",
       "   'laid',\n",
       "   'a',\n",
       "   'wreath',\n",
       "   'in',\n",
       "   'the',\n",
       "   'village',\n",
       "   'of',\n",
       "   'le',\n",
       "   'vernet',\n",
       "   ',',\n",
       "   'closer',\n",
       "   'to',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'site',\n",
       "   ',',\n",
       "   'where',\n",
       "   'grieving',\n",
       "   'families',\n",
       "   'have',\n",
       "   'left',\n",
       "   'flowers',\n",
       "   'at',\n",
       "   'a',\n",
       "   'simple',\n",
       "   'stone',\n",
       "   'memorial',\n",
       "   '.'],\n",
       "  ['menichini',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'late',\n",
       "   'tuesday',\n",
       "   'that',\n",
       "   'no',\n",
       "   'visible',\n",
       "   'human',\n",
       "   'remains',\n",
       "   'were',\n",
       "   'left',\n",
       "   'at',\n",
       "   'the',\n",
       "   'site',\n",
       "   'but',\n",
       "   'recovery',\n",
       "   'teams',\n",
       "   'would',\n",
       "   'keep',\n",
       "   'searching',\n",
       "   '.'],\n",
       "  ['french',\n",
       "   'president',\n",
       "   'francois',\n",
       "   'hollande',\n",
       "   ',',\n",
       "   'speaking',\n",
       "   'tuesday',\n",
       "   ',',\n",
       "   'said',\n",
       "   'that',\n",
       "   'it',\n",
       "   'should',\n",
       "   'be',\n",
       "   'possible',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'all',\n",
       "   'the',\n",
       "   'victims',\n",
       "   'using',\n",
       "   'dna',\n",
       "   'analysis',\n",
       "   'by',\n",
       "   'the',\n",
       "   'end',\n",
       "   'of',\n",
       "   'the',\n",
       "   'week',\n",
       "   ',',\n",
       "   'sooner',\n",
       "   'than',\n",
       "   'authorities',\n",
       "   'had',\n",
       "   'previously',\n",
       "   'suggested',\n",
       "   '.'],\n",
       "  ['in',\n",
       "   'the',\n",
       "   'meantime',\n",
       "   ',',\n",
       "   'the',\n",
       "   'recovery',\n",
       "   'of',\n",
       "   'the',\n",
       "   'victims',\n",
       "   \"'\",\n",
       "   'personal',\n",
       "   'belongings',\n",
       "   'will',\n",
       "   'start',\n",
       "   'wednesday',\n",
       "   ',',\n",
       "   'menichini',\n",
       "   'said',\n",
       "   '.'],\n",
       "  ['among',\n",
       "   'those',\n",
       "   'personal',\n",
       "   'belongings',\n",
       "   'could',\n",
       "   'be',\n",
       "   'more',\n",
       "   'cell',\n",
       "   'phones',\n",
       "   'belonging',\n",
       "   'to',\n",
       "   'the',\n",
       "   '144',\n",
       "   'passengers',\n",
       "   'and',\n",
       "   'six',\n",
       "   'crew',\n",
       "   'on',\n",
       "   'board',\n",
       "   '.'],\n",
       "  ['check', 'out', 'the', 'latest', 'from', 'our', 'correspondents', '.'],\n",
       "  ['the',\n",
       "   'details',\n",
       "   'about',\n",
       "   'lubitz',\n",
       "   \"'s\",\n",
       "   'correspondence',\n",
       "   'with',\n",
       "   'the',\n",
       "   'flight',\n",
       "   'school',\n",
       "   'during',\n",
       "   'his',\n",
       "   'training',\n",
       "   'were',\n",
       "   'among',\n",
       "   'several',\n",
       "   'developments',\n",
       "   'as',\n",
       "   'investigators',\n",
       "   'continued',\n",
       "   'to',\n",
       "   'delve',\n",
       "   'into',\n",
       "   'what',\n",
       "   'caused',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'and',\n",
       "   'lubitz',\n",
       "   \"'s\",\n",
       "   'possible',\n",
       "   'motive',\n",
       "   'for',\n",
       "   'downing',\n",
       "   'the',\n",
       "   'jet',\n",
       "   '.'],\n",
       "  ['a',\n",
       "   'lufthansa',\n",
       "   'spokesperson',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'on',\n",
       "   'tuesday',\n",
       "   'that',\n",
       "   'lubitz',\n",
       "   'had',\n",
       "   'a',\n",
       "   'valid',\n",
       "   'medical',\n",
       "   'certificate',\n",
       "   ',',\n",
       "   'had',\n",
       "   'passed',\n",
       "   'all',\n",
       "   'his',\n",
       "   'examinations',\n",
       "   'and',\n",
       "   '``',\n",
       "   'held',\n",
       "   'all',\n",
       "   'the',\n",
       "   'licenses',\n",
       "   'required',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['earlier',\n",
       "   ',',\n",
       "   'a',\n",
       "   'spokesman',\n",
       "   'for',\n",
       "   'the',\n",
       "   'prosecutor',\n",
       "   \"'s\",\n",
       "   'office',\n",
       "   'in',\n",
       "   'dusseldorf',\n",
       "   ',',\n",
       "   'christoph',\n",
       "   'kumpa',\n",
       "   ',',\n",
       "   'said',\n",
       "   'medical',\n",
       "   'records',\n",
       "   'reveal',\n",
       "   'lubitz',\n",
       "   'suffered',\n",
       "   'from',\n",
       "   'suicidal',\n",
       "   'tendencies',\n",
       "   'at',\n",
       "   'some',\n",
       "   'point',\n",
       "   'before',\n",
       "   'his',\n",
       "   'aviation',\n",
       "   'career',\n",
       "   'and',\n",
       "   'underwent',\n",
       "   'psychotherapy',\n",
       "   'before',\n",
       "   'he',\n",
       "   'got',\n",
       "   'his',\n",
       "   'pilot',\n",
       "   \"'s\",\n",
       "   'license',\n",
       "   '.'],\n",
       "  ['kumpa',\n",
       "   'emphasized',\n",
       "   'there',\n",
       "   \"'s\",\n",
       "   'no',\n",
       "   'evidence',\n",
       "   'suggesting',\n",
       "   'lubitz',\n",
       "   'was',\n",
       "   'suicidal',\n",
       "   'or',\n",
       "   'acting',\n",
       "   'aggressively',\n",
       "   'before',\n",
       "   'the',\n",
       "   'crash',\n",
       "   '.'],\n",
       "  ['investigators',\n",
       "   'are',\n",
       "   'looking',\n",
       "   'into',\n",
       "   'whether',\n",
       "   'lubitz',\n",
       "   'feared',\n",
       "   'his',\n",
       "   'medical',\n",
       "   'condition',\n",
       "   'would',\n",
       "   'cause',\n",
       "   'him',\n",
       "   'to',\n",
       "   'lose',\n",
       "   'his',\n",
       "   'pilot',\n",
       "   \"'s\",\n",
       "   'license',\n",
       "   ',',\n",
       "   'a',\n",
       "   'european',\n",
       "   'government',\n",
       "   'official',\n",
       "   'briefed',\n",
       "   'on',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'on',\n",
       "   'tuesday',\n",
       "   '.'],\n",
       "  ['while',\n",
       "   'flying',\n",
       "   'was',\n",
       "   '``',\n",
       "   'a',\n",
       "   'big',\n",
       "   'part',\n",
       "   'of',\n",
       "   'his',\n",
       "   'life',\n",
       "   ',',\n",
       "   '``',\n",
       "   'the',\n",
       "   'source',\n",
       "   'said',\n",
       "   ',',\n",
       "   'it',\n",
       "   \"'s\",\n",
       "   'only',\n",
       "   'one',\n",
       "   'theory',\n",
       "   'being',\n",
       "   'considered',\n",
       "   '.'],\n",
       "  ['another',\n",
       "   'source',\n",
       "   ',',\n",
       "   'a',\n",
       "   'law',\n",
       "   'enforcement',\n",
       "   'official',\n",
       "   'briefed',\n",
       "   'on',\n",
       "   'the',\n",
       "   'investigation',\n",
       "   ',',\n",
       "   'also',\n",
       "   'told',\n",
       "   'cnn',\n",
       "   'that',\n",
       "   'authorities',\n",
       "   'believe',\n",
       "   'the',\n",
       "   'primary',\n",
       "   'motive',\n",
       "   'for',\n",
       "   'lubitz',\n",
       "   'to',\n",
       "   'bring',\n",
       "   'down',\n",
       "   'the',\n",
       "   'plane',\n",
       "   'was',\n",
       "   'that',\n",
       "   'he',\n",
       "   'feared',\n",
       "   'he',\n",
       "   'would',\n",
       "   'not',\n",
       "   'be',\n",
       "   'allowed',\n",
       "   'to',\n",
       "   'fly',\n",
       "   'because',\n",
       "   'of',\n",
       "   'his',\n",
       "   'medical',\n",
       "   'problems',\n",
       "   '.'],\n",
       "  ['lubitz',\n",
       "   \"'s\",\n",
       "   'girlfriend',\n",
       "   'told',\n",
       "   'investigators',\n",
       "   'he',\n",
       "   'had',\n",
       "   'seen',\n",
       "   'an',\n",
       "   'eye',\n",
       "   'doctor',\n",
       "   'and',\n",
       "   'a',\n",
       "   'neuropsychologist',\n",
       "   ',',\n",
       "   'both',\n",
       "   'of',\n",
       "   'whom',\n",
       "   'deemed',\n",
       "   'him',\n",
       "   'unfit',\n",
       "   'to',\n",
       "   'work',\n",
       "   'recently',\n",
       "   'and',\n",
       "   'concluded',\n",
       "   'he',\n",
       "   'had',\n",
       "   'psychological',\n",
       "   'issues',\n",
       "   ',',\n",
       "   'the',\n",
       "   'european',\n",
       "   'government',\n",
       "   'official',\n",
       "   'said',\n",
       "   '.'],\n",
       "  ['but',\n",
       "   'no',\n",
       "   'matter',\n",
       "   'what',\n",
       "   'details',\n",
       "   'emerge',\n",
       "   'about',\n",
       "   'his',\n",
       "   'previous',\n",
       "   'mental',\n",
       "   'health',\n",
       "   'struggles',\n",
       "   ',',\n",
       "   'there',\n",
       "   \"'s\",\n",
       "   'more',\n",
       "   'to',\n",
       "   'the',\n",
       "   'story',\n",
       "   ',',\n",
       "   'said',\n",
       "   'brian',\n",
       "   'russell',\n",
       "   ',',\n",
       "   'a',\n",
       "   'forensic',\n",
       "   'psychologist',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['psychology',\n",
       "   'can',\n",
       "   'explain',\n",
       "   'why',\n",
       "   'somebody',\n",
       "   'would',\n",
       "   'turn',\n",
       "   'rage',\n",
       "   'inward',\n",
       "   'on',\n",
       "   'themselves',\n",
       "   'about',\n",
       "   'the',\n",
       "   'fact',\n",
       "   'that',\n",
       "   'maybe',\n",
       "   'they',\n",
       "   'were',\n",
       "   \"n't\",\n",
       "   'going',\n",
       "   'to',\n",
       "   'keep',\n",
       "   'doing',\n",
       "   'their',\n",
       "   'job',\n",
       "   'and',\n",
       "   'they',\n",
       "   \"'re\",\n",
       "   'upset',\n",
       "   'about',\n",
       "   'that',\n",
       "   'and',\n",
       "   'so',\n",
       "   'they',\n",
       "   \"'re\",\n",
       "   'suicidal',\n",
       "   ',',\n",
       "   '``',\n",
       "   'he',\n",
       "   'said',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['but',\n",
       "   'there',\n",
       "   'is',\n",
       "   'no',\n",
       "   'mental',\n",
       "   'illness',\n",
       "   'that',\n",
       "   'explains',\n",
       "   'why',\n",
       "   'somebody',\n",
       "   'then',\n",
       "   'feels',\n",
       "   'entitled',\n",
       "   'to',\n",
       "   'also',\n",
       "   'take',\n",
       "   'that',\n",
       "   'rage',\n",
       "   'and',\n",
       "   'turn',\n",
       "   'it',\n",
       "   'outward',\n",
       "   'on',\n",
       "   '149',\n",
       "   'other',\n",
       "   'people',\n",
       "   'who',\n",
       "   'had',\n",
       "   'nothing',\n",
       "   'to',\n",
       "   'do',\n",
       "   'with',\n",
       "   'the',\n",
       "   'person',\n",
       "   \"'s\",\n",
       "   'problems',\n",
       "   '.',\n",
       "   '``'],\n",
       "  ['germanwings', 'crash', 'compensation', ':', 'what', 'we', 'know', '.'],\n",
       "  ['who', 'was', 'the', 'captain', 'of', 'germanwings', 'flight', '9525', '?'],\n",
       "  ['cnn',\n",
       "   \"'s\",\n",
       "   'margot',\n",
       "   'haddad',\n",
       "   'reported',\n",
       "   'from',\n",
       "   'marseille',\n",
       "   'and',\n",
       "   'pamela',\n",
       "   'brown',\n",
       "   'from',\n",
       "   'dusseldorf',\n",
       "   ',',\n",
       "   'while',\n",
       "   'laura',\n",
       "   'smith-spark',\n",
       "   'wrote',\n",
       "   'from',\n",
       "   'london',\n",
       "   '.'],\n",
       "  ['cnn',\n",
       "   \"'s\",\n",
       "   'frederik',\n",
       "   'pleitgen',\n",
       "   ',',\n",
       "   'pamela',\n",
       "   'boykoff',\n",
       "   ',',\n",
       "   'antonia',\n",
       "   'mortensen',\n",
       "   ',',\n",
       "   'sandrine',\n",
       "   'amiel',\n",
       "   'and',\n",
       "   'anna-maja',\n",
       "   'rappard',\n",
       "   'contributed',\n",
       "   'to',\n",
       "   'this',\n",
       "   'report',\n",
       "   '.']],\n",
       " 'tgt': [['marseille',\n",
       "   'prosecutor',\n",
       "   'says',\n",
       "   '``',\n",
       "   'so',\n",
       "   'far',\n",
       "   'no',\n",
       "   'videos',\n",
       "   'were',\n",
       "   'used',\n",
       "   'in',\n",
       "   'the',\n",
       "   'crash',\n",
       "   'investigation',\n",
       "   '``',\n",
       "   'despite',\n",
       "   'media',\n",
       "   'reports',\n",
       "   '.'],\n",
       "  ['journalists',\n",
       "   'at',\n",
       "   'bild',\n",
       "   'and',\n",
       "   'paris',\n",
       "   'match',\n",
       "   'are',\n",
       "   '``',\n",
       "   'very',\n",
       "   'confident',\n",
       "   '``',\n",
       "   'the',\n",
       "   'video',\n",
       "   'clip',\n",
       "   'is',\n",
       "   'real',\n",
       "   ',',\n",
       "   'an',\n",
       "   'editor',\n",
       "   'says',\n",
       "   '.'],\n",
       "  ['andreas',\n",
       "   'lubitz',\n",
       "   'had',\n",
       "   'informed',\n",
       "   'his',\n",
       "   'lufthansa',\n",
       "   'training',\n",
       "   'school',\n",
       "   'of',\n",
       "   'an',\n",
       "   'episode',\n",
       "   'of',\n",
       "   'severe',\n",
       "   'depression',\n",
       "   ',',\n",
       "   'airline',\n",
       "   'says',\n",
       "   '.'],\n",
       "  []]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['src', 'labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "bert_format_data = torch.load(PROCESSED_TRAIN_FILE)\n",
    "print(len(bert_format_data))\n",
    "bert_format_data[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1006,\n",
       " 13229,\n",
       " 1007,\n",
       " 1011,\n",
       " 1011,\n",
       " 1996,\n",
       " 2120,\n",
       " 2374,\n",
       " 2223,\n",
       " 2038,\n",
       " 20733,\n",
       " 6731,\n",
       " 5865,\n",
       " 14929,\n",
       " 9074,\n",
       " 2745,\n",
       " 10967,\n",
       " 2243,\n",
       " 2302,\n",
       " 3477,\n",
       " 1010,\n",
       " 4584,\n",
       " 2007,\n",
       " 1996,\n",
       " 2223,\n",
       " 2056,\n",
       " 5958,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 5088,\n",
       " 2732,\n",
       " 2745,\n",
       " 10967,\n",
       " 2243,\n",
       " 2003,\n",
       " 2275,\n",
       " 2000,\n",
       " 3711,\n",
       " 1999,\n",
       " 2457,\n",
       " 6928,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1037,\n",
       " 3648,\n",
       " 2097,\n",
       " 2031,\n",
       " 1996,\n",
       " 2345,\n",
       " 2360,\n",
       " 2006,\n",
       " 1037,\n",
       " 14865,\n",
       " 3066,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 3041,\n",
       " 1010,\n",
       " 10967,\n",
       " 2243,\n",
       " 4914,\n",
       " 2000,\n",
       " 8019,\n",
       " 1999,\n",
       " 1037,\n",
       " 3899,\n",
       " 22158,\n",
       " 3614,\n",
       " 2004,\n",
       " 2112,\n",
       " 1997,\n",
       " 1037,\n",
       " 14865,\n",
       " 3820,\n",
       " 2007,\n",
       " 2976,\n",
       " 19608,\n",
       " 1999,\n",
       " 3448,\n",
       " 1012,\n",
       " 1036,\n",
       " 1036,\n",
       " 102,\n",
       " 101,\n",
       " 2115,\n",
       " 4914,\n",
       " 6204,\n",
       " 2001,\n",
       " 2025,\n",
       " 2069,\n",
       " 6206,\n",
       " 1010,\n",
       " 2021,\n",
       " 2036,\n",
       " 10311,\n",
       " 1998,\n",
       " 16360,\n",
       " 2890,\n",
       " 10222,\n",
       " 19307,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2115,\n",
       " 2136,\n",
       " 1010,\n",
       " 1996,\n",
       " 5088,\n",
       " 1010,\n",
       " 1998,\n",
       " 5088,\n",
       " 4599,\n",
       " 2031,\n",
       " 2035,\n",
       " 2042,\n",
       " 3480,\n",
       " 2011,\n",
       " 2115,\n",
       " 4506,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 5088,\n",
       " 5849,\n",
       " 5074,\n",
       " 2204,\n",
       " 5349,\n",
       " 2056,\n",
       " 1999,\n",
       " 1037,\n",
       " 3661,\n",
       " 2000,\n",
       " 10967,\n",
       " 2243,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2204,\n",
       " 5349,\n",
       " 2056,\n",
       " 2002,\n",
       " 2052,\n",
       " 3319,\n",
       " 1996,\n",
       " 3570,\n",
       " 1997,\n",
       " 1996,\n",
       " 8636,\n",
       " 2044,\n",
       " 1996,\n",
       " 3423,\n",
       " 8931,\n",
       " 2024,\n",
       " 2058,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1999,\n",
       " 4981,\n",
       " 6406,\n",
       " 5958,\n",
       " 2007,\n",
       " 1037,\n",
       " 2976,\n",
       " 2457,\n",
       " 1999,\n",
       " 3448,\n",
       " 1010,\n",
       " 10967,\n",
       " 2243,\n",
       " 2036,\n",
       " 4914,\n",
       " 2008,\n",
       " 2002,\n",
       " 1998,\n",
       " 2048,\n",
       " 2522,\n",
       " 1011,\n",
       " 9530,\n",
       " 13102,\n",
       " 7895,\n",
       " 6591,\n",
       " 2730,\n",
       " 6077,\n",
       " 2008,\n",
       " 2106,\n",
       " 2025,\n",
       " 2954,\n",
       " 2092,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 14929,\n",
       " 3954,\n",
       " 4300,\n",
       " 8744,\n",
       " 2056,\n",
       " 10967,\n",
       " 2243,\n",
       " 1005,\n",
       " 1055,\n",
       " 20247,\n",
       " 6235,\n",
       " 4506,\n",
       " 2008,\n",
       " 2024,\n",
       " 1036,\n",
       " 1036,\n",
       " 4297,\n",
       " 25377,\n",
       " 2890,\n",
       " 10222,\n",
       " 19307,\n",
       " 1998,\n",
       " 21873,\n",
       " 1012,\n",
       " 1036,\n",
       " 1036,\n",
       " 102,\n",
       " 101,\n",
       " 1996,\n",
       " 8636,\n",
       " 3084,\n",
       " 1036,\n",
       " 1036,\n",
       " 1037,\n",
       " 2844,\n",
       " 4861,\n",
       " 2008,\n",
       " 6204,\n",
       " 2029,\n",
       " 16985,\n",
       " 24014,\n",
       " 2229,\n",
       " 1996,\n",
       " 2204,\n",
       " 5891,\n",
       " 1997,\n",
       " 1996,\n",
       " 5088,\n",
       " 2097,\n",
       " 2025,\n",
       " 2022,\n",
       " 25775,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 2002,\n",
       " 2056,\n",
       " 1999,\n",
       " 1037,\n",
       " 4861,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 3422,\n",
       " 2054,\n",
       " 2419,\n",
       " 2000,\n",
       " 10967,\n",
       " 2243,\n",
       " 1005,\n",
       " 1055,\n",
       " 8636,\n",
       " 1036,\n",
       " 1036,\n",
       " 2204,\n",
       " 5349,\n",
       " 2056,\n",
       " 1996,\n",
       " 14929,\n",
       " 2071,\n",
       " 1036,\n",
       " 1036,\n",
       " 20865,\n",
       " 2151,\n",
       " 4447,\n",
       " 2030,\n",
       " 2128,\n",
       " 7583,\n",
       " 3111,\n",
       " 1036,\n",
       " 1036,\n",
       " 2000,\n",
       " 8980,\n",
       " 1002,\n",
       " 2570,\n",
       " 2454,\n",
       " 1997,\n",
       " 10967,\n",
       " 2243,\n",
       " 1005,\n",
       " 1055,\n",
       " 6608,\n",
       " 6781,\n",
       " 2013,\n",
       " 1996,\n",
       " 2184,\n",
       " 1011,\n",
       " 2095,\n",
       " 1010,\n",
       " 1002,\n",
       " 7558,\n",
       " 2454,\n",
       " 3206,\n",
       " 2002,\n",
       " 2772,\n",
       " 1999,\n",
       " 2432,\n",
       " 1010,\n",
       " 2429,\n",
       " 2000,\n",
       " 1996,\n",
       " 3378,\n",
       " 2811,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 10967,\n",
       " 2243,\n",
       " 2056,\n",
       " 2002,\n",
       " 2052,\n",
       " 25803,\n",
       " 5905,\n",
       " 2000,\n",
       " 2028,\n",
       " 4175,\n",
       " 1997,\n",
       " 1036,\n",
       " 1036,\n",
       " 9714,\n",
       " 2000,\n",
       " 3604,\n",
       " 1999,\n",
       " 7553,\n",
       " 6236,\n",
       " 1999,\n",
       " 4681,\n",
       " 1997,\n",
       " 22300,\n",
       " 3450,\n",
       " 1998,\n",
       " 2000,\n",
       " 10460,\n",
       " 1037,\n",
       " 3899,\n",
       " 1999,\n",
       " 2019,\n",
       " 4111,\n",
       " 3554,\n",
       " 6957,\n",
       " 1036,\n",
       " 1036,\n",
       " 1999,\n",
       " 1037,\n",
       " 14865,\n",
       " 3820,\n",
       " 6406,\n",
       " 2012,\n",
       " 1057,\n",
       " 1012,\n",
       " 1055,\n",
       " 1012,\n",
       " 2212,\n",
       " 2457,\n",
       " 1999,\n",
       " 6713,\n",
       " 1010,\n",
       " 3448,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1996,\n",
       " 3715,\n",
       " 2003,\n",
       " 16385,\n",
       " 3085,\n",
       " 2011,\n",
       " 2039,\n",
       " 2000,\n",
       " 2274,\n",
       " 2086,\n",
       " 1999,\n",
       " 3827,\n",
       " 1010,\n",
       " 1037,\n",
       " 1002,\n",
       " 5539,\n",
       " 1010,\n",
       " 2199,\n",
       " 2986,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 2440,\n",
       " 2717,\n",
       " 4183,\n",
       " 13700,\n",
       " 1010,\n",
       " 1037,\n",
       " 2569,\n",
       " 7667,\n",
       " 1998,\n",
       " 1017,\n",
       " 2086,\n",
       " 1997,\n",
       " 13588,\n",
       " 2713,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 1996,\n",
       " 14865,\n",
       " 3066,\n",
       " 2056,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2976,\n",
       " 19608,\n",
       " 3530,\n",
       " 2000,\n",
       " 3198,\n",
       " 2005,\n",
       " 1996,\n",
       " 2659,\n",
       " 2203,\n",
       " 1997,\n",
       " 1996,\n",
       " 23280,\n",
       " 11594,\n",
       " 1012,\n",
       " 1036,\n",
       " 1036,\n",
       " 102,\n",
       " 101,\n",
       " 1996,\n",
       " 13474,\n",
       " 2097,\n",
       " 25803,\n",
       " 5905,\n",
       " 2138,\n",
       " 1996,\n",
       " 13474,\n",
       " 2003,\n",
       " 1999,\n",
       " 2755,\n",
       " 5905,\n",
       " 1997,\n",
       " 1996,\n",
       " 5338,\n",
       " 10048,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 1996,\n",
       " 14865,\n",
       " 3820,\n",
       " 2056,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1999,\n",
       " 2019,\n",
       " 3176,\n",
       " 12654,\n",
       " 1997,\n",
       " 8866,\n",
       " 1010,\n",
       " 2772,\n",
       " 2011,\n",
       " 10967,\n",
       " 2243,\n",
       " 1998,\n",
       " 6406,\n",
       " 2007,\n",
       " 1996,\n",
       " 3820,\n",
       " 1010,\n",
       " 10967,\n",
       " 2243,\n",
       " 4914,\n",
       " 9343,\n",
       " 6770,\n",
       " 12065,\n",
       " 1998,\n",
       " 1996,\n",
       " 3200,\n",
       " 2109,\n",
       " 2005,\n",
       " 2731,\n",
       " 1998,\n",
       " 3554,\n",
       " 1996,\n",
       " 6077,\n",
       " 1010,\n",
       " 2021,\n",
       " 1996,\n",
       " 4861,\n",
       " 2056,\n",
       " 2002,\n",
       " 2106,\n",
       " 2025,\n",
       " 6655,\n",
       " 2006,\n",
       " 1996,\n",
       " 102]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[5]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new : nfl chief , atlanta falcons owner critical of michael vick 's conduct .<q>nfl suspends falcons quarterback indefinitely without pay .<q>vick admits funding dogfighting operation but says he did not gamble .<q>vick due in federal court monday ; future in nfl remains uncertain .<q>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[5]['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[5]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['( cnn ) -- the national football league has indefinitely suspended atlanta falcons quarterback michael vick without pay , officials with the league said friday .',\n",
       " 'nfl star michael vick is set to appear in court monday .',\n",
       " 'a judge will have the final say on a plea deal .',\n",
       " 'earlier , vick admitted to participating in a dogfighting ring as part of a plea agreement with federal prosecutors in virginia . ``',\n",
       " 'your admitted conduct was not only illegal , but also cruel and reprehensible .',\n",
       " 'your team , the nfl , and nfl fans have all been hurt by your actions , `` nfl commissioner roger goodell said in a letter to vick .',\n",
       " 'goodell said he would review the status of the suspension after the legal proceedings are over .',\n",
       " 'in papers filed friday with a federal court in virginia , vick also admitted that he and two co-conspirators killed dogs that did not fight well .',\n",
       " \"falcons owner arthur blank said vick 's admissions describe actions that are `` incomprehensible and unacceptable . ``\",\n",
       " 'the suspension makes `` a strong statement that conduct which tarnishes the good reputation of the nfl will not be tolerated , `` he said in a statement .',\n",
       " \"watch what led to vick 's suspension `` goodell said the falcons could `` assert any claims or remedies `` to recover $ 22 million of vick 's signing bonus from the 10-year , $ 130 million contract he signed in 2004 , according to the associated press .\",\n",
       " 'vick said he would plead guilty to one count of `` conspiracy to travel in interstate commerce in aid of unlawful activities and to sponsor a dog in an animal fighting venture `` in a plea agreement filed at u.s. district court in richmond , virginia .',\n",
       " 'the charge is punishable by up to five years in prison , a $ 250,000 fine , `` full restitution , a special assessment and 3 years of supervised release , `` the plea deal said .',\n",
       " 'federal prosecutors agreed to ask for the low end of the sentencing guidelines . ``',\n",
       " 'the defendant will plead guilty because the defendant is in fact guilty of the charged offense , `` the plea agreement said .',\n",
       " 'in an additional summary of facts , signed by vick and filed with the agreement , vick admitted buying pit bulls and the property used for training and fighting the dogs , but the statement said he did not bet on the fights or receive any of the money won . ``',\n",
       " \"most of the ` bad newz kennels ' operations and gambling monies were provided by vick , `` the official summary of facts said .\",\n",
       " 'gambling wins were generally split among co-conspirators tony taylor , quanis phillips and sometimes purnell peace , it continued . ``',\n",
       " 'vick did not gamble by placing side bets on any of the fights .',\n",
       " \"vick did not receive any of the proceeds from the purses that were won by ` bad newz kennels . '\",\n",
       " '`` vick also agreed that `` collective efforts `` by him and two others caused the deaths of at least six dogs .',\n",
       " \"around april , vick , peace and phillips tested some dogs in fighting sessions at vick 's property in virginia , the statement said . ``\",\n",
       " \"peace , phillips and vick agreed to the killing of approximately 6-8 dogs that did not perform well in ` testing ' sessions at 1915 moonlight road and all of those dogs were killed by various methods , including hanging and drowning . ``\",\n",
       " 'vick agrees and stipulates that these dogs all died as a result of the collective efforts of peace , phillips and vick , `` the summary said .',\n",
       " 'peace , 35 , of virginia beach , virginia ; phillips , 28 , of atlanta , georgia ; and taylor , 34 , of hampton , virginia , already have accepted agreements to plead guilty in exchange for reduced sentences .',\n",
       " 'vick , 27 , is scheduled to appear monday in court , where he is expected to plead guilty before a judge .',\n",
       " 'see a timeline of the case against vick `` the judge in the case will have the final say over the plea agreement .',\n",
       " \"the federal case against vick focused on the interstate conspiracy , but vick 's admission that he was involved in the killing of dogs could lead to local charges , according to cnn legal analyst jeffrey toobin . ``\",\n",
       " 'it sometimes happens -- not often -- that the state will follow a federal prosecution by charging its own crimes for exactly the same behavior , `` toobin said friday . ``',\n",
       " 'the risk for vick is , if he makes admissions in his federal guilty plea , the state of virginia could say , ` hey , look , you admitted violating virginia state law as well .',\n",
       " \"we 're going to introduce that against you and charge you in our court . '\",\n",
       " '`` in the plea deal , vick agreed to cooperate with investigators and provide all information he may have on any criminal activity and to testify if necessary .',\n",
       " 'vick also agreed to turn over any documents he has and to submit to polygraph tests .',\n",
       " 'vick agreed to `` make restitution for the full amount of the costs associated `` with the dogs that are being held by the government . ``',\n",
       " 'such costs may include , but are not limited to , all costs associated with the care of the dogs involved in that case , including if necessary , the long-term care and/or the humane euthanasia of some or all of those animals . ``',\n",
       " 'prosecutors , with the support of animal rights activists , have asked for permission to euthanize the dogs .',\n",
       " 'but the dogs could serve as important evidence in the cases against vick and his admitted co-conspirators .',\n",
       " 'judge henry e. hudson issued an order thursday telling the u.s. marshals service to `` arrest and seize the defendant property , and use discretion and whatever means appropriate to protect and maintain said defendant property . ``',\n",
       " \"both the judge 's order and vick 's filing refer to `` approximately `` 53 pit bull dogs .\",\n",
       " \"after vick 's indictment last month , goodell ordered the quarterback not to report to the falcons training camp , and the league is reviewing the case .\",\n",
       " \"blank told the nfl network on monday he could not speculate on vick 's future as a falcon , at least not until he had seen `` a statement of facts `` in the case .\",\n",
       " \"cnn 's mike phelan contributed to this report .\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[5]['src_txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "To start model training, we need to create a instance of BertSumExtractiveSummarizer, a wrapper for running BertSum-based finetuning. You can select any device ID on your machine, but make sure that you include the string version of the device ID in the gpu_ranks argument.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose which GPU device to use\n",
    "device_id = 0\n",
    "gpu_ranks = str(device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the encoder algorithm. There are four options:\n",
    "- baseline: it used a smaller transformer model to replace the bert model and with transformer summarization layer\n",
    "- classifier: it uses pretrained BERT and fine-tune BERT with **simple logistic classification** summarization layer\n",
    "- transformer: it uses pretrained BERT and fine-tune BERT with **transformer** summarization layer\n",
    "- RNN: it uses pretrained BERT and fine-tune BERT with **LSTM** summarization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = 'transformer'\n",
    "model_base_path = './models/'\n",
    "log_base_path = './logs/'\n",
    "result_base_path = './results'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(model_base_path):\n",
    "    os.makedirs(model_base_path)\n",
    "if not os.path.exists(log_base_path):\n",
    "    os.makedirs(log_base_path)\n",
    "if not os.path.exists(result_base_path):\n",
    "    os.makedirs(result_base_path)\n",
    "    \n",
    "from random import random\n",
    "random_number = random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel,  PretrainedConfig, DistilBertModel, BertModel, DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logs/transformer0.053089538280404525'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_base_path + encoder + str(random_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.models.bert.extractive_text_summarization import BertSumExtractiveSummarizer\n",
    "bertsum_model = BertSumExtractiveSummarizer(encoder = encoder, \n",
    "                                            model_path = model_base_path + encoder + str(random_number),\n",
    "                                            log_file = log_base_path + encoder + str(random_number),\n",
    "                                            bert_config_path = BERT_CONFIG_PATH,\n",
    "                                            gpu_ranks = gpu_ranks,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the fully processed CNN/DM dataset to train the model. During the training, you can stop any time and retrain from the previous saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PREPROCESSED_DATA is False:\n",
    "    training_data_files = [PROCESSED_TRAIN_FILE]\n",
    "else:    \n",
    "    import glob\n",
    "    pts = sorted(glob.glob(BERT_DATA_PATH + 'cnndm.train' + '.[0-9]*.pt'))\n",
    "    training_data_files = pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training_steps is (number of epoches * the total number of batches in the training data )/ accumulation_steps \n",
    "## batch_size is the maximum number of tokens among all the training examples * number of training examples,\n",
    "## training steps used by each GPU should be divided by number of GPU used for fair comparison.\n",
    "## usually 10K steps can yield a model with decent performance\n",
    "if QUICK_RUN:\n",
    "    train_steps = 10000\n",
    "else:\n",
    "    train_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-19 02:32:55,339 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ./temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "[2019-10-19 02:32:55,340 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accum_count': 2, 'batch_size': 3000, 'beta1': 0.9, 'beta2': 0.999, 'block_trigram': True, 'decay_method': 'noam', 'dropout': 0.1, 'encoder': 'transformer', 'ff_size': 512, 'gpu_ranks': '0', 'heads': 4, 'hidden_size': 128, 'inter_layers': 2, 'lr': 0.002, 'max_grad_norm': 0, 'max_nsents': 100, 'max_src_ntokens': 200, 'min_nsents': 3, 'min_src_ntokens': 10, 'optim': 'adam', 'oracle_mode': 'combination', 'param_init': 0.0, 'param_init_glorot': True, 'recall_eval': False, 'report_every': 50, 'report_rouge': True, 'rnn_size': 512, 'save_checkpoint_steps': 500, 'seed': 42, 'temp_dir': './temp', 'test_all': False, 'test_from': '', 'train_from': '', 'use_interval': True, 'visible_gpus': '0', 'warmup_steps': 2000, 'world_size': 1, 'model_path': './models/transformer0.986928701409742', 'log_file': './logs/transformer0.986928701409742', 'bert_config_path': './bert_config_uncased_base.json', 'gpu_ranks_map': {0: 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-19 02:32:55,460 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ./temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "[2019-10-19 02:33:00,740 INFO] * number of parameters: 115790849\n",
      "[2019-10-19 02:33:00,741 INFO] Start training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 0\n",
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-19 02:33:21,029 INFO] Step 50/10000; xent: 3.90; lr: 0.0000011;  50 docs/s;     20 sec\n",
      "[2019-10-19 02:33:40,979 INFO] Step 100/10000; xent: 3.45; lr: 0.0000022;  51 docs/s;     40 sec\n",
      "[2019-10-19 02:34:01,038 INFO] Step 150/10000; xent: 3.42; lr: 0.0000034;  50 docs/s;     60 sec\n",
      "[2019-10-19 02:34:21,678 INFO] Step 200/10000; xent: 3.33; lr: 0.0000045;  49 docs/s;     81 sec\n",
      "[2019-10-19 02:34:41,616 INFO] Step 250/10000; xent: 3.23; lr: 0.0000056;  50 docs/s;    101 sec\n",
      "[2019-10-19 02:35:01,703 INFO] Step 300/10000; xent: 3.14; lr: 0.0000067;  51 docs/s;    121 sec\n",
      "[2019-10-19 02:35:21,877 INFO] Step 350/10000; xent: 3.27; lr: 0.0000078;  51 docs/s;    141 sec\n",
      "[2019-10-19 02:35:42,363 INFO] Step 400/10000; xent: 3.19; lr: 0.0000089;  49 docs/s;    162 sec\n",
      "[2019-10-19 02:36:02,311 INFO] Step 450/10000; xent: 3.20; lr: 0.0000101;  50 docs/s;    181 sec\n",
      "[2019-10-19 02:36:22,238 INFO] Step 500/10000; xent: 3.15; lr: 0.0000112;  51 docs/s;    201 sec\n",
      "[2019-10-19 02:36:22,243 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_500.pt\n",
      "[2019-10-19 02:36:43,577 INFO] Step 550/10000; xent: 3.20; lr: 0.0000123;  47 docs/s;    223 sec\n",
      "[2019-10-19 02:37:04,098 INFO] Step 600/10000; xent: 3.15; lr: 0.0000134;  48 docs/s;    243 sec\n",
      "[2019-10-19 02:37:24,028 INFO] Step 650/10000; xent: 3.09; lr: 0.0000145;  51 docs/s;    263 sec\n",
      "[2019-10-19 02:37:44,052 INFO] Step 700/10000; xent: 3.07; lr: 0.0000157;  50 docs/s;    283 sec\n",
      "[2019-10-19 02:38:03,992 INFO] Step 750/10000; xent: 3.09; lr: 0.0000168;  50 docs/s;    303 sec\n",
      "[2019-10-19 02:38:24,385 INFO] Step 800/10000; xent: 3.11; lr: 0.0000179;  49 docs/s;    324 sec\n",
      "[2019-10-19 02:38:44,304 INFO] Step 850/10000; xent: 3.03; lr: 0.0000190;  50 docs/s;    343 sec\n",
      "[2019-10-19 02:39:04,298 INFO] Step 900/10000; xent: 3.09; lr: 0.0000201;  51 docs/s;    363 sec\n",
      "[2019-10-19 02:39:24,058 INFO] Step 950/10000; xent: 3.01; lr: 0.0000212;  50 docs/s;    383 sec\n",
      "[2019-10-19 02:39:44,710 INFO] Step 1000/10000; xent: 3.03; lr: 0.0000224;  49 docs/s;    404 sec\n",
      "[2019-10-19 02:39:44,714 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_1000.pt\n",
      "[2019-10-19 02:40:05,989 INFO] Step 1050/10000; xent: 2.98; lr: 0.0000235;  48 docs/s;    425 sec\n",
      "[2019-10-19 02:40:25,825 INFO] Step 1100/10000; xent: 3.05; lr: 0.0000246;  50 docs/s;    445 sec\n",
      "[2019-10-19 02:40:45,655 INFO] Step 1150/10000; xent: 2.92; lr: 0.0000257;  51 docs/s;    465 sec\n",
      "[2019-10-19 02:41:06,318 INFO] Step 1200/10000; xent: 3.06; lr: 0.0000268;  49 docs/s;    485 sec\n",
      "[2019-10-19 02:41:26,237 INFO] Step 1250/10000; xent: 2.97; lr: 0.0000280;  51 docs/s;    505 sec\n",
      "[2019-10-19 02:41:46,149 INFO] Step 1300/10000; xent: 2.98; lr: 0.0000291;  51 docs/s;    525 sec\n",
      "[2019-10-19 02:42:05,946 INFO] Step 1350/10000; xent: 2.97; lr: 0.0000302;  50 docs/s;    545 sec\n",
      "[2019-10-19 02:42:26,622 INFO] Step 1400/10000; xent: 2.92; lr: 0.0000313;  49 docs/s;    566 sec\n",
      "[2019-10-19 02:42:46,524 INFO] Step 1450/10000; xent: 2.96; lr: 0.0000324;  50 docs/s;    586 sec\n",
      "[2019-10-19 02:43:06,344 INFO] Step 1500/10000; xent: 2.93; lr: 0.0000335;  50 docs/s;    605 sec\n",
      "[2019-10-19 02:43:06,349 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_1500.pt\n",
      "[2019-10-19 02:43:27,532 INFO] Step 1550/10000; xent: 3.00; lr: 0.0000347;  48 docs/s;    627 sec\n",
      "[2019-10-19 02:43:48,869 INFO] Step 1600/10000; xent: 2.95; lr: 0.0000358;  47 docs/s;    648 sec\n",
      "[2019-10-19 02:44:08,743 INFO] Step 1650/10000; xent: 2.92; lr: 0.0000369;  50 docs/s;    668 sec\n",
      "[2019-10-19 02:44:28,794 INFO] Step 1700/10000; xent: 2.90; lr: 0.0000380;  51 docs/s;    688 sec\n",
      "[2019-10-19 02:44:48,474 INFO] Step 1750/10000; xent: 2.94; lr: 0.0000391;  51 docs/s;    708 sec\n",
      "[2019-10-19 02:45:09,125 INFO] Step 1800/10000; xent: 2.88; lr: 0.0000402;  49 docs/s;    728 sec\n",
      "[2019-10-19 02:45:29,074 INFO] Step 1850/10000; xent: 3.02; lr: 0.0000414;  51 docs/s;    748 sec\n",
      "[2019-10-19 02:45:49,046 INFO] Step 1900/10000; xent: 2.96; lr: 0.0000425;  50 docs/s;    768 sec\n",
      "[2019-10-19 02:46:09,041 INFO] Step 1950/10000; xent: 2.89; lr: 0.0000436;  51 docs/s;    788 sec\n",
      "[2019-10-19 02:46:29,682 INFO] Step 2000/10000; xent: 2.91; lr: 0.0000447;  49 docs/s;    809 sec\n",
      "[2019-10-19 02:46:29,686 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_2000.pt\n",
      "[2019-10-19 02:46:50,901 INFO] Step 2050/10000; xent: 2.81; lr: 0.0000442;  48 docs/s;    830 sec\n",
      "[2019-10-19 02:47:10,867 INFO] Step 2100/10000; xent: 2.85; lr: 0.0000436;  50 docs/s;    850 sec\n",
      "[2019-10-19 02:47:30,924 INFO] Step 2150/10000; xent: 2.95; lr: 0.0000431;  50 docs/s;    870 sec\n",
      "[2019-10-19 02:47:51,466 INFO] Step 2200/10000; xent: 2.87; lr: 0.0000426;  49 docs/s;    891 sec\n",
      "[2019-10-19 02:48:11,405 INFO] Step 2250/10000; xent: 2.89; lr: 0.0000422;  50 docs/s;    911 sec\n",
      "[2019-10-19 02:48:31,287 INFO] Step 2300/10000; xent: 2.87; lr: 0.0000417;  50 docs/s;    930 sec\n",
      "[2019-10-19 02:48:51,222 INFO] Step 2350/10000; xent: 2.88; lr: 0.0000413;  51 docs/s;    950 sec\n",
      "[2019-10-19 02:49:11,935 INFO] Step 2400/10000; xent: 2.82; lr: 0.0000408;  49 docs/s;    971 sec\n",
      "[2019-10-19 02:49:31,821 INFO] Step 2450/10000; xent: 2.79; lr: 0.0000404;  51 docs/s;    991 sec\n",
      "[2019-10-19 02:49:51,801 INFO] Step 2500/10000; xent: 2.85; lr: 0.0000400;  51 docs/s;   1011 sec\n",
      "[2019-10-19 02:49:51,805 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_2500.pt\n",
      "[2019-10-19 02:50:12,963 INFO] Step 2550/10000; xent: 2.95; lr: 0.0000396;  48 docs/s;   1032 sec\n",
      "[2019-10-19 02:50:33,442 INFO] Step 2600/10000; xent: 2.78; lr: 0.0000392;  49 docs/s;   1053 sec\n",
      "[2019-10-19 02:50:53,355 INFO] Step 2650/10000; xent: 2.93; lr: 0.0000389;  51 docs/s;   1073 sec\n",
      "[2019-10-19 02:51:13,191 INFO] Step 2700/10000; xent: 2.96; lr: 0.0000385;  51 docs/s;   1092 sec\n",
      "[2019-10-19 02:51:33,095 INFO] Step 2750/10000; xent: 2.87; lr: 0.0000381;  51 docs/s;   1112 sec\n",
      "[2019-10-19 02:51:53,403 INFO] Step 2800/10000; xent: 2.80; lr: 0.0000378;  49 docs/s;   1133 sec\n",
      "[2019-10-19 02:52:13,224 INFO] Step 2850/10000; xent: 2.89; lr: 0.0000375;  50 docs/s;   1152 sec\n",
      "[2019-10-19 02:52:33,198 INFO] Step 2900/10000; xent: 2.89; lr: 0.0000371;  51 docs/s;   1172 sec\n",
      "[2019-10-19 02:52:53,090 INFO] Step 2950/10000; xent: 2.87; lr: 0.0000368;  51 docs/s;   1192 sec\n",
      "[2019-10-19 02:53:13,511 INFO] Step 3000/10000; xent: 2.88; lr: 0.0000365;  49 docs/s;   1213 sec\n",
      "[2019-10-19 02:53:13,514 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_3000.pt\n",
      "[2019-10-19 02:53:34,637 INFO] Step 3050/10000; xent: 2.92; lr: 0.0000362;  48 docs/s;   1234 sec\n",
      "[2019-10-19 02:53:54,402 INFO] Step 3100/10000; xent: 2.81; lr: 0.0000359;  51 docs/s;   1254 sec\n",
      "[2019-10-19 02:54:14,136 INFO] Step 3150/10000; xent: 2.89; lr: 0.0000356;  51 docs/s;   1273 sec\n",
      "[2019-10-19 02:54:34,743 INFO] Step 3200/10000; xent: 2.85; lr: 0.0000354;  49 docs/s;   1294 sec\n",
      "[2019-10-19 02:54:54,686 INFO] Step 3250/10000; xent: 2.84; lr: 0.0000351;  51 docs/s;   1314 sec\n",
      "[2019-10-19 02:55:14,555 INFO] Step 3300/10000; xent: 2.73; lr: 0.0000348;  50 docs/s;   1334 sec\n",
      "[2019-10-19 02:55:34,379 INFO] Step 3350/10000; xent: 2.86; lr: 0.0000346;  51 docs/s;   1354 sec\n",
      "[2019-10-19 02:55:54,853 INFO] Step 3400/10000; xent: 2.79; lr: 0.0000343;  49 docs/s;   1374 sec\n",
      "[2019-10-19 02:56:14,734 INFO] Step 3450/10000; xent: 2.82; lr: 0.0000341;  51 docs/s;   1394 sec\n",
      "[2019-10-19 02:56:34,512 INFO] Step 3500/10000; xent: 2.81; lr: 0.0000338;  51 docs/s;   1414 sec\n",
      "[2019-10-19 02:56:34,516 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_3500.pt\n",
      "[2019-10-19 02:56:55,590 INFO] Step 3550/10000; xent: 2.81; lr: 0.0000336;  48 docs/s;   1435 sec\n",
      "[2019-10-19 02:57:16,000 INFO] Step 3600/10000; xent: 2.77; lr: 0.0000333;  49 docs/s;   1455 sec\n",
      "[2019-10-19 02:57:35,833 INFO] Step 3650/10000; xent: 2.81; lr: 0.0000331;  51 docs/s;   1475 sec\n",
      "[2019-10-19 02:57:55,680 INFO] Step 3700/10000; xent: 2.85; lr: 0.0000329;  51 docs/s;   1495 sec\n",
      "[2019-10-19 02:58:15,494 INFO] Step 3750/10000; xent: 2.83; lr: 0.0000327;  51 docs/s;   1515 sec\n",
      "[2019-10-19 02:58:35,990 INFO] Step 3800/10000; xent: 2.89; lr: 0.0000324;  49 docs/s;   1535 sec\n",
      "[2019-10-19 02:58:55,844 INFO] Step 3850/10000; xent: 2.87; lr: 0.0000322;  51 docs/s;   1555 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-19 02:59:15,758 INFO] Step 3900/10000; xent: 2.80; lr: 0.0000320;  50 docs/s;   1575 sec\n",
      "[2019-10-19 02:59:35,515 INFO] Step 3950/10000; xent: 2.89; lr: 0.0000318;  51 docs/s;   1595 sec\n",
      "[2019-10-19 02:59:57,046 INFO] Step 4000/10000; xent: 2.81; lr: 0.0000316;  47 docs/s;   1616 sec\n",
      "[2019-10-19 02:59:57,049 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_4000.pt\n",
      "[2019-10-19 03:00:18,131 INFO] Step 4050/10000; xent: 2.90; lr: 0.0000314;  47 docs/s;   1637 sec\n",
      "[2019-10-19 03:00:37,971 INFO] Step 4100/10000; xent: 2.85; lr: 0.0000312;  51 docs/s;   1657 sec\n",
      "[2019-10-19 03:00:57,735 INFO] Step 4150/10000; xent: 2.87; lr: 0.0000310;  51 docs/s;   1677 sec\n",
      "[2019-10-19 03:01:18,349 INFO] Step 4200/10000; xent: 2.92; lr: 0.0000309;  49 docs/s;   1698 sec\n",
      "[2019-10-19 03:01:38,242 INFO] Step 4250/10000; xent: 2.86; lr: 0.0000307;  51 docs/s;   1717 sec\n",
      "[2019-10-19 03:01:57,970 INFO] Step 4300/10000; xent: 2.81; lr: 0.0000305;  51 docs/s;   1737 sec\n",
      "[2019-10-19 03:02:17,720 INFO] Step 4350/10000; xent: 2.87; lr: 0.0000303;  51 docs/s;   1757 sec\n",
      "[2019-10-19 03:02:38,092 INFO] Step 4400/10000; xent: 2.80; lr: 0.0000302;  49 docs/s;   1777 sec\n",
      "[2019-10-19 03:02:57,965 INFO] Step 4450/10000; xent: 2.82; lr: 0.0000300;  51 docs/s;   1797 sec\n",
      "[2019-10-19 03:03:17,682 INFO] Step 4500/10000; xent: 2.88; lr: 0.0000298;  51 docs/s;   1817 sec\n",
      "[2019-10-19 03:03:17,685 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_4500.pt\n",
      "[2019-10-19 03:03:38,792 INFO] Step 4550/10000; xent: 2.82; lr: 0.0000296;  48 docs/s;   1838 sec\n",
      "[2019-10-19 03:03:59,232 INFO] Step 4600/10000; xent: 2.82; lr: 0.0000295;  49 docs/s;   1858 sec\n",
      "[2019-10-19 03:04:19,051 INFO] Step 4650/10000; xent: 2.82; lr: 0.0000293;  51 docs/s;   1878 sec\n",
      "[2019-10-19 03:04:38,825 INFO] Step 4700/10000; xent: 2.85; lr: 0.0000292;  51 docs/s;   1898 sec\n",
      "[2019-10-19 03:04:58,717 INFO] Step 4750/10000; xent: 2.82; lr: 0.0000290;  51 docs/s;   1918 sec\n",
      "[2019-10-19 03:05:20,029 INFO] Step 4800/10000; xent: 2.86; lr: 0.0000289;  47 docs/s;   1939 sec\n",
      "[2019-10-19 03:05:39,788 INFO] Step 4850/10000; xent: 2.82; lr: 0.0000287;  51 docs/s;   1959 sec\n",
      "[2019-10-19 03:05:59,515 INFO] Step 4900/10000; xent: 2.82; lr: 0.0000286;  52 docs/s;   1979 sec\n",
      "[2019-10-19 03:06:19,197 INFO] Step 4950/10000; xent: 2.84; lr: 0.0000284;  51 docs/s;   1998 sec\n",
      "[2019-10-19 03:06:39,516 INFO] Step 5000/10000; xent: 2.79; lr: 0.0000283;  49 docs/s;   2019 sec\n",
      "[2019-10-19 03:06:39,519 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_5000.pt\n",
      "[2019-10-19 03:07:00,548 INFO] Step 5050/10000; xent: 2.84; lr: 0.0000281;  48 docs/s;   2040 sec\n",
      "[2019-10-19 03:07:20,369 INFO] Step 5100/10000; xent: 2.73; lr: 0.0000280;  51 docs/s;   2060 sec\n",
      "[2019-10-19 03:07:40,154 INFO] Step 5150/10000; xent: 2.86; lr: 0.0000279;  50 docs/s;   2079 sec\n",
      "[2019-10-19 03:08:00,127 INFO] Step 5200/10000; xent: 2.80; lr: 0.0000277;  50 docs/s;   2099 sec\n",
      "[2019-10-19 03:08:19,959 INFO] Step 5250/10000; xent: 2.89; lr: 0.0000276;  51 docs/s;   2119 sec\n",
      "[2019-10-19 03:08:39,783 INFO] Step 5300/10000; xent: 2.91; lr: 0.0000275;  51 docs/s;   2139 sec\n",
      "[2019-10-19 03:08:59,614 INFO] Step 5350/10000; xent: 2.75; lr: 0.0000273;  51 docs/s;   2159 sec\n",
      "[2019-10-19 03:09:19,675 INFO] Step 5400/10000; xent: 2.81; lr: 0.0000272;  50 docs/s;   2179 sec\n",
      "[2019-10-19 03:09:39,495 INFO] Step 5450/10000; xent: 2.81; lr: 0.0000271;  51 docs/s;   2199 sec\n",
      "[2019-10-19 03:09:59,266 INFO] Step 5500/10000; xent: 2.84; lr: 0.0000270;  51 docs/s;   2218 sec\n",
      "[2019-10-19 03:09:59,269 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_5500.pt\n",
      "[2019-10-19 03:10:20,352 INFO] Step 5550/10000; xent: 2.82; lr: 0.0000268;  48 docs/s;   2240 sec\n",
      "[2019-10-19 03:10:40,538 INFO] Step 5600/10000; xent: 2.85; lr: 0.0000267;  51 docs/s;   2260 sec\n",
      "[2019-10-19 03:11:00,400 INFO] Step 5650/10000; xent: 2.84; lr: 0.0000266;  50 docs/s;   2280 sec\n",
      "[2019-10-19 03:11:20,172 INFO] Step 5700/10000; xent: 2.73; lr: 0.0000265;  51 docs/s;   2299 sec\n",
      "[2019-10-19 03:11:40,047 INFO] Step 5750/10000; xent: 2.84; lr: 0.0000264;  51 docs/s;   2319 sec\n",
      "[2019-10-19 03:11:59,932 INFO] Step 5800/10000; xent: 2.78; lr: 0.0000263;  50 docs/s;   2339 sec\n",
      "[2019-10-19 03:12:19,839 INFO] Step 5850/10000; xent: 2.90; lr: 0.0000261;  51 docs/s;   2359 sec\n",
      "[2019-10-19 03:12:39,543 INFO] Step 5900/10000; xent: 2.86; lr: 0.0000260;  51 docs/s;   2379 sec\n",
      "[2019-10-19 03:12:59,289 INFO] Step 5950/10000; xent: 2.74; lr: 0.0000259;  51 docs/s;   2398 sec\n",
      "[2019-10-19 03:13:19,324 INFO] Step 6000/10000; xent: 2.80; lr: 0.0000258;  50 docs/s;   2418 sec\n",
      "[2019-10-19 03:13:19,326 INFO] Saving checkpoint ./models/transformer0.986928701409742/model_step_6000.pt\n",
      "[2019-10-19 03:13:40,112 INFO] Step 6050/10000; xent: 2.86; lr: 0.0000257;  48 docs/s;   2439 sec\n",
      "[2019-10-19 03:13:59,765 INFO] Step 6100/10000; xent: 2.82; lr: 0.0000256;  51 docs/s;   2459 sec\n",
      "[2019-10-19 03:14:19,449 INFO] Step 6150/10000; xent: 2.80; lr: 0.0000255;  51 docs/s;   2479 sec\n",
      "[2019-10-19 03:14:39,429 INFO] Step 6200/10000; xent: 2.80; lr: 0.0000254;  51 docs/s;   2499 sec\n"
     ]
    }
   ],
   "source": [
    "bertsum_model.fit(device_id, training_data_files, BertModel, \"bert-base-uncased\", None, train_steps=train_steps, train_from=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)), or Recall-Oriented Understudy for Gisting Evaluation has been commonly used for evaluation text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils_nlp.models.bert.extractive_text_summarization import get_data_iter\n",
    "import os\n",
    "if USE_PREPROCESSED_DATA is False: \n",
    "    test_dataset=torch.load(PROCESSED_TEST_FILE)\n",
    "else:\n",
    "    test_dataset=[]\n",
    "    for i in range(0,6):\n",
    "        filename = os.path.join(BERT_DATA_PATH, \"cnndm.test.{0}.bert.pt\".format(i))\n",
    "        test_dataset.extend(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-21 15:22:04,427 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ./temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "[2019-10-21 15:22:04,434 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2019-10-21 15:22:04,556 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ./temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "[2019-10-21 15:22:29,189 INFO] * number of parameters: 115790849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 0\n",
      "gpu_rank 0\n"
     ]
    }
   ],
   "source": [
    "checkpoint_to_test = 10000\n",
    "model_for_test = \"./models/transformer0.986928701409742/model_step_10000.pt\" # os.path.join(model_base_path + encoder + str(random_number), f\"model_step_{checkpoint_to_test}.pt\")\n",
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter(test_dataset),  BertModel, \"bert-base-uncased\",None,\n",
    "                                   test_from=model_for_test,\n",
    "                                   sentence_seperator='<q>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11489"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11489\n",
      "11489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-21 15:25:00,660 [MainThread  ] [INFO ]  Writing summaries.\n",
      "[2019-10-21 15:25:00,660 INFO] Writing summaries.\n",
      "2019-10-21 15:25:00,667 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./results/tmp2zfj1op9/system and model files to ./results/tmp2zfj1op9/model.\n",
      "[2019-10-21 15:25:00,667 INFO] Processing summaries. Saving system files to ./results/tmp2zfj1op9/system and model files to ./results/tmp2zfj1op9/model.\n",
      "2019-10-21 15:25:00,669 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-10-21-15-24-59/candidate/.\n",
      "[2019-10-21 15:25:00,669 INFO] Processing files in ./results/rouge-tmp-2019-10-21-15-24-59/candidate/.\n",
      "2019-10-21 15:25:01,839 [MainThread  ] [INFO ]  Saved processed files to ./results/tmp2zfj1op9/system.\n",
      "[2019-10-21 15:25:01,839 INFO] Saved processed files to ./results/tmp2zfj1op9/system.\n",
      "2019-10-21 15:25:01,841 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-10-21-15-24-59/reference/.\n",
      "[2019-10-21 15:25:01,841 INFO] Processing files in ./results/rouge-tmp-2019-10-21-15-24-59/reference/.\n",
      "2019-10-21 15:25:03,041 [MainThread  ] [INFO ]  Saved processed files to ./results/tmp2zfj1op9/model.\n",
      "[2019-10-21 15:25:03,041 INFO] Saved processed files to ./results/tmp2zfj1op9/model.\n",
      "2019-10-21 15:25:03,125 [MainThread  ] [INFO ]  Written ROUGE configuration to ./results/tmp7d7wxa89/rouge_conf.xml\n",
      "[2019-10-21 15:25:03,125 INFO] Written ROUGE configuration to ./results/tmp7d7wxa89/rouge_conf.xml\n",
      "2019-10-21 15:25:03,126 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmp7d7wxa89/rouge_conf.xml\n",
      "[2019-10-21 15:25:03,126 INFO] Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmp7d7wxa89/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.53312 (95%-conf.int. 0.53040 - 0.53597)\n",
      "1 ROUGE-1 Average_P: 0.37714 (95%-conf.int. 0.37466 - 0.37954)\n",
      "1 ROUGE-1 Average_F: 0.42706 (95%-conf.int. 0.42495 - 0.42920)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.24620 (95%-conf.int. 0.24345 - 0.24885)\n",
      "1 ROUGE-2 Average_P: 0.17488 (95%-conf.int. 0.17273 - 0.17718)\n",
      "1 ROUGE-2 Average_F: 0.19740 (95%-conf.int. 0.19524 - 0.19962)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.48760 (95%-conf.int. 0.48494 - 0.49034)\n",
      "1 ROUGE-L Average_P: 0.34555 (95%-conf.int. 0.34313 - 0.34796)\n",
      "1 ROUGE-L Average_F: 0.39100 (95%-conf.int. 0.38889 - 0.39312)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.eval.evaluate_summarization import get_rouge\n",
    "rouge_baseline = get_rouge(prediction, target, \"./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .<q>he was flown back to chicago via air ambulance on march 20 , but he died on sunday .<q>he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "args=Bunch({\"max_nsents\": int(1e5), \n",
    "            \"max_src_ntokens\": int(2e6), \n",
    "            \"min_nsents\": -1, \n",
    "            \"min_src_ntokens\": -1,  \n",
    "            \"use_interval\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-15 18:47:09,248 INFO] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/daden/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from bertsum.prepro.data_builder import BertData\n",
    "bertdata = BertData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from utils_nlp.dataset.harvardnlp_cnndm import preprocess\n",
    "from nltk import tokenize\n",
    "from bertsum.others.utils import clean\n",
    "def preprocess_source(line):\n",
    "    return preprocess((line, [clean, tokenize.sent_tokenize], nltk.word_tokenize))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '\\n'.join(test_dataset[0]['src_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_src = preprocess_source(text)\n",
    "b_data = bertdata.preprocess(new_src, None, None)\n",
    "indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "b_data_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "               'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a',\n",
       "  'university',\n",
       "  'of',\n",
       "  'iowa',\n",
       "  'student',\n",
       "  'has',\n",
       "  'died',\n",
       "  'nearly',\n",
       "  'three',\n",
       "  'months',\n",
       "  'after',\n",
       "  'a',\n",
       "  'fall',\n",
       "  'in',\n",
       "  'rome',\n",
       "  'in',\n",
       "  'a',\n",
       "  'suspected',\n",
       "  'robbery',\n",
       "  'attack',\n",
       "  'in',\n",
       "  'rome',\n",
       "  '.'],\n",
       " ['andrew',\n",
       "  'mogni',\n",
       "  ',',\n",
       "  '20',\n",
       "  ',',\n",
       "  'from',\n",
       "  'glen',\n",
       "  'ellyn',\n",
       "  ',',\n",
       "  'illinois',\n",
       "  ',',\n",
       "  'had',\n",
       "  'only',\n",
       "  'just',\n",
       "  'arrived',\n",
       "  'for',\n",
       "  'a',\n",
       "  'semester',\n",
       "  'program',\n",
       "  'in',\n",
       "  'italy',\n",
       "  'when',\n",
       "  'the',\n",
       "  'incident',\n",
       "  'happened',\n",
       "  'in',\n",
       "  'january',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'was',\n",
       "  'flown',\n",
       "  'back',\n",
       "  'to',\n",
       "  'chicago',\n",
       "  'via',\n",
       "  'air',\n",
       "  'ambulance',\n",
       "  'on',\n",
       "  'march',\n",
       "  '20',\n",
       "  ',',\n",
       "  'but',\n",
       "  'he',\n",
       "  'died',\n",
       "  'on',\n",
       "  'sunday',\n",
       "  '.'],\n",
       " ['andrew',\n",
       "  'mogni',\n",
       "  ',',\n",
       "  '20',\n",
       "  ',',\n",
       "  'from',\n",
       "  'glen',\n",
       "  'ellyn',\n",
       "  ',',\n",
       "  'illinois',\n",
       "  ',',\n",
       "  'a',\n",
       "  'university',\n",
       "  'of',\n",
       "  'iowa',\n",
       "  'student',\n",
       "  'has',\n",
       "  'died',\n",
       "  'nearly',\n",
       "  'three',\n",
       "  'months',\n",
       "  'after',\n",
       "  'a',\n",
       "  'fall',\n",
       "  'in',\n",
       "  'rome',\n",
       "  'in',\n",
       "  'a',\n",
       "  'suspected',\n",
       "  'robbery',\n",
       "  'he',\n",
       "  'was',\n",
       "  'taken',\n",
       "  'to',\n",
       "  'a',\n",
       "  'medical',\n",
       "  'facility',\n",
       "  'in',\n",
       "  'the',\n",
       "  'chicago',\n",
       "  'area',\n",
       "  ',',\n",
       "  'close',\n",
       "  'to',\n",
       "  'his',\n",
       "  'family',\n",
       "  'home',\n",
       "  'in',\n",
       "  'glen',\n",
       "  'ellyn',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'died',\n",
       "  'on',\n",
       "  'sunday',\n",
       "  'at',\n",
       "  'northwestern',\n",
       "  'memorial',\n",
       "  'hospital',\n",
       "  '-',\n",
       "  'medical',\n",
       "  'examiner',\n",
       "  \"'s\",\n",
       "  'office',\n",
       "  'spokesman',\n",
       "  'frank',\n",
       "  'shuftan',\n",
       "  'says',\n",
       "  'a',\n",
       "  'cause',\n",
       "  'of',\n",
       "  'death',\n",
       "  'wo',\n",
       "  \"n't\",\n",
       "  'be',\n",
       "  'released',\n",
       "  'until',\n",
       "  'monday',\n",
       "  'at',\n",
       "  'the',\n",
       "  'earliest',\n",
       "  '.'],\n",
       " ['initial',\n",
       "  'police',\n",
       "  'reports',\n",
       "  'indicated',\n",
       "  'the',\n",
       "  'fall',\n",
       "  'was',\n",
       "  'an',\n",
       "  'accident',\n",
       "  'but',\n",
       "  'authorities',\n",
       "  'are',\n",
       "  'investigating',\n",
       "  'the',\n",
       "  'possibility',\n",
       "  'that',\n",
       "  'mogni',\n",
       "  'was',\n",
       "  'robbed',\n",
       "  '.'],\n",
       " ['on',\n",
       "  'sunday',\n",
       "  ',',\n",
       "  'his',\n",
       "  'cousin',\n",
       "  'abby',\n",
       "  'wrote',\n",
       "  'online',\n",
       "  ':',\n",
       "  '`',\n",
       "  'this',\n",
       "  'morning',\n",
       "  'my',\n",
       "  'cousin',\n",
       "  'andrew',\n",
       "  \"'s\",\n",
       "  'soul',\n",
       "  'was',\n",
       "  'lifted',\n",
       "  'up',\n",
       "  'to',\n",
       "  'heaven',\n",
       "  '.'],\n",
       " ['initial',\n",
       "  'police',\n",
       "  'reports',\n",
       "  'indicated',\n",
       "  'the',\n",
       "  'fall',\n",
       "  'was',\n",
       "  'an',\n",
       "  'accident',\n",
       "  'but',\n",
       "  'authorities',\n",
       "  'are',\n",
       "  'investigating',\n",
       "  'the',\n",
       "  'possibility',\n",
       "  'that',\n",
       "  'mogni',\n",
       "  'was',\n",
       "  'robbed',\n",
       "  '`',\n",
       "  'at',\n",
       "  'the',\n",
       "  'beginning',\n",
       "  'of',\n",
       "  'january',\n",
       "  'he',\n",
       "  'went',\n",
       "  'to',\n",
       "  'rome',\n",
       "  'to',\n",
       "  'study',\n",
       "  'aboard',\n",
       "  'and',\n",
       "  'on',\n",
       "  'the',\n",
       "  'way',\n",
       "  'home',\n",
       "  'from',\n",
       "  'a',\n",
       "  'party',\n",
       "  'he',\n",
       "  'was',\n",
       "  'brutally',\n",
       "  'attacked',\n",
       "  'and',\n",
       "  'thrown',\n",
       "  'off',\n",
       "  'a',\n",
       "  '40ft',\n",
       "  'bridge',\n",
       "  'and',\n",
       "  'hit',\n",
       "  'the',\n",
       "  'concrete',\n",
       "  'below',\n",
       "  '.'],\n",
       " ['`',\n",
       "  'he',\n",
       "  'was',\n",
       "  'in',\n",
       "  'a',\n",
       "  'coma',\n",
       "  'and',\n",
       "  'in',\n",
       "  'critical',\n",
       "  'condition',\n",
       "  'for',\n",
       "  'months',\n",
       "  '.',\n",
       "  \"'\"],\n",
       " ['paula',\n",
       "  'barnett',\n",
       "  ',',\n",
       "  'who',\n",
       "  'said',\n",
       "  'she',\n",
       "  'is',\n",
       "  'a',\n",
       "  'close',\n",
       "  'family',\n",
       "  'friend',\n",
       "  ',',\n",
       "  'told',\n",
       "  'my',\n",
       "  'suburban',\n",
       "  'life',\n",
       "  ',',\n",
       "  'that',\n",
       "  'mogni',\n",
       "  'had',\n",
       "  'only',\n",
       "  'been',\n",
       "  'in',\n",
       "  'the',\n",
       "  'country',\n",
       "  'for',\n",
       "  'six',\n",
       "  'hours',\n",
       "  'when',\n",
       "  'the',\n",
       "  'incident',\n",
       "  'happened',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'said',\n",
       "  'he',\n",
       "  'was',\n",
       "  'was',\n",
       "  'alone',\n",
       "  'at',\n",
       "  'the',\n",
       "  'time',\n",
       "  'of',\n",
       "  'the',\n",
       "  'alleged',\n",
       "  'assault',\n",
       "  'and',\n",
       "  'personal',\n",
       "  'items',\n",
       "  'were',\n",
       "  'stolen',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'added',\n",
       "  'that',\n",
       "  'he',\n",
       "  'was',\n",
       "  'in',\n",
       "  'a',\n",
       "  'non-medically',\n",
       "  'induced',\n",
       "  'coma',\n",
       "  ',',\n",
       "  'having',\n",
       "  'suffered',\n",
       "  'serious',\n",
       "  'infection',\n",
       "  'and',\n",
       "  'internal',\n",
       "  'bleeding',\n",
       "  '.'],\n",
       " ['mogni',\n",
       "  'was',\n",
       "  'a',\n",
       "  'third-year',\n",
       "  'finance',\n",
       "  'major',\n",
       "  'from',\n",
       "  'glen',\n",
       "  'ellyn',\n",
       "  ',',\n",
       "  'ill.',\n",
       "  ',',\n",
       "  'who',\n",
       "  'was',\n",
       "  'participating',\n",
       "  'in',\n",
       "  'a',\n",
       "  'semester-long',\n",
       "  'program',\n",
       "  'at',\n",
       "  'john',\n",
       "  'cabot',\n",
       "  'university',\n",
       "  '.'],\n",
       " ['mogni',\n",
       "  'belonged',\n",
       "  'to',\n",
       "  'the',\n",
       "  'school',\n",
       "  \"'s\",\n",
       "  'chapter',\n",
       "  'of',\n",
       "  'the',\n",
       "  'sigma',\n",
       "  'nu',\n",
       "  'fraternity',\n",
       "  ',',\n",
       "  'reports',\n",
       "  'the',\n",
       "  'chicago',\n",
       "  'tribune',\n",
       "  'who',\n",
       "  'posted',\n",
       "  'a',\n",
       "  'sign',\n",
       "  'outside',\n",
       "  'a',\n",
       "  'building',\n",
       "  'reading',\n",
       "  '`',\n",
       "  'pray',\n",
       "  'for',\n",
       "  'mogni',\n",
       "  '.',\n",
       "  \"'\"],\n",
       " ['the',\n",
       "  'fraternity',\n",
       "  \"'s\",\n",
       "  'iowa',\n",
       "  'chapter',\n",
       "  'announced',\n",
       "  'sunday',\n",
       "  'afternoon',\n",
       "  'via',\n",
       "  'twitter',\n",
       "  'that',\n",
       "  'a',\n",
       "  'memorial',\n",
       "  'service',\n",
       "  'will',\n",
       "  'be',\n",
       "  'held',\n",
       "  'on',\n",
       "  'campus',\n",
       "  'to',\n",
       "  'remember',\n",
       "  'mogni',\n",
       "  '.']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .',\n",
       " 'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robbery he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .',\n",
       " \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .',\n",
       " \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed ` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .',\n",
       " \"` he was in a coma and in critical condition for months . '\",\n",
       " 'paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .',\n",
       " 'she said he was was alone at the time of the alleged assault and personal items were stolen .',\n",
       " 'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .',\n",
       " 'mogni was a third-year finance major from glen ellyn , ill. , who was participating in a semester-long program at john cabot university .',\n",
       " \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni . '\",\n",
       " \"the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_data_dict['src_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_data_dict['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-15 18:47:11,792 INFO] * number of parameters: 115790849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 0\n",
      "gpu_rank 0\n"
     ]
    }
   ],
   "source": [
    "model_for_test =  os.path.join(model_base_path + encoder + str(random_number), f\"model_step_{checkpoint_to_test}.pt\")\n",
    "#get_data_iter(output,batch_size=30000)\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter([b_data_dict], False),\n",
    "                                   test_from=model_for_test, sentence_seperator='<q>' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .<q>he was flown back to chicago via air ambulance on march 20 , but he died on sunday .<q>a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['tgt_txt']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6 cm3",
   "language": "python",
   "name": "cm3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
