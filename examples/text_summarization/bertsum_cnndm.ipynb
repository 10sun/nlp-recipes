{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Text Summerization on CNN/DM Dataset using BertSum\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook demonstrates how to fine tune BERT for extractive text summerization. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, result postprocessing, and model evaluation.\n",
    "\n",
    "BertSum refers to  [Fine-tune BERT for Extractive Summarization](https://arxiv.org/pdf/1903.10318.pdf) with [published example](https://github.com/nlpyang/BertSum/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Before we start the notebook, we should set the environment variable to make sure you can access the GPUs on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprossing\n",
    "\n",
    "The CNN/DM dataset we used can be downloaded from https://github.com/harvardnlp/sent-summary. The following notebook assumes the dataset has been unzipped to folder ./harvardnl_cnndm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/dadendev/BertSum/src')\n",
    "sys.path.insert(0, '/dadendev/textsum//wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing import harvardnlp_cnndm_preprocess,  bertsum_formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions defined specific in harvardnlp_cnndm_preprocess function are unique to CNN/DM dataset that's processed by harvardnlp. However, it provides a skeleton of how to preprocessing data into the format that BertSum takes. Assuming you have all articles and target summery each in a file, line seperated, the steps to preprocess the data are:\n",
    "1. sentence tokenization\n",
    "2. word tokenization\n",
    "3. format to bertdata\n",
    "    - use algorithms to label the sentences in the article with 1 meaning the sentence is selected\n",
    "    2. [CLS] and [SEP] are inserted before and after each sentence\n",
    "    3. segment ids are inserted\n",
    "    4. [CLS] token position are logged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK_RUN = True\n",
    "max_job_number = -1\n",
    "#if QUICK_RUN:\n",
    "#    max_job_number = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-03 13:53:40,310 INFO] loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/daden/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of training data: 11490\n"
     ]
    }
   ],
   "source": [
    "train_src_file = \"./harvardnlp_cnndm/test.txt.src\"\n",
    "train_tgt_file = \"./harvardnlp_cnndm/test.txt.tgt.tagged\"\n",
    "import multiprocessing\n",
    "n_cpus = multiprocessing.cpu_count() - 1\n",
    "jobs = harvardnlp_cnndm_preprocess(n_cpus, train_src_file, train_tgt_file)\n",
    "print(\"total length of training data:\", len(jobs))\n",
    "from prepro.data_builder import BertData\n",
    "from bertsum_config import args\n",
    "output_file = \"./harvardnlp_cnndm/test.bertdata\"\n",
    "bertdata = BertData(args)\n",
    "bertsum_formatting(n_cpus, bertdata,\"combination\", jobs[0:max_job_number], output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['src', 'labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "bert_format_data = torch.load(\"./bert_train_data_all_none_excluded\")\n",
    "print(len(bert_format_data))\n",
    "bert_format_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mentally ill inmates in miami are housed on the `` forgotten floor `` judge steven leifman says most are there as a result of `` avoidable felonies `` while cnn tours facility , patient shouts : `` i am the son of the president `` leifman says the system is unjust and he 's fighting for change .\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 31,\n",
       " 54,\n",
       " 80,\n",
       " 119,\n",
       " 142,\n",
       " 180,\n",
       " 194,\n",
       " 250,\n",
       " 278,\n",
       " 289,\n",
       " 307,\n",
       " 337,\n",
       " 362,\n",
       " 372,\n",
       " 399,\n",
       " 415,\n",
       " 433,\n",
       " 457,\n",
       " 484]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['clss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"editor 's note : in our behind the scenes series , cnn correspondents share their experiences in covering news and analyze the stories behind the events .\",\n",
       " \"here , soledad o'brien takes users inside a jail where many of the inmates are mentally ill .\",\n",
       " 'an inmate housed on the `` forgotten floor , `` where many mentally ill inmates are housed in miami before trial .',\n",
       " 'miami , florida -lrb- cnn -rrb- -- the ninth floor of the miami-dade pretrial detention facility is dubbed the `` forgotten floor . ``',\n",
       " \"here , inmates with the most severe mental illnesses are incarcerated until they 're ready to appear in court .\",\n",
       " 'most often , they face drug charges or charges of assaulting an officer -- charges that judge steven leifman says are usually `` avoidable felonies . ``',\n",
       " 'he says the arrests often result from confrontations with police .',\n",
       " \"mentally ill people often wo n't do what they 're told when police arrive on the scene -- confrontation seems to exacerbate their illness and they become more paranoid , delusional , and less likely to follow directions , according to leifman .\",\n",
       " \"so , they end up on the ninth floor severely mentally disturbed , but not getting any real help because they 're in jail .\",\n",
       " 'we toured the jail with leifman .',\n",
       " 'he is well known in miami as an advocate for justice and the mentally ill .',\n",
       " 'even though we were not exactly welcomed with open arms by the guards , we were given permission to shoot videotape and tour the floor .',\n",
       " \"go inside the ` forgotten floor ' `` at first , it 's hard to determine where the people are .\",\n",
       " 'the prisoners are wearing sleeveless robes .',\n",
       " \"imagine cutting holes for arms and feet in a heavy wool sleeping bag -- that 's kind of what they look like .\",\n",
       " \"they 're designed to keep the mentally ill patients from injuring themselves .\",\n",
       " \"that 's also why they have no shoes , laces or mattresses .\",\n",
       " 'leifman says about one-third of all people in miami-dade county jails are mentally ill .',\n",
       " 'so , he says , the sheer volume is overwhelming the system , and the result is what we see on the ninth floor .',\n",
       " \"of course , it is a jail , so it 's not supposed to be warm and comforting , but the lights glare , the cells are tiny and it 's loud .\",\n",
       " 'we see two , sometimes three men -- sometimes in the robes , sometimes naked , lying or sitting in their cells .',\n",
       " '`` i am the son of the president .',\n",
       " 'you need to get me out of here ! ``',\n",
       " 'one man shouts at me .',\n",
       " 'he is absolutely serious , convinced that help is on the way -- if only he could reach the white house .',\n",
       " 'leifman tells me that these prisoner-patients will often circulate through the system , occasionally stabilizing in a mental hospital , only to return to jail to face their charges .',\n",
       " \"it 's brutally unjust , in his mind , and he has become a strong advocate for changing things in miami .\",\n",
       " 'over a meal later , we talk about how things got this way for mental patients .',\n",
       " 'leifman says 200 years ago people were considered `` lunatics `` and they were locked up in jails even if they had no charges against them .',\n",
       " 'they were just considered unfit to be in society .',\n",
       " 'over the years , he says , there was some public outcry , and the mentally ill were moved out of jails and into hospitals .',\n",
       " 'but leifman says many of these mental hospitals were so horrible they were shut down .',\n",
       " 'where did the patients go ?',\n",
       " 'they became , in many cases , the homeless , he says .',\n",
       " 'they never got treatment .',\n",
       " 'leifman says in 1955 there were more than half a million people in state mental hospitals , and today that number has been reduced 90 percent , and 40,000 to 50,000 people are in mental hospitals .',\n",
       " \"the judge says he 's working to change this .\",\n",
       " 'starting in 2008 , many inmates who would otherwise have been brought to the `` forgotten floor `` will instead be sent to a new mental health facility -- the first step on a journey toward long-term treatment , not just punishment .',\n",
       " \"leifman says it 's not the complete answer , but it 's a start .\",\n",
       " \"leifman says the best part is that it 's a win-win solution .\",\n",
       " 'the patients win , the families are relieved , and the state saves money by simply not cycling these prisoners through again and again .',\n",
       " 'and , for leifman , justice is served .',\n",
       " 'e-mail to a friend .']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['src_txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "To start model training, we need to create a instance of BertSumExtractiveSummarizer, a wrapper for running BertSum-based finetuning. You can select any device ID on your machine, but make sure that you include the string version of the device ID in the gpu_ranks argument. Some of the default argument of BertSumExtractiveSummarizer is in bertsum_config file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extractive_text_summerization import BertSumExtractiveSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 2\n",
    "gpu_ranks = str(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_path = './models/'\n",
    "log_base_path = './logs/'\n",
    "encoder = 'baseline'\n",
    "from random import random\n",
    "random_number = random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2']\n",
      "{2: 0}\n"
     ]
    }
   ],
   "source": [
    "bertsum_model = BertSumExtractiveSummarizer(encoder = 'baseline', \n",
    "                                            model_path = model_base_path+encoder+str(random_number),\n",
    "                                            log_file = log_base_path+encoder+str(random_number),\n",
    "                                            device_id = device_id,\n",
    "                                            gpu_ranks = gpu_ranks,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the fully processed CNN/DM dataset to train the model. During the training, you can stop any time and retrain from the previous saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-03 05:08:36,132 INFO] Device ID 2\n",
      "[2019-10-03 05:08:36,136 INFO] loading archive file /dadendev/textsum/temp/bert-base-uncased\n",
      "[2019-10-03 05:08:36,137 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_nsents': 3, 'max_nsents': 100, 'max_src_ntokens': 200, 'min_src_ntokens': 10, 'oracle_mode': 'combination', 'temp_dir': './temp', 'param_init': 0.0, 'param_init_glorot': True, 'dropout': 0.1, 'optim': 'adam', 'lr': 0.002, 'beta1': 0.9, 'beta2': 0.999, 'decay_method': 'noam', 'max_grad_norm': 0, 'use_interval': True, 'accum_count': 2, 'report_every': 50, 'save_checkpoint_steps': 500, 'batch_size': 3000, 'warmup_steps': 10000, 'block_trigram': True, 'recall_eval': False, 'report_rouge': True, 'encoder': 'baseline', 'hidden_size': 128, 'ff_size': 512, 'heads': 4, 'inter_layers': 2, 'rnn_size': 512, 'world_size': 1, 'visible_gpus': '0', 'gpu_ranks': '2', 'seed': 42, 'test_all': False, 'train_from': '', 'test_from': '', 'mode': 'train', 'model_path': './models/baseline0.7355433644584792', 'log_file': './logs/baseline0.7355433644584792', 'bert_config_path': './bert_config_uncased_base.json', 'worls_size': 1, 'gpu_ranks_map': {2: 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-03 05:08:38,152 INFO] Summarizer(\n",
      "  (bert): Bert(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 128)\n",
      "        (token_type_embeddings): Embedding(2, 128)\n",
      "        (LayerNorm): BertLayerNorm()\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (LayerNorm): BertLayerNorm()\n",
      "                (dropout): Dropout(p=0.1)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Classifier(\n",
      "    (linear1): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "[2019-10-03 05:08:38,157 INFO] * number of parameters: 5179137\n",
      "[2019-10-03 05:08:38,158 INFO] Start training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 2\n",
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-03 05:09:21,414 INFO] Step 50/50000; xent: 4.22; lr: 0.0000001; 243 docs/s;      4 sec\n",
      "[2019-10-03 05:09:25,293 INFO] Step 100/50000; xent: 4.15; lr: 0.0000002; 257 docs/s;      8 sec\n",
      "[2019-10-03 05:09:29,123 INFO] Step 150/50000; xent: 4.15; lr: 0.0000003; 262 docs/s;     12 sec\n",
      "[2019-10-03 05:09:32,985 INFO] Step 200/50000; xent: 3.98; lr: 0.0000004; 262 docs/s;     16 sec\n",
      "[2019-10-03 05:09:36,827 INFO] Step 250/50000; xent: 3.93; lr: 0.0000005; 261 docs/s;     20 sec\n",
      "[2019-10-03 05:09:40,678 INFO] Step 300/50000; xent: 3.90; lr: 0.0000006; 260 docs/s;     23 sec\n",
      "[2019-10-03 05:09:44,546 INFO] Step 350/50000; xent: 3.86; lr: 0.0000007; 260 docs/s;     27 sec\n",
      "[2019-10-03 05:09:48,368 INFO] Step 400/50000; xent: 3.78; lr: 0.0000008; 264 docs/s;     31 sec\n",
      "[2019-10-03 05:09:52,208 INFO] Step 450/50000; xent: 3.81; lr: 0.0000009; 263 docs/s;     35 sec\n",
      "[2019-10-03 05:09:56,065 INFO] Step 500/50000; xent: 3.78; lr: 0.0000010; 260 docs/s;     39 sec\n",
      "[2019-10-03 05:09:56,068 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_500.pt\n",
      "[2019-10-03 05:09:59,995 INFO] Step 550/50000; xent: 3.75; lr: 0.0000011; 257 docs/s;     43 sec\n",
      "[2019-10-03 05:10:03,842 INFO] Step 600/50000; xent: 3.86; lr: 0.0000012; 262 docs/s;     47 sec\n",
      "[2019-10-03 05:10:07,709 INFO] Step 650/50000; xent: 3.90; lr: 0.0000013; 260 docs/s;     50 sec\n",
      "[2019-10-03 05:10:11,557 INFO] Step 700/50000; xent: 3.85; lr: 0.0000014; 258 docs/s;     54 sec\n",
      "[2019-10-03 05:10:15,402 INFO] Step 750/50000; xent: 3.80; lr: 0.0000015; 261 docs/s;     58 sec\n",
      "[2019-10-03 05:10:19,262 INFO] Step 800/50000; xent: 3.67; lr: 0.0000016; 258 docs/s;     62 sec\n",
      "[2019-10-03 05:10:23,152 INFO] Step 850/50000; xent: 3.80; lr: 0.0000017; 258 docs/s;     66 sec\n",
      "[2019-10-03 05:10:27,028 INFO] Step 900/50000; xent: 3.62; lr: 0.0000018; 259 docs/s;     70 sec\n",
      "[2019-10-03 05:10:30,952 INFO] Step 950/50000; xent: 3.60; lr: 0.0000019; 256 docs/s;     74 sec\n",
      "[2019-10-03 05:10:34,807 INFO] Step 1000/50000; xent: 3.72; lr: 0.0000020; 259 docs/s;     78 sec\n",
      "[2019-10-03 05:10:34,810 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_1000.pt\n",
      "[2019-10-03 05:10:38,752 INFO] Step 1050/50000; xent: 3.71; lr: 0.0000021; 256 docs/s;     81 sec\n",
      "[2019-10-03 05:10:42,601 INFO] Step 1100/50000; xent: 3.75; lr: 0.0000022; 259 docs/s;     85 sec\n",
      "[2019-10-03 05:10:46,454 INFO] Step 1150/50000; xent: 3.62; lr: 0.0000023; 264 docs/s;     89 sec\n",
      "[2019-10-03 05:10:50,300 INFO] Step 1200/50000; xent: 3.65; lr: 0.0000024; 262 docs/s;     93 sec\n",
      "[2019-10-03 05:10:54,154 INFO] Step 1250/50000; xent: 3.64; lr: 0.0000025; 258 docs/s;     97 sec\n",
      "[2019-10-03 05:10:58,037 INFO] Step 1300/50000; xent: 3.57; lr: 0.0000026; 257 docs/s;    101 sec\n",
      "[2019-10-03 05:11:01,903 INFO] Step 1350/50000; xent: 3.69; lr: 0.0000027; 261 docs/s;    105 sec\n",
      "[2019-10-03 05:11:05,751 INFO] Step 1400/50000; xent: 3.71; lr: 0.0000028; 257 docs/s;    108 sec\n",
      "[2019-10-03 05:11:09,598 INFO] Step 1450/50000; xent: 3.74; lr: 0.0000029; 260 docs/s;    112 sec\n",
      "[2019-10-03 05:11:13,456 INFO] Step 1500/50000; xent: 3.68; lr: 0.0000030; 260 docs/s;    116 sec\n",
      "[2019-10-03 05:11:13,459 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_1500.pt\n",
      "[2019-10-03 05:11:17,396 INFO] Step 1550/50000; xent: 3.65; lr: 0.0000031; 255 docs/s;    120 sec\n",
      "[2019-10-03 05:11:21,246 INFO] Step 1600/50000; xent: 3.70; lr: 0.0000032; 262 docs/s;    124 sec\n",
      "[2019-10-03 05:11:25,090 INFO] Step 1650/50000; xent: 3.57; lr: 0.0000033; 262 docs/s;    128 sec\n",
      "[2019-10-03 05:11:28,940 INFO] Step 1700/50000; xent: 3.57; lr: 0.0000034; 261 docs/s;    132 sec\n",
      "[2019-10-03 05:11:32,805 INFO] Step 1750/50000; xent: 3.59; lr: 0.0000035; 258 docs/s;    136 sec\n",
      "[2019-10-03 05:11:36,677 INFO] Step 1800/50000; xent: 3.61; lr: 0.0000036; 261 docs/s;    139 sec\n",
      "[2019-10-03 05:11:40,549 INFO] Step 1850/50000; xent: 3.57; lr: 0.0000037; 260 docs/s;    143 sec\n",
      "[2019-10-03 05:11:44,450 INFO] Step 1900/50000; xent: 3.58; lr: 0.0000038; 260 docs/s;    147 sec\n",
      "[2019-10-03 05:11:48,317 INFO] Step 1950/50000; xent: 3.59; lr: 0.0000039; 259 docs/s;    151 sec\n",
      "[2019-10-03 05:11:52,169 INFO] Step 2000/50000; xent: 3.56; lr: 0.0000040; 262 docs/s;    155 sec\n",
      "[2019-10-03 05:11:52,172 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_2000.pt\n",
      "[2019-10-03 05:11:56,126 INFO] Step 2050/50000; xent: 3.50; lr: 0.0000041; 251 docs/s;    159 sec\n",
      "[2019-10-03 05:11:59,990 INFO] Step 2100/50000; xent: 3.52; lr: 0.0000042; 262 docs/s;    163 sec\n",
      "[2019-10-03 05:12:03,845 INFO] Step 2150/50000; xent: 3.55; lr: 0.0000043; 262 docs/s;    167 sec\n",
      "[2019-10-03 05:12:07,690 INFO] Step 2200/50000; xent: 3.51; lr: 0.0000044; 262 docs/s;    170 sec\n",
      "[2019-10-03 05:12:11,589 INFO] Step 2250/50000; xent: 3.57; lr: 0.0000045; 256 docs/s;    174 sec\n",
      "[2019-10-03 05:12:15,474 INFO] Step 2300/50000; xent: 3.53; lr: 0.0000046; 257 docs/s;    178 sec\n",
      "[2019-10-03 05:12:19,331 INFO] Step 2350/50000; xent: 3.58; lr: 0.0000047; 261 docs/s;    182 sec\n",
      "[2019-10-03 05:12:23,193 INFO] Step 2400/50000; xent: 3.47; lr: 0.0000048; 260 docs/s;    186 sec\n",
      "[2019-10-03 05:12:27,052 INFO] Step 2450/50000; xent: 3.46; lr: 0.0000049; 261 docs/s;    190 sec\n",
      "[2019-10-03 05:12:30,912 INFO] Step 2500/50000; xent: 3.60; lr: 0.0000050; 256 docs/s;    194 sec\n",
      "[2019-10-03 05:12:30,915 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_2500.pt\n",
      "[2019-10-03 05:12:34,855 INFO] Step 2550/50000; xent: 3.50; lr: 0.0000051; 251 docs/s;    198 sec\n",
      "[2019-10-03 05:12:38,719 INFO] Step 2600/50000; xent: 3.57; lr: 0.0000052; 259 docs/s;    201 sec\n",
      "[2019-10-03 05:12:42,576 INFO] Step 2650/50000; xent: 3.52; lr: 0.0000053; 259 docs/s;    205 sec\n",
      "[2019-10-03 05:12:46,440 INFO] Step 2700/50000; xent: 3.49; lr: 0.0000054; 258 docs/s;    209 sec\n",
      "[2019-10-03 05:12:50,326 INFO] Step 2750/50000; xent: 3.42; lr: 0.0000055; 260 docs/s;    213 sec\n",
      "[2019-10-03 05:12:54,214 INFO] Step 2800/50000; xent: 3.57; lr: 0.0000056; 256 docs/s;    217 sec\n",
      "[2019-10-03 05:12:58,096 INFO] Step 2850/50000; xent: 3.43; lr: 0.0000057; 260 docs/s;    221 sec\n",
      "[2019-10-03 05:13:01,975 INFO] Step 2900/50000; xent: 3.43; lr: 0.0000058; 259 docs/s;    225 sec\n",
      "[2019-10-03 05:13:05,873 INFO] Step 2950/50000; xent: 3.43; lr: 0.0000059; 256 docs/s;    229 sec\n",
      "[2019-10-03 05:13:09,775 INFO] Step 3000/50000; xent: 3.42; lr: 0.0000060; 258 docs/s;    233 sec\n",
      "[2019-10-03 05:13:09,777 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_3000.pt\n",
      "[2019-10-03 05:13:13,749 INFO] Step 3050/50000; xent: 3.42; lr: 0.0000061; 254 docs/s;    236 sec\n",
      "[2019-10-03 05:13:17,606 INFO] Step 3100/50000; xent: 3.44; lr: 0.0000062; 260 docs/s;    240 sec\n",
      "[2019-10-03 05:13:21,446 INFO] Step 3150/50000; xent: 3.48; lr: 0.0000063; 258 docs/s;    244 sec\n",
      "[2019-10-03 05:13:25,366 INFO] Step 3200/50000; xent: 3.39; lr: 0.0000064; 256 docs/s;    248 sec\n",
      "[2019-10-03 05:13:29,251 INFO] Step 3250/50000; xent: 3.38; lr: 0.0000065; 261 docs/s;    252 sec\n",
      "[2019-10-03 05:13:33,113 INFO] Step 3300/50000; xent: 3.53; lr: 0.0000066; 260 docs/s;    256 sec\n",
      "[2019-10-03 05:13:36,967 INFO] Step 3350/50000; xent: 3.45; lr: 0.0000067; 259 docs/s;    260 sec\n",
      "[2019-10-03 05:13:40,839 INFO] Step 3400/50000; xent: 3.49; lr: 0.0000068; 260 docs/s;    264 sec\n",
      "[2019-10-03 05:13:44,729 INFO] Step 3450/50000; xent: 3.42; lr: 0.0000069; 258 docs/s;    267 sec\n",
      "[2019-10-03 05:13:48,603 INFO] Step 3500/50000; xent: 3.43; lr: 0.0000070; 260 docs/s;    271 sec\n",
      "[2019-10-03 05:13:48,605 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_3500.pt\n",
      "[2019-10-03 05:13:52,550 INFO] Step 3550/50000; xent: 3.38; lr: 0.0000071; 252 docs/s;    275 sec\n",
      "[2019-10-03 05:13:56,415 INFO] Step 3600/50000; xent: 3.39; lr: 0.0000072; 258 docs/s;    279 sec\n",
      "[2019-10-03 05:14:00,290 INFO] Step 3650/50000; xent: 3.34; lr: 0.0000073; 256 docs/s;    283 sec\n",
      "[2019-10-03 05:14:04,183 INFO] Step 3700/50000; xent: 3.44; lr: 0.0000074; 261 docs/s;    287 sec\n",
      "[2019-10-03 05:14:08,061 INFO] Step 3750/50000; xent: 3.47; lr: 0.0000075; 260 docs/s;    291 sec\n",
      "[2019-10-03 05:14:11,937 INFO] Step 3800/50000; xent: 3.43; lr: 0.0000076; 260 docs/s;    295 sec\n",
      "[2019-10-03 05:14:15,802 INFO] Step 3850/50000; xent: 3.42; lr: 0.0000077; 258 docs/s;    299 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-03 05:14:19,662 INFO] Step 3900/50000; xent: 3.35; lr: 0.0000078; 258 docs/s;    302 sec\n",
      "[2019-10-03 05:14:23,526 INFO] Step 3950/50000; xent: 3.45; lr: 0.0000079; 261 docs/s;    306 sec\n",
      "[2019-10-03 05:14:27,364 INFO] Step 4000/50000; xent: 3.43; lr: 0.0000080; 260 docs/s;    310 sec\n",
      "[2019-10-03 05:14:27,367 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_4000.pt\n",
      "[2019-10-03 05:14:31,280 INFO] Step 4050/50000; xent: 3.49; lr: 0.0000081; 262 docs/s;    314 sec\n",
      "[2019-10-03 05:14:35,131 INFO] Step 4100/50000; xent: 3.39; lr: 0.0000082; 260 docs/s;    318 sec\n",
      "[2019-10-03 05:14:39,008 INFO] Step 4150/50000; xent: 3.45; lr: 0.0000083; 260 docs/s;    322 sec\n",
      "[2019-10-03 05:14:42,872 INFO] Step 4200/50000; xent: 3.45; lr: 0.0000084; 260 docs/s;    326 sec\n",
      "[2019-10-03 05:14:46,737 INFO] Step 4250/50000; xent: 3.35; lr: 0.0000085; 261 docs/s;    329 sec\n",
      "[2019-10-03 05:14:50,588 INFO] Step 4300/50000; xent: 3.44; lr: 0.0000086; 258 docs/s;    333 sec\n",
      "[2019-10-03 05:14:54,445 INFO] Step 4350/50000; xent: 3.45; lr: 0.0000087; 257 docs/s;    337 sec\n",
      "[2019-10-03 05:14:58,294 INFO] Step 4400/50000; xent: 3.49; lr: 0.0000088; 262 docs/s;    341 sec\n",
      "[2019-10-03 05:15:02,154 INFO] Step 4450/50000; xent: 3.34; lr: 0.0000089; 260 docs/s;    345 sec\n",
      "[2019-10-03 05:15:06,021 INFO] Step 4500/50000; xent: 3.36; lr: 0.0000090; 259 docs/s;    349 sec\n",
      "[2019-10-03 05:15:06,024 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_4500.pt\n",
      "[2019-10-03 05:15:09,950 INFO] Step 4550/50000; xent: 3.39; lr: 0.0000091; 256 docs/s;    353 sec\n",
      "[2019-10-03 05:15:13,855 INFO] Step 4600/50000; xent: 3.31; lr: 0.0000092; 258 docs/s;    357 sec\n",
      "[2019-10-03 05:15:17,737 INFO] Step 4650/50000; xent: 3.30; lr: 0.0000093; 256 docs/s;    360 sec\n",
      "[2019-10-03 05:15:21,638 INFO] Step 4700/50000; xent: 3.34; lr: 0.0000094; 259 docs/s;    364 sec\n",
      "[2019-10-03 05:15:25,546 INFO] Step 4750/50000; xent: 3.39; lr: 0.0000095; 258 docs/s;    368 sec\n",
      "[2019-10-03 05:15:29,404 INFO] Step 4800/50000; xent: 3.30; lr: 0.0000096; 259 docs/s;    372 sec\n",
      "[2019-10-03 05:15:33,268 INFO] Step 4850/50000; xent: 3.32; lr: 0.0000097; 261 docs/s;    376 sec\n",
      "[2019-10-03 05:15:37,129 INFO] Step 4900/50000; xent: 3.31; lr: 0.0000098; 264 docs/s;    380 sec\n",
      "[2019-10-03 05:15:40,978 INFO] Step 4950/50000; xent: 3.35; lr: 0.0000099; 260 docs/s;    384 sec\n",
      "[2019-10-03 05:15:44,885 INFO] Step 5000/50000; xent: 3.36; lr: 0.0000100; 258 docs/s;    388 sec\n",
      "[2019-10-03 05:15:44,887 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_5000.pt\n",
      "[2019-10-03 05:15:48,912 INFO] Step 5050/50000; xent: 3.29; lr: 0.0000101; 249 docs/s;    392 sec\n",
      "[2019-10-03 05:15:52,810 INFO] Step 5100/50000; xent: 3.34; lr: 0.0000102; 255 docs/s;    396 sec\n",
      "[2019-10-03 05:15:56,672 INFO] Step 5150/50000; xent: 3.45; lr: 0.0000103; 258 docs/s;    399 sec\n",
      "[2019-10-03 05:16:00,539 INFO] Step 5200/50000; xent: 3.39; lr: 0.0000104; 257 docs/s;    403 sec\n",
      "[2019-10-03 05:16:04,422 INFO] Step 5250/50000; xent: 3.47; lr: 0.0000105; 259 docs/s;    407 sec\n",
      "[2019-10-03 05:16:08,289 INFO] Step 5300/50000; xent: 3.36; lr: 0.0000106; 261 docs/s;    411 sec\n",
      "[2019-10-03 05:16:12,141 INFO] Step 5350/50000; xent: 3.40; lr: 0.0000107; 257 docs/s;    415 sec\n",
      "[2019-10-03 05:16:16,023 INFO] Step 5400/50000; xent: 3.39; lr: 0.0000108; 259 docs/s;    419 sec\n",
      "[2019-10-03 05:16:19,909 INFO] Step 5450/50000; xent: 3.30; lr: 0.0000109; 256 docs/s;    423 sec\n",
      "[2019-10-03 05:16:23,776 INFO] Step 5500/50000; xent: 3.33; lr: 0.0000110; 258 docs/s;    427 sec\n",
      "[2019-10-03 05:16:23,779 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_5500.pt\n",
      "[2019-10-03 05:16:27,722 INFO] Step 5550/50000; xent: 3.31; lr: 0.0000111; 256 docs/s;    430 sec\n",
      "[2019-10-03 05:16:31,617 INFO] Step 5600/50000; xent: 3.36; lr: 0.0000112; 258 docs/s;    434 sec\n",
      "[2019-10-03 05:16:35,514 INFO] Step 5650/50000; xent: 3.32; lr: 0.0000113; 256 docs/s;    438 sec\n",
      "[2019-10-03 05:16:39,399 INFO] Step 5700/50000; xent: 3.29; lr: 0.0000114; 257 docs/s;    442 sec\n",
      "[2019-10-03 05:16:43,273 INFO] Step 5750/50000; xent: 3.26; lr: 0.0000115; 261 docs/s;    446 sec\n",
      "[2019-10-03 05:16:47,145 INFO] Step 5800/50000; xent: 3.37; lr: 0.0000116; 258 docs/s;    450 sec\n",
      "[2019-10-03 05:16:51,033 INFO] Step 5850/50000; xent: 3.40; lr: 0.0000117; 259 docs/s;    454 sec\n",
      "[2019-10-03 05:16:54,947 INFO] Step 5900/50000; xent: 3.32; lr: 0.0000118; 258 docs/s;    458 sec\n",
      "[2019-10-03 05:16:58,814 INFO] Step 5950/50000; xent: 3.42; lr: 0.0000119; 260 docs/s;    462 sec\n",
      "[2019-10-03 05:17:02,726 INFO] Step 6000/50000; xent: 3.32; lr: 0.0000120; 259 docs/s;    465 sec\n",
      "[2019-10-03 05:17:02,729 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_6000.pt\n",
      "[2019-10-03 05:17:06,702 INFO] Step 6050/50000; xent: 3.42; lr: 0.0000121; 252 docs/s;    469 sec\n",
      "[2019-10-03 05:17:10,583 INFO] Step 6100/50000; xent: 3.33; lr: 0.0000122; 260 docs/s;    473 sec\n",
      "[2019-10-03 05:17:14,469 INFO] Step 6150/50000; xent: 3.32; lr: 0.0000123; 261 docs/s;    477 sec\n",
      "[2019-10-03 05:17:18,344 INFO] Step 6200/50000; xent: 3.33; lr: 0.0000124; 258 docs/s;    481 sec\n",
      "[2019-10-03 05:17:22,228 INFO] Step 6250/50000; xent: 3.35; lr: 0.0000125; 260 docs/s;    485 sec\n",
      "[2019-10-03 05:17:26,101 INFO] Step 6300/50000; xent: 3.34; lr: 0.0000126; 257 docs/s;    489 sec\n",
      "[2019-10-03 05:17:29,983 INFO] Step 6350/50000; xent: 3.40; lr: 0.0000127; 260 docs/s;    493 sec\n",
      "[2019-10-03 05:17:33,847 INFO] Step 6400/50000; xent: 3.29; lr: 0.0000128; 264 docs/s;    497 sec\n",
      "[2019-10-03 05:17:37,722 INFO] Step 6450/50000; xent: 3.40; lr: 0.0000129; 258 docs/s;    500 sec\n",
      "[2019-10-03 05:17:41,610 INFO] Step 6500/50000; xent: 3.35; lr: 0.0000130; 256 docs/s;    504 sec\n",
      "[2019-10-03 05:17:41,613 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_6500.pt\n",
      "[2019-10-03 05:17:45,593 INFO] Step 6550/50000; xent: 3.32; lr: 0.0000131; 253 docs/s;    508 sec\n",
      "[2019-10-03 05:17:49,495 INFO] Step 6600/50000; xent: 3.37; lr: 0.0000132; 257 docs/s;    512 sec\n",
      "[2019-10-03 05:17:53,370 INFO] Step 6650/50000; xent: 3.26; lr: 0.0000133; 262 docs/s;    516 sec\n",
      "[2019-10-03 05:17:57,240 INFO] Step 6700/50000; xent: 3.34; lr: 0.0000134; 259 docs/s;    520 sec\n",
      "[2019-10-03 05:18:01,123 INFO] Step 6750/50000; xent: 3.29; lr: 0.0000135; 260 docs/s;    524 sec\n",
      "[2019-10-03 05:18:05,027 INFO] Step 6800/50000; xent: 3.34; lr: 0.0000136; 258 docs/s;    528 sec\n",
      "[2019-10-03 05:18:08,920 INFO] Step 6850/50000; xent: 3.30; lr: 0.0000137; 256 docs/s;    532 sec\n",
      "[2019-10-03 05:18:12,803 INFO] Step 6900/50000; xent: 3.37; lr: 0.0000138; 259 docs/s;    536 sec\n",
      "[2019-10-03 05:18:16,717 INFO] Step 6950/50000; xent: 3.27; lr: 0.0000139; 258 docs/s;    539 sec\n",
      "[2019-10-03 05:18:20,621 INFO] Step 7000/50000; xent: 3.27; lr: 0.0000140; 258 docs/s;    543 sec\n",
      "[2019-10-03 05:18:20,624 INFO] Saving checkpoint ./models/baseline0.7355433644584792/model_step_7000.pt\n",
      "[2019-10-03 05:18:24,591 INFO] Step 7050/50000; xent: 3.32; lr: 0.0000141; 253 docs/s;    547 sec\n",
      "[2019-10-03 05:18:28,486 INFO] Step 7100/50000; xent: 3.18; lr: 0.0000142; 259 docs/s;    551 sec\n"
     ]
    }
   ],
   "source": [
    "training_data_file = './bert_train_data_all_none_excluded'\n",
    "bertsum_model.fit(device_id, [training_data_file], train_steps=50000, train_from=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)), or Recall-Oriented Understudy for Gisting Evaluation has been commonly used for evaluation text summerization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.data_loader  import DataIterator,Batch,Dataloader\n",
    "import os\n",
    "test_dataset=torch.load(\"./harvardnlp_cnndm/test.bertdata\")\n",
    "from bertsum_config import Bunch\n",
    "\n",
    "import os\n",
    "dataset=[]\n",
    "for i in range(0,6):\n",
    "    filename = \"cnndm.test.{0}.bert.pt\".format(i)\n",
    "    dataset.extend(torch.load(os.path.join(\"./\"+\"bert_data/\", filename)))\n",
    "    \n",
    "def get_data_iter(dataset,batch_size=300):\n",
    "    args = Bunch({})\n",
    "    args.use_interval = True\n",
    "    args.batch_size = batch_size\n",
    "    test_data_iter = None\n",
    "    test_data_iter  = DataIterator(args, dataset, args.batch_size, 'cuda', is_test=True, shuffle=False, sort=False)\n",
    "    return test_data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-03 18:10:19,387 INFO] Device ID 2\n",
      "[2019-10-03 18:10:19,389 INFO] Loading checkpoint from ./models/baseline0.7355433644584792/model_step_50000.pt\n",
      "[2019-10-03 18:10:20,964 INFO] * number of parameters: 5179137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 2\n",
      "gpu_rank 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-f92dbfd5d82a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tgt_txt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m prediction = bertsum_model.predict(device_id, get_data_iter(test_dataset),\n\u001b[0;32m----> 4\u001b[0;31m                                    test_from=model_for_test)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_rouge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#rouge_baseline = get_rouge(prediction, target, \"/dadendev/textsum/results/rougetemp\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/textsum//wrapper/extractive_text_summerization.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, device_id, data_iter, test_from, cal_lead)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_seperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_lead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/BertSum/src/models/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_iter, cal_lead, cal_oracle)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/BertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations_this_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/BertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, device, is_test)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mpre_clss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/BertSum/src/models/data_loader.py\u001b[0m in \u001b[0;36m_pad\u001b[0;34m(self, data, pad_id, width)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mrtn_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrtn_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "model_for_test = \"./models/baseline0.7355433644584792/model_step_50000.pt\"\n",
    "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter(test_dataset),\n",
    "                                   test_from=model_for_test)\n",
    "from utils import get_rouge\n",
    "#rouge_baseline = get_rouge(prediction, target, \"/dadendev/textsum/results/rougetemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11486"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-03 17:46:35,071 INFO] Device ID 2\n",
      "[2019-10-03 17:46:35,082 INFO] Loading checkpoint from ./models/rnn/model_step_50000.pt\n",
      "[2019-10-03 17:46:37,559 INFO] * number of parameters: 113041921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 2\n",
      "gpu_rank 0\n",
      "11486\n",
      "11486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-03 17:48:59,222 [MainThread  ] [INFO ]  Writing summaries.\n",
      "[2019-10-03 17:48:59,222 INFO] Writing summaries.\n",
      "2019-10-03 17:48:59,224 [MainThread  ] [INFO ]  Processing summaries. Saving system files to /dadendev/textsum/results/rougetemp/tmp9w889acx/system and model files to /dadendev/textsum/results/rougetemp/tmp9w889acx/model.\n",
      "[2019-10-03 17:48:59,224 INFO] Processing summaries. Saving system files to /dadendev/textsum/results/rougetemp/tmp9w889acx/system and model files to /dadendev/textsum/results/rougetemp/tmp9w889acx/model.\n",
      "2019-10-03 17:48:59,225 [MainThread  ] [INFO ]  Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-17-48-58/candidate/.\n",
      "[2019-10-03 17:48:59,225 INFO] Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-17-48-58/candidate/.\n",
      "2019-10-03 17:49:00,406 [MainThread  ] [INFO ]  Saved processed files to /dadendev/textsum/results/rougetemp/tmp9w889acx/system.\n",
      "[2019-10-03 17:49:00,406 INFO] Saved processed files to /dadendev/textsum/results/rougetemp/tmp9w889acx/system.\n",
      "2019-10-03 17:49:00,408 [MainThread  ] [INFO ]  Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-17-48-58/reference/.\n",
      "[2019-10-03 17:49:00,408 INFO] Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-17-48-58/reference/.\n",
      "2019-10-03 17:49:01,539 [MainThread  ] [INFO ]  Saved processed files to /dadendev/textsum/results/rougetemp/tmp9w889acx/model.\n",
      "[2019-10-03 17:49:01,539 INFO] Saved processed files to /dadendev/textsum/results/rougetemp/tmp9w889acx/model.\n",
      "2019-10-03 17:49:01,631 [MainThread  ] [INFO ]  Written ROUGE configuration to /dadendev/textsum/results/rougetemp/tmp74wedt83/rouge_conf.xml\n",
      "[2019-10-03 17:49:01,631 INFO] Written ROUGE configuration to /dadendev/textsum/results/rougetemp/tmp74wedt83/rouge_conf.xml\n",
      "2019-10-03 17:49:01,632 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a /dadendev/textsum/results/rougetemp/tmp74wedt83/rouge_conf.xml\n",
      "[2019-10-03 17:49:01,632 INFO] Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a /dadendev/textsum/results/rougetemp/tmp74wedt83/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.53540 (95%-conf.int. 0.53270 - 0.53815)\n",
      "1 ROUGE-1 Average_P: 0.37945 (95%-conf.int. 0.37709 - 0.38191)\n",
      "1 ROUGE-1 Average_F: 0.42948 (95%-conf.int. 0.42737 - 0.43167)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.24836 (95%-conf.int. 0.24555 - 0.25117)\n",
      "1 ROUGE-2 Average_P: 0.17692 (95%-conf.int. 0.17482 - 0.17919)\n",
      "1 ROUGE-2 Average_F: 0.19954 (95%-conf.int. 0.19734 - 0.20177)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.34366 (95%-conf.int. 0.34118 - 0.34616)\n",
      "1 ROUGE-L Average_P: 0.24172 (95%-conf.int. 0.23978 - 0.24381)\n",
      "1 ROUGE-L Average_F: 0.27432 (95%-conf.int. 0.27234 - 0.27626)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_for_test = \"./models/rnn/model_step_50000.pt\"\n",
    "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter(test_dataset),\n",
    "                                   test_from=model_for_test)\n",
    "from utils import get_rouge\n",
    "rouge_rnn = get_rouge(prediction, target, \"/dadendev/textsum/results/rougetemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-03 17:50:56,970 INFO] Device ID 2\n",
      "[2019-10-03 17:50:56,973 INFO] Loading checkpoint from ./models/transformer/model_step_50000.pt\n",
      "[2019-10-03 17:50:59,066 INFO] * number of parameters: 115790849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 2\n",
      "gpu_rank 0\n",
      "11486\n",
      "11486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-03 17:53:15,587 [MainThread  ] [INFO ]  Writing summaries.\n",
      "[2019-10-03 17:53:15,587 INFO] Writing summaries.\n",
      "2019-10-03 17:53:15,590 [MainThread  ] [INFO ]  Processing summaries. Saving system files to /dadendev/textsum/results/rougetemp/tmp6gynufns/system and model files to /dadendev/textsum/results/rougetemp/tmp6gynufns/model.\n",
      "[2019-10-03 17:53:15,590 INFO] Processing summaries. Saving system files to /dadendev/textsum/results/rougetemp/tmp6gynufns/system and model files to /dadendev/textsum/results/rougetemp/tmp6gynufns/model.\n",
      "2019-10-03 17:53:15,591 [MainThread  ] [INFO ]  Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-17-53-14/candidate/.\n",
      "[2019-10-03 17:53:15,591 INFO] Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-17-53-14/candidate/.\n",
      "2019-10-03 17:53:16,773 [MainThread  ] [INFO ]  Saved processed files to /dadendev/textsum/results/rougetemp/tmp6gynufns/system.\n",
      "[2019-10-03 17:53:16,773 INFO] Saved processed files to /dadendev/textsum/results/rougetemp/tmp6gynufns/system.\n",
      "2019-10-03 17:53:16,774 [MainThread  ] [INFO ]  Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-17-53-14/reference/.\n",
      "[2019-10-03 17:53:16,774 INFO] Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-17-53-14/reference/.\n",
      "2019-10-03 17:53:17,921 [MainThread  ] [INFO ]  Saved processed files to /dadendev/textsum/results/rougetemp/tmp6gynufns/model.\n",
      "[2019-10-03 17:53:17,921 INFO] Saved processed files to /dadendev/textsum/results/rougetemp/tmp6gynufns/model.\n",
      "2019-10-03 17:53:18,008 [MainThread  ] [INFO ]  Written ROUGE configuration to /dadendev/textsum/results/rougetemp/tmpoytr7s3w/rouge_conf.xml\n",
      "[2019-10-03 17:53:18,008 INFO] Written ROUGE configuration to /dadendev/textsum/results/rougetemp/tmpoytr7s3w/rouge_conf.xml\n",
      "2019-10-03 17:53:18,009 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a /dadendev/textsum/results/rougetemp/tmpoytr7s3w/rouge_conf.xml\n",
      "[2019-10-03 17:53:18,009 INFO] Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a /dadendev/textsum/results/rougetemp/tmpoytr7s3w/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.53732 (95%-conf.int. 0.53457 - 0.54013)\n",
      "1 ROUGE-1 Average_P: 0.37491 (95%-conf.int. 0.37254 - 0.37733)\n",
      "1 ROUGE-1 Average_F: 0.42705 (95%-conf.int. 0.42484 - 0.42920)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.24855 (95%-conf.int. 0.24582 - 0.25123)\n",
      "1 ROUGE-2 Average_P: 0.17405 (95%-conf.int. 0.17196 - 0.17624)\n",
      "1 ROUGE-2 Average_F: 0.19768 (95%-conf.int. 0.19553 - 0.19985)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.34365 (95%-conf.int. 0.34113 - 0.34615)\n",
      "1 ROUGE-L Average_P: 0.23772 (95%-conf.int. 0.23571 - 0.23975)\n",
      "1 ROUGE-L Average_F: 0.27163 (95%-conf.int. 0.26963 - 0.27360)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_for_test = \"./models/transformer/model_step_50000.pt\"\n",
    "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter(test_dataset),\n",
    "                                   test_from=model_for_test)\n",
    "from utils import get_rouge\n",
    "rouge_transformer = get_rouge(prediction, target, \"/dadendev/textsum/results/rougetemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-03 18:41:50,554 INFO] Device ID 2\n",
      "[2019-10-03 18:41:50,555 INFO] Loading checkpoint from ./models/transformer/model_step_50000.pt\n",
      "[2019-10-03 18:41:52,881 INFO] * number of parameters: 115790849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 2\n",
      "gpu_rank 0\n",
      "11489\n",
      "11489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-03 18:44:25,911 [MainThread  ] [INFO ]  Writing summaries.\n",
      "[2019-10-03 18:44:25,911 INFO] Writing summaries.\n",
      "2019-10-03 18:44:25,919 [MainThread  ] [INFO ]  Processing summaries. Saving system files to /dadendev/textsum/results/rougetemp/tmpjsn2arpn/system and model files to /dadendev/textsum/results/rougetemp/tmpjsn2arpn/model.\n",
      "[2019-10-03 18:44:25,919 INFO] Processing summaries. Saving system files to /dadendev/textsum/results/rougetemp/tmpjsn2arpn/system and model files to /dadendev/textsum/results/rougetemp/tmpjsn2arpn/model.\n",
      "2019-10-03 18:44:25,920 [MainThread  ] [INFO ]  Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-18-44-24/candidate/.\n",
      "[2019-10-03 18:44:25,920 INFO] Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-18-44-24/candidate/.\n",
      "2019-10-03 18:44:27,124 [MainThread  ] [INFO ]  Saved processed files to /dadendev/textsum/results/rougetemp/tmpjsn2arpn/system.\n",
      "[2019-10-03 18:44:27,124 INFO] Saved processed files to /dadendev/textsum/results/rougetemp/tmpjsn2arpn/system.\n",
      "2019-10-03 18:44:27,126 [MainThread  ] [INFO ]  Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-18-44-24/reference/.\n",
      "[2019-10-03 18:44:27,126 INFO] Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-18-44-24/reference/.\n",
      "2019-10-03 18:44:28,311 [MainThread  ] [INFO ]  Saved processed files to /dadendev/textsum/results/rougetemp/tmpjsn2arpn/model.\n",
      "[2019-10-03 18:44:28,311 INFO] Saved processed files to /dadendev/textsum/results/rougetemp/tmpjsn2arpn/model.\n",
      "2019-10-03 18:44:28,401 [MainThread  ] [INFO ]  Written ROUGE configuration to /dadendev/textsum/results/rougetemp/tmph9qs47ba/rouge_conf.xml\n",
      "[2019-10-03 18:44:28,401 INFO] Written ROUGE configuration to /dadendev/textsum/results/rougetemp/tmph9qs47ba/rouge_conf.xml\n",
      "2019-10-03 18:44:28,402 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a /dadendev/textsum/results/rougetemp/tmph9qs47ba/rouge_conf.xml\n",
      "[2019-10-03 18:44:28,402 INFO] Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a /dadendev/textsum/results/rougetemp/tmph9qs47ba/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.53487 (95%-conf.int. 0.53211 - 0.53764)\n",
      "1 ROUGE-1 Average_P: 0.38059 (95%-conf.int. 0.37813 - 0.38318)\n",
      "1 ROUGE-1 Average_F: 0.42995 (95%-conf.int. 0.42787 - 0.43226)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.24873 (95%-conf.int. 0.24604 - 0.25157)\n",
      "1 ROUGE-2 Average_P: 0.17760 (95%-conf.int. 0.17536 - 0.17989)\n",
      "1 ROUGE-2 Average_F: 0.20002 (95%-conf.int. 0.19784 - 0.20247)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.48956 (95%-conf.int. 0.48697 - 0.49221)\n",
      "1 ROUGE-L Average_P: 0.34903 (95%-conf.int. 0.34667 - 0.35158)\n",
      "1 ROUGE-L Average_F: 0.39396 (95%-conf.int. 0.39183 - 0.39629)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_for_test = \"./models/transformer/model_step_50000.pt\"\n",
    "target = [dataset[i]['tgt_txt'] for i in range(len(dataset))]\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter(dataset, 3000),sentence_seperator=\"<q>\",\n",
    "                                   test_from=model_for_test)\n",
    "from utils import get_rouge\n",
    "rouge_transformer = get_rouge(prediction, target, \"/dadendev/textsum/results/rougetemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-03 18:27:21,995 INFO] Device ID 2\n",
      "[2019-10-03 18:27:21,996 INFO] Loading checkpoint from ./models/transformer/model_step_50000.pt\n",
      "[2019-10-03 18:27:24,295 INFO] * number of parameters: 115790849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 2\n",
      "gpu_rank 0\n",
      "11489\n",
      "11489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-03 18:27:27,926 [MainThread  ] [INFO ]  Writing summaries.\n",
      "[2019-10-03 18:27:27,926 INFO] Writing summaries.\n",
      "2019-10-03 18:27:27,932 [MainThread  ] [INFO ]  Processing summaries. Saving system files to /dadendev/textsum/results/rougetemp/tmp8evwxrll/system and model files to /dadendev/textsum/results/rougetemp/tmp8evwxrll/model.\n",
      "[2019-10-03 18:27:27,932 INFO] Processing summaries. Saving system files to /dadendev/textsum/results/rougetemp/tmp8evwxrll/system and model files to /dadendev/textsum/results/rougetemp/tmp8evwxrll/model.\n",
      "2019-10-03 18:27:27,934 [MainThread  ] [INFO ]  Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-18-27-26/candidate/.\n",
      "[2019-10-03 18:27:27,934 INFO] Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-18-27-26/candidate/.\n",
      "2019-10-03 18:27:29,138 [MainThread  ] [INFO ]  Saved processed files to /dadendev/textsum/results/rougetemp/tmp8evwxrll/system.\n",
      "[2019-10-03 18:27:29,138 INFO] Saved processed files to /dadendev/textsum/results/rougetemp/tmp8evwxrll/system.\n",
      "2019-10-03 18:27:29,140 [MainThread  ] [INFO ]  Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-18-27-26/reference/.\n",
      "[2019-10-03 18:27:29,140 INFO] Processing files in /dadendev/textsum/results/rougetemp/rouge-tmp-2019-10-03-18-27-26/reference/.\n",
      "2019-10-03 18:27:30,346 [MainThread  ] [INFO ]  Saved processed files to /dadendev/textsum/results/rougetemp/tmp8evwxrll/model.\n",
      "[2019-10-03 18:27:30,346 INFO] Saved processed files to /dadendev/textsum/results/rougetemp/tmp8evwxrll/model.\n",
      "2019-10-03 18:27:30,438 [MainThread  ] [INFO ]  Written ROUGE configuration to /dadendev/textsum/results/rougetemp/tmph0uom0ij/rouge_conf.xml\n",
      "[2019-10-03 18:27:30,438 INFO] Written ROUGE configuration to /dadendev/textsum/results/rougetemp/tmph0uom0ij/rouge_conf.xml\n",
      "2019-10-03 18:27:30,440 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a /dadendev/textsum/results/rougetemp/tmph0uom0ij/rouge_conf.xml\n",
      "[2019-10-03 18:27:30,440 INFO] Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a /dadendev/textsum/results/rougetemp/tmph0uom0ij/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.52371 (95%-conf.int. 0.52067 - 0.52692)\n",
      "1 ROUGE-1 Average_P: 0.34716 (95%-conf.int. 0.34484 - 0.34937)\n",
      "1 ROUGE-1 Average_F: 0.40370 (95%-conf.int. 0.40154 - 0.40592)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.22740 (95%-conf.int. 0.22473 - 0.23047)\n",
      "1 ROUGE-2 Average_P: 0.14969 (95%-conf.int. 0.14781 - 0.15166)\n",
      "1 ROUGE-2 Average_F: 0.17444 (95%-conf.int. 0.17241 - 0.17662)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.47465 (95%-conf.int. 0.47167 - 0.47766)\n",
      "1 ROUGE-L Average_P: 0.31501 (95%-conf.int. 0.31282 - 0.31717)\n",
      "1 ROUGE-L Average_F: 0.36614 (95%-conf.int. 0.36399 - 0.36826)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_for_test = \"./models/transformer/model_step_50000.pt\"\n",
    "target = [dataset[i]['tgt_txt'] for i in range(len(dataset))]\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter(dataset, 3000),sentence_seperator=\"<q>\",\n",
    "                                   test_from=model_for_test, cal_lead=True)\n",
    "from utils import get_rouge\n",
    "rouge_transformer = get_rouge(prediction, target, \"/dadendev/textsum/results/rougetemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11486"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marseille , france -lrb- cnn -rrb- the french prosecutor leading an investigation into the crash of germanwings flight 9525 insisted wednesday that he was not aware of any video footage from on board the plane .',\n",
       " 'marseille prosecutor brice robin told cnn that `` so far no videos were used in the crash investigation . ``',\n",
       " 'he added , `` a person who has such a video needs to immediately give it to the investigators . ``',\n",
       " \"robin 's comments follow claims by two magazines , german daily bild and french paris match , of a cell phone video showing the harrowing final seconds from on board germanwings flight 9525 as it crashed into the french alps .\",\n",
       " 'paris match and bild reported that the video was recovered from a phone at the wreckage site .',\n",
       " 'the two publications described the supposed video , but did not post it on their websites .',\n",
       " 'the publications said that they watched the video , which was found by a source close to the investigation .',\n",
       " \"`` one can hear cries of ` my god ' in several languages , `` paris match reported .\",\n",
       " '`` metallic banging can also be heard more than three times , perhaps of the pilot trying to open the cockpit door with a heavy object .',\n",
       " 'towards the end , after a heavy shake , stronger than the others , the screaming intensifies .',\n",
       " '`` it is a very disturbing scene , `` said julian reichelt , editor-in-chief of bild online .',\n",
       " \"an official with france 's accident investigation agency , the bea , said the agency is not aware of any such video .\",\n",
       " 'lt. col. jean-marc menichini , a french gendarmerie spokesman in charge of communications on rescue efforts around the germanwings crash site , told cnn that the reports were `` completely wrong `` and `` unwarranted . ``',\n",
       " \"cell phones have been collected at the site , he said , but that they `` had n't been exploited yet . ``\",\n",
       " 'menichini said he believed the cell phones would need to be sent to the criminal research institute in rosny sous-bois , near paris , in order to be analyzed by specialized technicians working hand-in-hand with investigators .',\n",
       " 'but none of the cell phones found so far have been sent to the institute , menichini said .',\n",
       " 'asked whether staff involved in the search could have leaked a memory card to the media , menichini answered with a categorical `` no . ``',\n",
       " 'reichelt told `` erin burnett : outfront `` that he had watched the video and stood by the report , saying bild and paris match are `` very confident `` that the clip is real .',\n",
       " \"he noted that investigators only revealed they 'd recovered cell phones from the crash site after bild and paris match published their reports .\",\n",
       " \"... overall we can say many things of the investigation were n't revealed by the investigation at the beginning , `` he said .\",\n",
       " \"german airline lufthansa confirmed tuesday that co-pilot andreas lubitz had battled depression years before he took the controls of germanwings flight 9525 , which he 's accused of deliberately crashing last week in the french alps .\",\n",
       " 'lubitz told his lufthansa flight training school in 2009 that he had a `` previous episode of severe depression , `` the airline said tuesday .',\n",
       " 'email correspondence between lubitz and the school discovered in an internal investigation , lufthansa said , included medical documents he submitted in connection with resuming his flight training .',\n",
       " \"the announcement indicates that lufthansa , the parent company of germanwings , knew of lubitz 's battle with depression , allowed him to continue training and ultimately put him in the cockpit .\",\n",
       " 'lufthansa , whose ceo carsten spohr previously said lubitz was 100 % fit to fly , described its statement tuesday as a `` swift and seamless clarification `` and said it was sharing the information and documents -- including training and medical records -- with public prosecutors .',\n",
       " 'spohr traveled to the crash site wednesday , where recovery teams have been working for the past week to recover human remains and plane debris scattered across a steep mountainside .',\n",
       " 'he saw the crisis center set up in seyne-les-alpes , laid a wreath in the village of le vernet , closer to the crash site , where grieving families have left flowers at a simple stone memorial .',\n",
       " 'menichini told cnn late tuesday that no visible human remains were left at the site but recovery teams would keep searching .',\n",
       " 'french president francois hollande , speaking tuesday , said that it should be possible to identify all the victims using dna analysis by the end of the week , sooner than authorities had previously suggested .',\n",
       " \"in the meantime , the recovery of the victims ' personal belongings will start wednesday , menichini said .\",\n",
       " 'among those personal belongings could be more cell phones belonging to the 144 passengers and six crew on board .',\n",
       " \"the details about lubitz 's correspondence with the flight school during his training were among several developments as investigators continued to delve into what caused the crash and lubitz 's possible motive for downing the jet .\",\n",
       " 'a lufthansa spokesperson told cnn on tuesday that lubitz had a valid medical certificate , had passed all his examinations and `` held all the licenses required . ``',\n",
       " \"earlier , a spokesman for the prosecutor 's office in dusseldorf , christoph kumpa , said medical records reveal lubitz suffered from suicidal tendencies at some point before his aviation career and underwent psychotherapy before he got his pilot 's license .\",\n",
       " \"kumpa emphasized there 's no evidence suggesting lubitz was suicidal or acting aggressively before the crash .\",\n",
       " \"investigators are looking into whether lubitz feared his medical condition would cause him to lose his pilot 's license , a european government official briefed on the investigation told cnn on tuesday .\",\n",
       " \"while flying was `` a big part of his life , `` the source said , it 's only one theory being considered .\",\n",
       " 'another source , a law enforcement official briefed on the investigation , also told cnn that authorities believe the primary motive for lubitz to bring down the plane was that he feared he would not be allowed to fly because of his medical problems .',\n",
       " \"lubitz 's girlfriend told investigators he had seen an eye doctor and a neuropsychologist , both of whom deemed him unfit to work recently and concluded he had psychological issues , the european government official said .\",\n",
       " \"but no matter what details emerge about his previous mental health struggles , there 's more to the story , said brian russell , a forensic psychologist .\",\n",
       " \"`` psychology can explain why somebody would turn rage inward on themselves about the fact that maybe they were n't going to keep doing their job and they 're upset about that and so they 're suicidal , `` he said .\",\n",
       " \"`` but there is no mental illness that explains why somebody then feels entitled to also take that rage and turn it outward on 149 other people who had nothing to do with the person 's problems . ``\",\n",
       " \"cnn 's margot haddad reported from marseille and pamela brown from dusseldorf , while laura smith-spark wrote from london .\",\n",
       " \"cnn 's frederik pleitgen , pamela boykoff , antonia mortensen , sandrine amiel and anna-maja rappard contributed to this report .\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['src_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marseille prosecutor says `` so far no videos were used in the crash investigation `` despite media reports . journalists at bild and paris match are `` very confident `` the video clip is real , an editor says . andreas lubitz had informed his lufthansa training school of an episode of severe depression , airline says .'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marseille prosecutor brice robin told cnn that `` so far no videos were used in the crash investigation . ``paris match and bild reported that the video was recovered from a phone at the wreckage site .marseille , france -lrb- cnn -rrb- the french prosecutor leading an investigation into the crash of germanwings flight 9525 insisted wednesday that he was not aware of any video footage from on board the plane .'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [test_dataset[0]['src_txt']]\n",
    "get_data_iter(article,batch_size=30000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6 cm3",
   "language": "python",
   "name": "cm3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
