{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Text Summerization on CNN/DM Dataset using BertSum\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook demonstrates how to fine tune BERT for extractive text summerization. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, result postprocessing, and model evaluation.\n",
    "\n",
    "BertSum refers to  [Fine-tune BERT for Extractive Summarization (https://arxiv.org/pdf/1903.10318.pdf) with [published example](https://github.com/nlpyang/BertSum/). Extractive summarization are usually used in document summarization where each input document consists of mutiple sentences. The preprocessing of the input training data involves assigning label 0 or 1 to the document sentences based on the give summary. The summarization problem is also simplfied to classifying whether each document sentence should be included in the summary. \n",
    "\n",
    "The figure below illustrates how BERTSum can be fine tuned for extractive summarization task. Each sentence is inserted with [CLS] token at the beginning and  [SEP] at the end. Interval segment embedding and positional embedding are added upon the token embedding before input the BERT model. The [CLS] token representation is used as sentence embedding and only the [CLS] tokens are used as input for the summarization model. The summarization layer predicts whether the probability of each each sentence token should be included in the summary or not. Techniques like trigram blocking can be used to improve model accuarcy.   \n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/?.PNG\">\n",
    "\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "The running time shown in this notebook is on a Standard_NC24s_v3 Azure Deep Learning Virtual Machine with 4 NVIDIA Tesla V100 GPUs. \n",
    "> **Tip**: If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`** to run the notebook on a small subset of the data and a smaller number of epochs. \n",
    "\n",
    "The table below provides some reference running time on different machine configurations.  \n",
    "\n",
    "|QUICK_RUN|Machine Configurations|Running time|\n",
    "|:---------|:----------------------|:------------|\n",
    "|True|1 NVIDIA Tesla K80 GPUs, 12GB GPU memory| ~ ? minutes |\n",
    "|False|4 NVIDIA Tesla V100 GPUs, 64GB GPU memory| ~ ? hours|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = True\n",
    "USE_PREPROCESSED_DARA =  False\n",
    "if not USE_PREPROCESSED_DARA:\n",
    "    BERT_DATA_PATH=\"/dadendev/BertSum/bert_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Before we start the notebook, we should set the environment variable to make sure you can access the GPUs on your machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to clone a modified version of BertSum so that it works for prediction cases and can run on any GPU device ID on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'BertSum' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/daden-ms/BertSum.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "nlp_path = os.path.abspath('../../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "    \n",
    "sys.path.insert(0, './BertSum/src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to install the dependencies for pyrouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies for ROUGE-1.5.5.pl\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install expat\n",
    "!sudo apt-get install libexpat-dev -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in your terminal\n",
    "1. sudo cpan install XML::Parser\n",
    "1. sudo cpan install XML::Parser::PerlSAX\n",
    "1. sudo cpan install XML::DOM\n",
    "\n",
    "Also you need to set up file2rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprossing\n",
    "\n",
    "The dataset we used for this notebook is CNN/DM dataset which contains the documents and accompanying questions from the news articles of CNN and Daily mail. The highlights in each article are used as summary. The dataset consits of ~289K training examples, ~11K valiation and ~11K test dataset.  You can choose to use the preprocessed version at [BERTSum published example](https://github.com/nlpyang/BertSum/) or use the following section to preprocess the data. Since it takes up to 28 hours to preprocess the training data  to run on 10  Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz, if you choose to run the preprocessing, we suggest you run with QUICKRUN set as True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you choose to use preprocessed data, continue to section #Model training.\n",
    "To continue with the data preprocessing, run the following command to download from https://github.com/harvardnlp/sent-summary and unzip the data to folder ./harvardnlp_cnndm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz &&\\\n",
    "    mkdir -p harvardnlp_cnndm &&\\\n",
    "    mv cnndm.tar.gz ./harvardnlp_cnndm && cd ./harvardnlp_cnndm &&\\\n",
    "    tar -xvf cnndm.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details of Data Preprocessing\n",
    "\n",
    "The purpose of preprocessing is to process the input articles to the format that BertSum takes.  Functions defined specific in harvardnlp_cnndm_preprocess function are unique to CNN/DM dataset that's processed by harvardnlp. However, it provides a skeleton of how to preprocessing data into the format that BertSum takes. Assuming you have all articles and target summery each in a file, line-breaker seperated, the steps to preprocess the data are:\n",
    "1. sentence tokenization\n",
    "2. word tokenization\n",
    "3. label the sentences in the article with 1 meaning the sentence is selected and 0 meaning the sentence is not selected. The options for the selection algorithms are \"greedy\" and \"combination\"\n",
    "3. convert each example to  BertSum format\n",
    "    - filter the sentences in the example based on the min_src_ntokens argument. If the lefted total sentence number is less than min_nsents, the example is discarded.\n",
    "    - truncate the sentences in the example if the length is greater than max_src_ntokens\n",
    "    - truncate the sentences in the example and the labels if the totle number of sentences is greater than max_nsents\n",
    "    - [CLS] and [SEP] are inserted before and after each sentence\n",
    "    - wordPiece tokenization\n",
    "    - truncate the example to 512 tokens\n",
    "    - convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary.\n",
    "    - segment ids are generated\n",
    "    - [CLS] token positions are logged\n",
    "    - [CLS] token labels are truncated if it's greater than 512, which is the maximum input length that can be taken by the BERT model.\n",
    "    \n",
    "    \n",
    "Note that the original BERTSum paper use Stanford CoreNLP for data proprocessing, here we'll first how to use NLTK version, and then we also provide instruction of how to set up Stanford NLP and code examples of how to use Standford CoreNLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.dataset.harvardnlp_cnndm import harvardnlp_cnndm_preprocess\n",
    "from utils_nlp.models.bert.extractive_text_summarization import bertsum_formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_train_job_number = -1\n",
    "max_test_job_number = -1\n",
    "if QUICK_RUN:\n",
    "    max_train_job_number = 100\n",
    "    max_test_job_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f\"./harvardnlp_cnndm/test.bertdata_{QUICK_RUN}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of training data: 100\n",
      "CPU times: user 3.14 s, sys: 1.63 s, total: 4.77 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TRAIN_SRC_FILE = \"./harvardnlp_cnndm/train.txt.src\"\n",
    "TRAIN_TGT_FILE = \"./harvardnlp_cnndm/train.txt.tgt.tagged\"\n",
    "PROCESSED_TRAIN_FILE = f\"./harvardnlp_cnndm/train.bertdata_{QUICK_RUN}\" \n",
    "import multiprocessing\n",
    "n_cpus = multiprocessing.cpu_count() - 1\n",
    "jobs = harvardnlp_cnndm_preprocess(n_cpus, TRAIN_SRC_FILE, TRAIN_TGT_FILE, max_train_job_number)\n",
    "print(\"total length of training data:\", len(jobs))\n",
    "from prepro.data_builder import BertData\n",
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "default_preprocessing_parameters =  {\"max_nsents\": 200, \"max_src_ntokens\": 2000, \"min_nsents\": 3, \"min_src_ntokens\": 5, \"use_interval\": True}\n",
    "args=Bunch(default_preprocessing_parameters)\n",
    "bertdata = BertData(args)\n",
    "bertsum_formatting(n_cpus, bertdata,\"combination\", jobs[0:max_train_job_number], PROCESSED_TRAIN_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of training data: 10\n",
      "CPU times: user 2.9 s, sys: 1.59 s, total: 4.49 s\n",
      "Wall time: 5.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEST_SRC_FILE = \"./harvardnlp_cnndm/test.txt.src\"\n",
    "TEST_TGT_FILE = \"./harvardnlp_cnndm/test.txt.tgt.tagged\"\n",
    "PROCESSED_TEST_FILE = f\"./harvardnlp_cnndm/test.bertdata_{QUICK_RUN}\" \n",
    "import multiprocessing\n",
    "n_cpus = multiprocessing.cpu_count() - 1\n",
    "jobs = harvardnlp_cnndm_preprocess(n_cpus, TRAIN_SRC_FILE, TRAIN_TGT_FILE, max_test_job_number)\n",
    "print(\"total length of training data:\", len(jobs))\n",
    "from prepro.data_builder import BertData\n",
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "default_preprocessing_parameters =  {\"max_nsents\": 200, \"max_src_ntokens\": 2000, \"min_nsents\": 3, \"min_src_ntokens\": 5, \"use_interval\": True}\n",
    "args=Bunch(default_preprocessing_parameters)\n",
    "bertdata = BertData(args)\n",
    "bertsum_formatting(n_cpus, bertdata,\"combination\", jobs[0:max_test_job_number], PROCESSED_TEST_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['src', 'labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "bert_format_data = torch.load(PROCESSED_TRAIN_FILE)\n",
    "print(len(bert_format_data))\n",
    "bert_format_data[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 3559,\n",
       " 1005,\n",
       " 1055,\n",
       " 3602,\n",
       " 1024,\n",
       " 1999,\n",
       " 2256,\n",
       " 2369,\n",
       " 1996,\n",
       " 5019,\n",
       " 2186,\n",
       " 1010,\n",
       " 13229,\n",
       " 11370,\n",
       " 2015,\n",
       " 3745,\n",
       " 2037,\n",
       " 6322,\n",
       " 1999,\n",
       " 5266,\n",
       " 2739,\n",
       " 1998,\n",
       " 17908,\n",
       " 1996,\n",
       " 3441,\n",
       " 2369,\n",
       " 1996,\n",
       " 2824,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2182,\n",
       " 1010,\n",
       " 7082,\n",
       " 14697,\n",
       " 1051,\n",
       " 1005,\n",
       " 9848,\n",
       " 3138,\n",
       " 5198,\n",
       " 2503,\n",
       " 1037,\n",
       " 7173,\n",
       " 2073,\n",
       " 2116,\n",
       " 1997,\n",
       " 1996,\n",
       " 13187,\n",
       " 2024,\n",
       " 10597,\n",
       " 5665,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2019,\n",
       " 24467,\n",
       " 7431,\n",
       " 2006,\n",
       " 1996,\n",
       " 1036,\n",
       " 1036,\n",
       " 6404,\n",
       " 2723,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 2073,\n",
       " 2116,\n",
       " 10597,\n",
       " 5665,\n",
       " 13187,\n",
       " 2024,\n",
       " 7431,\n",
       " 1999,\n",
       " 5631,\n",
       " 2077,\n",
       " 3979,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 5631,\n",
       " 1010,\n",
       " 3516,\n",
       " 1006,\n",
       " 13229,\n",
       " 1007,\n",
       " 1011,\n",
       " 1011,\n",
       " 1996,\n",
       " 6619,\n",
       " 2723,\n",
       " 1997,\n",
       " 1996,\n",
       " 5631,\n",
       " 1011,\n",
       " 27647,\n",
       " 3653,\n",
       " 18886,\n",
       " 2389,\n",
       " 12345,\n",
       " 4322,\n",
       " 2003,\n",
       " 9188,\n",
       " 1996,\n",
       " 1036,\n",
       " 1036,\n",
       " 6404,\n",
       " 2723,\n",
       " 1012,\n",
       " 1036,\n",
       " 1036,\n",
       " 102,\n",
       " 101,\n",
       " 2182,\n",
       " 1010,\n",
       " 13187,\n",
       " 2007,\n",
       " 1996,\n",
       " 2087,\n",
       " 5729,\n",
       " 5177,\n",
       " 24757,\n",
       " 2024,\n",
       " 23995,\n",
       " 2127,\n",
       " 2027,\n",
       " 1005,\n",
       " 2128,\n",
       " 3201,\n",
       " 2000,\n",
       " 3711,\n",
       " 1999,\n",
       " 2457,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2087,\n",
       " 2411,\n",
       " 1010,\n",
       " 2027,\n",
       " 2227,\n",
       " 4319,\n",
       " 5571,\n",
       " 2030,\n",
       " 5571,\n",
       " 1997,\n",
       " 6101,\n",
       " 2075,\n",
       " 2019,\n",
       " 2961,\n",
       " 1011,\n",
       " 1011,\n",
       " 5571,\n",
       " 2008,\n",
       " 3648,\n",
       " 7112,\n",
       " 26947,\n",
       " 16715,\n",
       " 2319,\n",
       " 2758,\n",
       " 2024,\n",
       " 2788,\n",
       " 1036,\n",
       " 1036,\n",
       " 4468,\n",
       " 3085,\n",
       " 10768,\n",
       " 7811,\n",
       " 3111,\n",
       " 1012,\n",
       " 1036,\n",
       " 1036,\n",
       " 102,\n",
       " 101,\n",
       " 2002,\n",
       " 2758,\n",
       " 1996,\n",
       " 17615,\n",
       " 2411,\n",
       " 2765,\n",
       " 2013,\n",
       " 13111,\n",
       " 2015,\n",
       " 2007,\n",
       " 2610,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 10597,\n",
       " 5665,\n",
       " 2111,\n",
       " 2411,\n",
       " 24185,\n",
       " 1050,\n",
       " 1005,\n",
       " 1056,\n",
       " 2079,\n",
       " 2054,\n",
       " 2027,\n",
       " 1005,\n",
       " 2128,\n",
       " 2409,\n",
       " 2043,\n",
       " 2610,\n",
       " 7180,\n",
       " 2006,\n",
       " 1996,\n",
       " 3496,\n",
       " 1011,\n",
       " 1011,\n",
       " 13111,\n",
       " 3849,\n",
       " 2000,\n",
       " 4654,\n",
       " 10732,\n",
       " 28483,\n",
       " 2618,\n",
       " 2037,\n",
       " 7355,\n",
       " 1998,\n",
       " 2027,\n",
       " 2468,\n",
       " 2062,\n",
       " 19810,\n",
       " 1010,\n",
       " 3972,\n",
       " 14499,\n",
       " 2389,\n",
       " 1010,\n",
       " 1998,\n",
       " 2625,\n",
       " 3497,\n",
       " 2000,\n",
       " 3582,\n",
       " 7826,\n",
       " 1010,\n",
       " 2429,\n",
       " 2000,\n",
       " 26947,\n",
       " 16715,\n",
       " 2319,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2061,\n",
       " 1010,\n",
       " 2027,\n",
       " 2203,\n",
       " 2039,\n",
       " 2006,\n",
       " 1996,\n",
       " 6619,\n",
       " 2723,\n",
       " 8949,\n",
       " 10597,\n",
       " 12491,\n",
       " 1010,\n",
       " 2021,\n",
       " 2025,\n",
       " 2893,\n",
       " 2151,\n",
       " 2613,\n",
       " 2393,\n",
       " 2138,\n",
       " 2027,\n",
       " 1005,\n",
       " 2128,\n",
       " 1999,\n",
       " 7173,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2057,\n",
       " 7255,\n",
       " 1996,\n",
       " 7173,\n",
       " 2007,\n",
       " 26947,\n",
       " 16715,\n",
       " 2319,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2002,\n",
       " 2003,\n",
       " 2092,\n",
       " 2124,\n",
       " 1999,\n",
       " 5631,\n",
       " 2004,\n",
       " 2019,\n",
       " 8175,\n",
       " 2005,\n",
       " 3425,\n",
       " 1998,\n",
       " 1996,\n",
       " 10597,\n",
       " 5665,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2130,\n",
       " 2295,\n",
       " 2057,\n",
       " 2020,\n",
       " 2025,\n",
       " 3599,\n",
       " 10979,\n",
       " 2007,\n",
       " 2330,\n",
       " 2608,\n",
       " 2011,\n",
       " 1996,\n",
       " 4932,\n",
       " 1010,\n",
       " 2057,\n",
       " 2020,\n",
       " 2445,\n",
       " 6656,\n",
       " 2000,\n",
       " 5607,\n",
       " 2678,\n",
       " 2696,\n",
       " 5051,\n",
       " 1998,\n",
       " 2778,\n",
       " 1996,\n",
       " 2723,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2175,\n",
       " 2503,\n",
       " 1996,\n",
       " 1036,\n",
       " 6404,\n",
       " 2723,\n",
       " 1005,\n",
       " 1036,\n",
       " 1036,\n",
       " 2012,\n",
       " 2034,\n",
       " 1010,\n",
       " 2009,\n",
       " 1005,\n",
       " 1055,\n",
       " 2524,\n",
       " 2000,\n",
       " 5646,\n",
       " 2073,\n",
       " 1996,\n",
       " 2111,\n",
       " 2024,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1996,\n",
       " 5895,\n",
       " 2024,\n",
       " 4147,\n",
       " 10353,\n",
       " 3238,\n",
       " 17925,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 5674,\n",
       " 6276,\n",
       " 8198,\n",
       " 2005,\n",
       " 2608,\n",
       " 1998,\n",
       " 2519,\n",
       " 1999,\n",
       " 1037,\n",
       " 3082,\n",
       " 12121,\n",
       " 5777,\n",
       " 4524,\n",
       " 1011,\n",
       " 1011,\n",
       " 2008,\n",
       " 1005,\n",
       " 1055,\n",
       " 2785,\n",
       " 1997,\n",
       " 2054,\n",
       " 2027,\n",
       " 2298,\n",
       " 2066,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2027,\n",
       " 1005,\n",
       " 2128,\n",
       " 2881,\n",
       " 2000,\n",
       " 2562,\n",
       " 1996,\n",
       " 10597,\n",
       " 5665,\n",
       " 5022,\n",
       " 2013,\n",
       " 22736,\n",
       " 3209,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2008,\n",
       " 1005,\n",
       " 1055,\n",
       " 2036,\n",
       " 2339,\n",
       " 2027,\n",
       " 2031,\n",
       " 2053,\n",
       " 6007,\n",
       " 1010,\n",
       " 12922,\n",
       " 2015,\n",
       " 2030,\n",
       " 13342,\n",
       " 2229,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 26947,\n",
       " 16715,\n",
       " 2319,\n",
       " 2758,\n",
       " 2055,\n",
       " 2028,\n",
       " 1011,\n",
       " 2353,\n",
       " 1997,\n",
       " 2035,\n",
       " 2111,\n",
       " 1999,\n",
       " 5631,\n",
       " 1011,\n",
       " 27647,\n",
       " 2221,\n",
       " 7173,\n",
       " 2015,\n",
       " 2024,\n",
       " 10597,\n",
       " 5665,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2061,\n",
       " 1010,\n",
       " 2002,\n",
       " 2758,\n",
       " 1010,\n",
       " 1996,\n",
       " 11591,\n",
       " 3872,\n",
       " 2003,\n",
       " 10827,\n",
       " 1996,\n",
       " 2291,\n",
       " 1010,\n",
       " 1998,\n",
       " 1996,\n",
       " 2765,\n",
       " 2003,\n",
       " 2054,\n",
       " 2057,\n",
       " 2156,\n",
       " 2006,\n",
       " 1996,\n",
       " 6619,\n",
       " 2723,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1997,\n",
       " 2607,\n",
       " 1010,\n",
       " 2009,\n",
       " 2003,\n",
       " 1037,\n",
       " 7173,\n",
       " 1010,\n",
       " 2061,\n",
       " 2009,\n",
       " 1005,\n",
       " 1055,\n",
       " 2025,\n",
       " 4011,\n",
       " 2000,\n",
       " 2022,\n",
       " 4010,\n",
       " 1998,\n",
       " 16334,\n",
       " 1010,\n",
       " 2021,\n",
       " 1996,\n",
       " 4597,\n",
       " 10982,\n",
       " 1010,\n",
       " 1996,\n",
       " 4442,\n",
       " 2024,\n",
       " 4714,\n",
       " 1998,\n",
       " 2009,\n",
       " 1005,\n",
       " 102]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .',\n",
       " 'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robbery',\n",
       " 'he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .',\n",
       " \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .',\n",
       " \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed',\n",
       " '` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .',\n",
       " \"` he was in a coma and in critical condition for months . '\",\n",
       " 'paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .',\n",
       " 'she said he was was alone at the time of the alleged assault and personal items were stolen .',\n",
       " 'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .',\n",
       " 'mogni was a third-year finance major from glen ellyn , ill. , who was participating in a semester-long program at john cabot university .',\n",
       " \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni . '\",\n",
       " \"the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['src_txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "To start model training, we need to create a instance of BertSumExtractiveSummarizer, a wrapper for running BertSum-based finetuning. You can select any device ID on your machine, but make sure that you include the string version of the device ID in the gpu_ranks argument.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose which GPU device to use\n",
    "device_id = 1\n",
    "gpu_ranks = str(device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the encoder algorithm. There are four options:\n",
    "- baseline: it used a smaller transformer model to replace the bert model and with transformer summarization layer\n",
    "- classifier: it uses pretrained BERT and fine-tune BERT with **simple logistic classification** summarization layer\n",
    "- transformer: it uses pretrained BERT and fine-tune BERT with **transformer** summarization layer\n",
    "- RNN: it uses pretrained BERT and fine-tune BERT with **LSTM** summarization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = 'baseline'\n",
    "model_base_path = './models/'\n",
    "log_base_path = './logs/'\n",
    "result_base_path = './results'\n",
    "\n",
    "BERT_CONFIG_PATH = \"/dadendev/nlp/BertSum/bert_config_uncased_base.json\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(model_base_path):\n",
    "    os.makedirs(model_base_path)\n",
    "if not os.path.exists(log_base_path):\n",
    "    os.makedirs(log_base_path)\n",
    "if not os.path.exists(result_base_path):\n",
    "    os.makedirs(result_base_path)\n",
    "    \n",
    "from random import random\n",
    "random_number = random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1']\n",
      "{1: 0}\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.models.bert.extractive_text_summarization import BertSumExtractiveSummarizer\n",
    "bertsum_model = BertSumExtractiveSummarizer(encoder = 'baseline', \n",
    "                                            model_path = model_base_path+encoder+str(random_number),\n",
    "                                            log_file = log_base_path+encoder+str(random_number),\n",
    "                                            bert_config_path=BERT_CONFIG_PATH,\n",
    "                                            device_id = device_id,\n",
    "                                            gpu_ranks = gpu_ranks,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the fully processed CNN/DM dataset to train the model. During the training, you can stop any time and retrain from the previous saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PREPROCESSED_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PREPROCESSED_DATA is True:\n",
    "    PROCESSED_TRAIN_FILE = './bert_train_data_all_none_excluded'\n",
    "    training_data_files = [PROCESSED_TRAIN_FILE]\n",
    "else:    \n",
    "    BERT_DATA_PATH=\"/dadendev/BertSum/bert_data/\"\n",
    "    import glob\n",
    "    pts = sorted(glob.glob(BERT_DATA_PATH + 'cnndm.train' + '.[0-9]*.pt'))\n",
    "    training_data_files = pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bertsum_model.fit(device_id, training_data_files, train_steps=50000, train_from=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)), or Recall-Oriented Understudy for Gisting Evaluation has been commonly used for evaluation text summerization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.data_loader  import DataIterator,Batch,Dataloader\n",
    "import os\n",
    "\n",
    "USE_PREPROCESSED_DATA = False\n",
    "if USE_PREPROCESSED_DATA is True: \n",
    "    test_dataset=torch.load(PROCESSED_TEST_FILE)\n",
    "else:\n",
    "    test_dataset=[]\n",
    "    for i in range(0,6):\n",
    "        filename = os.path.join(BERT_DATA_PATH, \"test/cnndm.test.{0}.bert.pt\".format(i))\n",
    "        test_dataset.extend(torch.load(filename))\n",
    "\n",
    "    \n",
    "def get_data_iter(dataset,is_test=False, batch_size=3000):\n",
    "    args = Bunch({})\n",
    "    args.use_interval = True\n",
    "    args.batch_size = batch_size\n",
    "    test_data_iter = None\n",
    "    test_data_iter  = DataIterator(args, dataset, args.batch_size, 'cuda', is_test=is_test, shuffle=False, sort=False)\n",
    "    return test_data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-07 03:29:59,368 INFO] Device ID 1\n",
      "[2019-10-07 03:29:59,373 INFO] Loading checkpoint from ./models/baseline0.14344633695274556/model_step_30000.pt\n",
      "[2019-10-07 03:30:02,139 INFO] * number of parameters: 5179137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 1\n",
      "gpu_rank 0\n"
     ]
    }
   ],
   "source": [
    "model_for_test = \"./models/baseline0.14344633695274556/model_step_30000.pt\"\n",
    "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter(test_dataset),\n",
    "                                   test_from=model_for_test,\n",
    "                                   sentence_seperator='<q>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11489"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11489\n",
      "11489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-07 03:31:33,696 [MainThread  ] [INFO ]  Writing summaries.\n",
      "[2019-10-07 03:31:33,696 INFO] Writing summaries.\n",
      "2019-10-07 03:31:33,698 [MainThread  ] [INFO ]  Processing summaries. Saving system files to /dadendev/textsum/results/tmpekvbkkdp/system and model files to /dadendev/textsum/results/tmpekvbkkdp/model.\n",
      "[2019-10-07 03:31:33,698 INFO] Processing summaries. Saving system files to /dadendev/textsum/results/tmpekvbkkdp/system and model files to /dadendev/textsum/results/tmpekvbkkdp/model.\n",
      "2019-10-07 03:31:33,700 [MainThread  ] [INFO ]  Processing files in /dadendev/textsum/results/rouge-tmp-2019-10-07-03-31-32/candidate/.\n",
      "[2019-10-07 03:31:33,700 INFO] Processing files in /dadendev/textsum/results/rouge-tmp-2019-10-07-03-31-32/candidate/.\n",
      "2019-10-07 03:31:35,332 [MainThread  ] [INFO ]  Saved processed files to /dadendev/textsum/results/tmpekvbkkdp/system.\n",
      "[2019-10-07 03:31:35,332 INFO] Saved processed files to /dadendev/textsum/results/tmpekvbkkdp/system.\n",
      "2019-10-07 03:31:35,335 [MainThread  ] [INFO ]  Processing files in /dadendev/textsum/results/rouge-tmp-2019-10-07-03-31-32/reference/.\n",
      "[2019-10-07 03:31:35,335 INFO] Processing files in /dadendev/textsum/results/rouge-tmp-2019-10-07-03-31-32/reference/.\n",
      "2019-10-07 03:31:36,920 [MainThread  ] [INFO ]  Saved processed files to /dadendev/textsum/results/tmpekvbkkdp/model.\n",
      "[2019-10-07 03:31:36,920 INFO] Saved processed files to /dadendev/textsum/results/tmpekvbkkdp/model.\n",
      "2019-10-07 03:31:37,246 [MainThread  ] [INFO ]  Written ROUGE configuration to /dadendev/textsum/results/tmp7nz1xogo/rouge_conf.xml\n",
      "[2019-10-07 03:31:37,246 INFO] Written ROUGE configuration to /dadendev/textsum/results/tmp7nz1xogo/rouge_conf.xml\n",
      "2019-10-07 03:31:37,248 [MainThread  ] [INFO ]  Running ROUGE with command /home/daden/.files2rouge/ROUGE-1.5.5.pl -e /home/daden/.files2rouge/data -c 95 -m -r 1000 -n 2 -a /dadendev/textsum/results/tmp7nz1xogo/rouge_conf.xml\n",
      "[2019-10-07 03:31:37,248 INFO] Running ROUGE with command /home/daden/.files2rouge/ROUGE-1.5.5.pl -e /home/daden/.files2rouge/data -c 95 -m -r 1000 -n 2 -a /dadendev/textsum/results/tmp7nz1xogo/rouge_conf.xml\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.eval.evaluate_summarization import get_rouge\n",
    "rouge_baseline = get_rouge(prediction, target, \"/dadendev/textsum/results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11489"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .he was flown back to chicago via air ambulance on march 20 , but he died on sunday .a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "args=Bunch({\"max_nsents\": int(1e5), \n",
    "            \"max_src_ntokens\": int(2e6), \n",
    "            \"min_nsents\": -1, \n",
    "            \"min_src_ntokens\": -1,  \n",
    "            \"use_interval\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro.data_builder import BertData\n",
    "bertdata = BertData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "#sys.path.insert(0, '../src')\n",
    "from others.utils import clean\n",
    "from multiprocess import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"]=\"/home/daden/stanfordnlp_resources/stanford-corenlp-full-2018-10-05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordnlp.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from utils_nlp.models.bert.extractive_text_summarization import tokenize_to_list, bertify\n",
    "import re\n",
    "\n",
    "def preprocess_target(line):\n",
    "    def _remove_ttags(line):\n",
    "        line = re.sub(r'<t>', '', line)\n",
    "        # change </t> to <q>\n",
    "        # pyrouge test requires <q> as  sentence splitter\n",
    "        line = re.sub(r'</t>', '<q>', line)\n",
    "        return line\n",
    "\n",
    "    return tokenize_to_list(client, _remove_ttags(line))\n",
    "def preprocess_source(line):\n",
    "    return tokenize_to_list(client, clean(line))\n",
    "\n",
    "def preprocess_cnndm(param):\n",
    "    source, target = param\n",
    "    return bertify(bertdata, source, target)\n",
    "\n",
    "def harvardnlp_cnndm_standfordnlp(client, source_file, target_file, n_cpus=2, top_n=-1):\n",
    "    source_list = []\n",
    "    i = 0\n",
    "    with open(source_file) as fd:\n",
    "        for line in fd:\n",
    "            source_list.append(line)\n",
    "            i +=1\n",
    "    \n",
    "    pool = Pool(n_cpus)\n",
    "    \n",
    "\n",
    "    tokenized_source_data =  pool.map(preprocess_source, source_list[0:top_n], int(len(source_list[0:top_n])/n_cpus))\n",
    "    pool.close()\n",
    "    pool.join\n",
    "    \n",
    "    i = 0\n",
    "    target_list = []\n",
    "    with open(target_file) as fd:\n",
    "        for line in fd:\n",
    "            target_list.append(line)\n",
    "            i +=1\n",
    "\n",
    "    pool = Pool(n_cpus)\n",
    "    tokenized_target_data =  pool.map(preprocess_target, target_list[0:top_n], int(len(target_list[0:top_n])/n_cpus))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "            \n",
    "\n",
    "    #return tokenized_source_data, tokenized_target_data\n",
    "\n",
    "    pool = Pool(n_cpus)\n",
    "    bertified_data =  pool.map(preprocess_cnndm, zip(tokenized_source_data[0:top_n], tokenized_target_data[0:top_n]), int(len(tokenized_source_data[0:top_n])/n_cpus))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return bertified_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx5G -cp /home/daden/stanfordnlp_resources/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e3b74cd551ba43f1.props -preload tokenize,ssplit\n",
      "Starting server with command: java -Xmx5G -cp /home/daden/stanfordnlp_resources/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e3b74cd551ba43f1.props -preload tokenize,ssplit\n",
      "Starting server with command: java -Xmx5G -cp /home/daden/stanfordnlp_resources/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e3b74cd551ba43f1.props -preload tokenize,ssplit\n",
      "Starting server with command: java -Xmx5G -cp /home/daden/stanfordnlp_resources/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e3b74cd551ba43f1.props -preload tokenize,ssplit\n",
      "CPU times: user 79.8 ms, sys: 85.4 ms, total: 165 ms\n",
      "Wall time: 7.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "source_file = './harvardnlp_cnndm/test.txt.src'\n",
    "target_file = './harvardnlp_cnndm/test.txt.tgt.tagged'\n",
    "client = CoreNLPClient(annotators=['tokenize','ssplit'])\n",
    "new_data = harvardnlp_cnndm_standfordnlp(client, source_file, target_file, n_cpus=2, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.data_loader  import DataIterator,Batch,Dataloader\n",
    "import os\n",
    "\n",
    "USE_PREPROCESSED_DATA = False\n",
    "if USE_PREPROCESSED_DATA is True: \n",
    "    test_dataset=torch.load(PROCESSED_TEST_FILE)\n",
    "else:\n",
    "    test_dataset=[]\n",
    "    for i in range(0,6):\n",
    "        filename = os.path.join(BERT_DATA_PATH, \"test/cnndm.test.{0}.bert.pt\".format(i))\n",
    "        test_dataset.extend(torch.load(filename))\n",
    "def get_data_iter(dataset,is_test=False, batch_size=3000):\n",
    "    args = Bunch({})\n",
    "    args.use_interval = True\n",
    "    args.batch_size = batch_size\n",
    "    test_data_iter = None\n",
    "    test_data_iter  = DataIterator(args, dataset, args.batch_size, 'cuda', is_test=is_test, shuffle=False, sort=False)\n",
    "    return test_data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_src = preprocess_source(\"\".join(test_dataset[0]['src_txt']))\n",
    "b_data = bertdata.preprocess(new_src, None, None)\n",
    "indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "b_data_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "               'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .',\n",
       " 'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robberyhe was taken to a medical facility in the chicago area , close to his family home in glen ellyn .',\n",
       " \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .',\n",
       " \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed ` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .',\n",
       " '` he was in a coma and in critical condition for months .',\n",
       " \"' paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .\",\n",
       " 'she said he was was alone at the time of the alleged assault and personal items were stolen .',\n",
       " 'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .',\n",
       " 'mogni was a third-year finance major from glen ellyn , ill .',\n",
       " ', who was participating in a semester-long program at john cabot university .',\n",
       " \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni .\",\n",
       " \"' the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_data_dict['src_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_data_dict['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-07 03:28:55,446 INFO] Device ID 1\n",
      "[2019-10-07 03:28:55,452 INFO] Loading checkpoint from ./models/baseline0.14344633695274556/model_step_30000.pt\n",
      "[2019-10-07 03:29:01,758 INFO] * number of parameters: 5179137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 1\n",
      "gpu_rank 0\n"
     ]
    }
   ],
   "source": [
    "model_for_test = \"./models/baseline0.14344633695274556/model_step_30000.pt\"\n",
    "#get_data_iter(output,batch_size=30000)\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter([b_data_dict], False),\n",
    "                                   test_from=model_for_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .he was flown back to chicago via air ambulance on march 20 , but he died on sunday .a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_gpu)",
   "language": "python",
   "name": "cm3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
