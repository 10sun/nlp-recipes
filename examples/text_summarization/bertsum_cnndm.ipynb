{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Text Summerization on CNN/DM Dataset using BertSum\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook demonstrates how to fine tune BERT for extractive text summerization. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, result postprocessing, and model evaluation.\n",
    "\n",
    "BertSum refers to  [Fine-tune BERT for Extractive Summarization (https://arxiv.org/pdf/1903.10318.pdf) with [published example](https://github.com/nlpyang/BertSum/). Extractive summarization are usually used in document summarization where each input document consists of mutiple sentences. The preprocessing of the input training data involves assigning label 0 or 1 to the document sentences based on the give summary. The summarization problem is also simplfied to classifying whether each document sentence should be included in the summary. \n",
    "\n",
    "The figure below illustrates how BERTSum can be fine tuned for extractive summarization task. Each sentence is inserted with [CLS] token at the beginning and  [SEP] at the end. Interval segment embedding and positional embedding are added upon the token embedding before input the BERT model. The [CLS] token representation is used as sentence embedding and only the [CLS] tokens are used as input for the summarization model. The summarization layer predicts whether the probability of each each sentence token should be included in the summary or not. Techniques like trigram blocking can be used to improve model accuarcy.   \n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/BertSum.PNG\">\n",
    "\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "The running time shown in this notebook is on a Standard_NC24s_v3 Azure Deep Learning Virtual Machine with 4 NVIDIA Tesla V100 GPUs. \n",
    "> **Tip**: If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`** to run the notebook on a small subset of the data and a smaller number of epochs. \n",
    "\n",
    "The table below provides some reference running time on different machine configurations.  \n",
    "\n",
    "|QUICK_RUN|Machine Configurations|Running time|\n",
    "|:---------|:----------------------|:------------|\n",
    "|True|1 NVIDIA Tesla K80 GPUs, 12GB GPU memory| ~ ? minutes |\n",
    "|False|4 NVIDIA Tesla V100 GPUs, 64GB GPU memory| ~ ? hours|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = True\n",
    "USE_PREPROCESSED_DATA =  False\n",
    "if not USE_PREPROCESSED_DATA:\n",
    "    #BERT_DATA_PATH=\"/dadendev/BertSum/bert_data/\"\n",
    "    BERT_DATA_PATH=\"/dadendev/textsum/bert_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Before we start the notebook, we should set the environment variable to make sure you can access the GPUs on your machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to clone a modified version of BertSum so that it works for prediction cases and can run on any GPU device ID on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-08 19:30:42--  https://raw.githubusercontent.com/nlpyang/BertSum/master/bert_config_uncased_base.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.124.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.124.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 313 [text/plain]\n",
      "Saving to: ‘bert_config_uncased_base.json.1’\n",
      "\n",
      "bert_config_uncased 100%[===================>]     313  --.-KB/s    in 0s      \n",
      "\n",
      "2019-10-08 19:30:43 (56.3 MB/s) - ‘bert_config_uncased_base.json.1’ saved [313/313]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/nlpyang/BertSum/master/bert_config_uncased_base.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_CONFIG_PATH=\"./bert_config_uncased_base.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "nlp_path = os.path.abspath('../../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "sys.path.insert(0, \"./\")\n",
    "sys.path.insert(0, \"/dadendev/nlp/examples/text_summarization/BertSum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to install the dependencies for pyrouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies for ROUGE-1.5.5.pl\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install expat\n",
    "!sudo apt-get install libexpat-dev -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in your terminal\n",
    "1. sudo cpan install XML::Parser\n",
    "1. sudo cpan install XML::Parser::PerlSAX\n",
    "1. sudo cpan install XML::DOM\n",
    "\n",
    "Also you need to set up file2rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprossing\n",
    "\n",
    "The dataset we used for this notebook is CNN/DM dataset which contains the documents and accompanying questions from the news articles of CNN and Daily mail. The highlights in each article are used as summary. The dataset consits of ~289K training examples, ~11K valiation and ~11K test dataset.  You can choose to use the preprocessed version at [BERTSum published example](https://github.com/nlpyang/BertSum/) or use the following section to preprocess the data. Since it takes up to 28 hours to preprocess the training data  to run on 10  Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz, if you choose to run the preprocessing, we suggest you run with QUICKRUN set as True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you choose to use preprocessed data, continue to section #Model training.\n",
    "To continue with the data preprocessing, run the following command to download from https://github.com/harvardnlp/sent-summary and unzip the data to folder ./harvardnlp_cnndm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/opennmt-models/Summary/cnndm.tar.gz &&\\\n",
    "    mkdir -p harvardnlp_cnndm &&\\\n",
    "    mv cnndm.tar.gz ./harvardnlp_cnndm && cd ./harvardnlp_cnndm &&\\\n",
    "    tar -xvf cnndm.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details of Data Preprocessing\n",
    "\n",
    "The purpose of preprocessing is to process the input articles to the format that BertSum takes.  Functions defined specific in harvardnlp_cnndm_preprocess function are unique to CNN/DM dataset that's processed by harvardnlp. However, it provides a skeleton of how to preprocessing data into the format that BertSum takes. Assuming you have all articles and target summery each in a file, line-breaker seperated, the steps to preprocess the data are:\n",
    "1. sentence tokenization\n",
    "2. word tokenization\n",
    "3. label the sentences in the article with 1 meaning the sentence is selected and 0 meaning the sentence is not selected. The options for the selection algorithms are \"greedy\" and \"combination\"\n",
    "3. convert each example to  BertSum format\n",
    "    - filter the sentences in the example based on the min_src_ntokens argument. If the lefted total sentence number is less than min_nsents, the example is discarded.\n",
    "    - truncate the sentences in the example if the length is greater than max_src_ntokens\n",
    "    - truncate the sentences in the example and the labels if the totle number of sentences is greater than max_nsents\n",
    "    - [CLS] and [SEP] are inserted before and after each sentence\n",
    "    - wordPiece tokenization\n",
    "    - truncate the example to 512 tokens\n",
    "    - convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary.\n",
    "    - segment ids are generated\n",
    "    - [CLS] token positions are logged\n",
    "    - [CLS] token labels are truncated if it's greater than 512, which is the maximum input length that can be taken by the BERT model.\n",
    "    \n",
    "    \n",
    "Note that the original BERTSum paper use Stanford CoreNLP for data proprocessing, here we'll first how to use NLTK version, and then we also provide instruction of how to set up Stanford NLP and code examples of how to use Standford CoreNLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.dataset.harvardnlp_cnndm import harvardnlp_cnndm_preprocess\n",
    "from utils_nlp.models.bert.extractive_text_summarization import bertsum_formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_train_job_number = -1\n",
    "max_test_job_number = -1\n",
    "if QUICK_RUN:\n",
    "    max_train_job_number = 100\n",
    "    max_test_job_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f\"./harvardnlp_cnndm/test.bertdata_{QUICK_RUN}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of training data: 100\n",
      "CPU times: user 3.14 s, sys: 1.63 s, total: 4.77 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TRAIN_SRC_FILE = \"./harvardnlp_cnndm/train.txt.src\"\n",
    "TRAIN_TGT_FILE = \"./harvardnlp_cnndm/train.txt.tgt.tagged\"\n",
    "PROCESSED_TRAIN_FILE = f\"./harvardnlp_cnndm/train.bertdata_{QUICK_RUN}\" \n",
    "import multiprocessing\n",
    "n_cpus = multiprocessing.cpu_count() - 1\n",
    "jobs = harvardnlp_cnndm_preprocess(n_cpus, TRAIN_SRC_FILE, TRAIN_TGT_FILE, max_train_job_number)\n",
    "print(\"total length of training data:\", len(jobs))\n",
    "from prepro.data_builder import BertData\n",
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "default_preprocessing_parameters =  {\"max_nsents\": 200, \"max_src_ntokens\": 2000, \"min_nsents\": 3, \"min_src_ntokens\": 5, \"use_interval\": True}\n",
    "args=Bunch(default_preprocessing_parameters)\n",
    "bertdata = BertData(args)\n",
    "bertsum_formatting(n_cpus, bertdata,\"combination\", jobs[0:max_train_job_number], PROCESSED_TRAIN_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of training data: 10\n",
      "CPU times: user 2.9 s, sys: 1.59 s, total: 4.49 s\n",
      "Wall time: 5.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEST_SRC_FILE = \"./harvardnlp_cnndm/test.txt.src\"\n",
    "TEST_TGT_FILE = \"./harvardnlp_cnndm/test.txt.tgt.tagged\"\n",
    "PROCESSED_TEST_FILE = f\"./harvardnlp_cnndm/test.bertdata_{QUICK_RUN}\" \n",
    "import multiprocessing\n",
    "n_cpus = multiprocessing.cpu_count() - 1\n",
    "jobs = harvardnlp_cnndm_preprocess(n_cpus, TRAIN_SRC_FILE, TRAIN_TGT_FILE, max_test_job_number)\n",
    "print(\"total length of training data:\", len(jobs))\n",
    "from prepro.data_builder import BertData\n",
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "default_preprocessing_parameters =  {\"max_nsents\": 200, \"max_src_ntokens\": 2000, \"min_nsents\": 3, \"min_src_ntokens\": 5, \"use_interval\": True}\n",
    "args=Bunch(default_preprocessing_parameters)\n",
    "bertdata = BertData(args)\n",
    "bertsum_formatting(n_cpus, bertdata,\"combination\", jobs[0:max_test_job_number], PROCESSED_TEST_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['src', 'labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "bert_format_data = torch.load(PROCESSED_TRAIN_FILE)\n",
    "print(len(bert_format_data))\n",
    "bert_format_data[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 3559,\n",
       " 1005,\n",
       " 1055,\n",
       " 3602,\n",
       " 1024,\n",
       " 1999,\n",
       " 2256,\n",
       " 2369,\n",
       " 1996,\n",
       " 5019,\n",
       " 2186,\n",
       " 1010,\n",
       " 13229,\n",
       " 11370,\n",
       " 2015,\n",
       " 3745,\n",
       " 2037,\n",
       " 6322,\n",
       " 1999,\n",
       " 5266,\n",
       " 2739,\n",
       " 1998,\n",
       " 17908,\n",
       " 1996,\n",
       " 3441,\n",
       " 2369,\n",
       " 1996,\n",
       " 2824,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2182,\n",
       " 1010,\n",
       " 7082,\n",
       " 14697,\n",
       " 1051,\n",
       " 1005,\n",
       " 9848,\n",
       " 3138,\n",
       " 5198,\n",
       " 2503,\n",
       " 1037,\n",
       " 7173,\n",
       " 2073,\n",
       " 2116,\n",
       " 1997,\n",
       " 1996,\n",
       " 13187,\n",
       " 2024,\n",
       " 10597,\n",
       " 5665,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2019,\n",
       " 24467,\n",
       " 7431,\n",
       " 2006,\n",
       " 1996,\n",
       " 1036,\n",
       " 1036,\n",
       " 6404,\n",
       " 2723,\n",
       " 1010,\n",
       " 1036,\n",
       " 1036,\n",
       " 2073,\n",
       " 2116,\n",
       " 10597,\n",
       " 5665,\n",
       " 13187,\n",
       " 2024,\n",
       " 7431,\n",
       " 1999,\n",
       " 5631,\n",
       " 2077,\n",
       " 3979,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 5631,\n",
       " 1010,\n",
       " 3516,\n",
       " 1006,\n",
       " 13229,\n",
       " 1007,\n",
       " 1011,\n",
       " 1011,\n",
       " 1996,\n",
       " 6619,\n",
       " 2723,\n",
       " 1997,\n",
       " 1996,\n",
       " 5631,\n",
       " 1011,\n",
       " 27647,\n",
       " 3653,\n",
       " 18886,\n",
       " 2389,\n",
       " 12345,\n",
       " 4322,\n",
       " 2003,\n",
       " 9188,\n",
       " 1996,\n",
       " 1036,\n",
       " 1036,\n",
       " 6404,\n",
       " 2723,\n",
       " 1012,\n",
       " 1036,\n",
       " 1036,\n",
       " 102,\n",
       " 101,\n",
       " 2182,\n",
       " 1010,\n",
       " 13187,\n",
       " 2007,\n",
       " 1996,\n",
       " 2087,\n",
       " 5729,\n",
       " 5177,\n",
       " 24757,\n",
       " 2024,\n",
       " 23995,\n",
       " 2127,\n",
       " 2027,\n",
       " 1005,\n",
       " 2128,\n",
       " 3201,\n",
       " 2000,\n",
       " 3711,\n",
       " 1999,\n",
       " 2457,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2087,\n",
       " 2411,\n",
       " 1010,\n",
       " 2027,\n",
       " 2227,\n",
       " 4319,\n",
       " 5571,\n",
       " 2030,\n",
       " 5571,\n",
       " 1997,\n",
       " 6101,\n",
       " 2075,\n",
       " 2019,\n",
       " 2961,\n",
       " 1011,\n",
       " 1011,\n",
       " 5571,\n",
       " 2008,\n",
       " 3648,\n",
       " 7112,\n",
       " 26947,\n",
       " 16715,\n",
       " 2319,\n",
       " 2758,\n",
       " 2024,\n",
       " 2788,\n",
       " 1036,\n",
       " 1036,\n",
       " 4468,\n",
       " 3085,\n",
       " 10768,\n",
       " 7811,\n",
       " 3111,\n",
       " 1012,\n",
       " 1036,\n",
       " 1036,\n",
       " 102,\n",
       " 101,\n",
       " 2002,\n",
       " 2758,\n",
       " 1996,\n",
       " 17615,\n",
       " 2411,\n",
       " 2765,\n",
       " 2013,\n",
       " 13111,\n",
       " 2015,\n",
       " 2007,\n",
       " 2610,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 10597,\n",
       " 5665,\n",
       " 2111,\n",
       " 2411,\n",
       " 24185,\n",
       " 1050,\n",
       " 1005,\n",
       " 1056,\n",
       " 2079,\n",
       " 2054,\n",
       " 2027,\n",
       " 1005,\n",
       " 2128,\n",
       " 2409,\n",
       " 2043,\n",
       " 2610,\n",
       " 7180,\n",
       " 2006,\n",
       " 1996,\n",
       " 3496,\n",
       " 1011,\n",
       " 1011,\n",
       " 13111,\n",
       " 3849,\n",
       " 2000,\n",
       " 4654,\n",
       " 10732,\n",
       " 28483,\n",
       " 2618,\n",
       " 2037,\n",
       " 7355,\n",
       " 1998,\n",
       " 2027,\n",
       " 2468,\n",
       " 2062,\n",
       " 19810,\n",
       " 1010,\n",
       " 3972,\n",
       " 14499,\n",
       " 2389,\n",
       " 1010,\n",
       " 1998,\n",
       " 2625,\n",
       " 3497,\n",
       " 2000,\n",
       " 3582,\n",
       " 7826,\n",
       " 1010,\n",
       " 2429,\n",
       " 2000,\n",
       " 26947,\n",
       " 16715,\n",
       " 2319,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2061,\n",
       " 1010,\n",
       " 2027,\n",
       " 2203,\n",
       " 2039,\n",
       " 2006,\n",
       " 1996,\n",
       " 6619,\n",
       " 2723,\n",
       " 8949,\n",
       " 10597,\n",
       " 12491,\n",
       " 1010,\n",
       " 2021,\n",
       " 2025,\n",
       " 2893,\n",
       " 2151,\n",
       " 2613,\n",
       " 2393,\n",
       " 2138,\n",
       " 2027,\n",
       " 1005,\n",
       " 2128,\n",
       " 1999,\n",
       " 7173,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2057,\n",
       " 7255,\n",
       " 1996,\n",
       " 7173,\n",
       " 2007,\n",
       " 26947,\n",
       " 16715,\n",
       " 2319,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2002,\n",
       " 2003,\n",
       " 2092,\n",
       " 2124,\n",
       " 1999,\n",
       " 5631,\n",
       " 2004,\n",
       " 2019,\n",
       " 8175,\n",
       " 2005,\n",
       " 3425,\n",
       " 1998,\n",
       " 1996,\n",
       " 10597,\n",
       " 5665,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2130,\n",
       " 2295,\n",
       " 2057,\n",
       " 2020,\n",
       " 2025,\n",
       " 3599,\n",
       " 10979,\n",
       " 2007,\n",
       " 2330,\n",
       " 2608,\n",
       " 2011,\n",
       " 1996,\n",
       " 4932,\n",
       " 1010,\n",
       " 2057,\n",
       " 2020,\n",
       " 2445,\n",
       " 6656,\n",
       " 2000,\n",
       " 5607,\n",
       " 2678,\n",
       " 2696,\n",
       " 5051,\n",
       " 1998,\n",
       " 2778,\n",
       " 1996,\n",
       " 2723,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2175,\n",
       " 2503,\n",
       " 1996,\n",
       " 1036,\n",
       " 6404,\n",
       " 2723,\n",
       " 1005,\n",
       " 1036,\n",
       " 1036,\n",
       " 2012,\n",
       " 2034,\n",
       " 1010,\n",
       " 2009,\n",
       " 1005,\n",
       " 1055,\n",
       " 2524,\n",
       " 2000,\n",
       " 5646,\n",
       " 2073,\n",
       " 1996,\n",
       " 2111,\n",
       " 2024,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1996,\n",
       " 5895,\n",
       " 2024,\n",
       " 4147,\n",
       " 10353,\n",
       " 3238,\n",
       " 17925,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 5674,\n",
       " 6276,\n",
       " 8198,\n",
       " 2005,\n",
       " 2608,\n",
       " 1998,\n",
       " 2519,\n",
       " 1999,\n",
       " 1037,\n",
       " 3082,\n",
       " 12121,\n",
       " 5777,\n",
       " 4524,\n",
       " 1011,\n",
       " 1011,\n",
       " 2008,\n",
       " 1005,\n",
       " 1055,\n",
       " 2785,\n",
       " 1997,\n",
       " 2054,\n",
       " 2027,\n",
       " 2298,\n",
       " 2066,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2027,\n",
       " 1005,\n",
       " 2128,\n",
       " 2881,\n",
       " 2000,\n",
       " 2562,\n",
       " 1996,\n",
       " 10597,\n",
       " 5665,\n",
       " 5022,\n",
       " 2013,\n",
       " 22736,\n",
       " 3209,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2008,\n",
       " 1005,\n",
       " 1055,\n",
       " 2036,\n",
       " 2339,\n",
       " 2027,\n",
       " 2031,\n",
       " 2053,\n",
       " 6007,\n",
       " 1010,\n",
       " 12922,\n",
       " 2015,\n",
       " 2030,\n",
       " 13342,\n",
       " 2229,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 26947,\n",
       " 16715,\n",
       " 2319,\n",
       " 2758,\n",
       " 2055,\n",
       " 2028,\n",
       " 1011,\n",
       " 2353,\n",
       " 1997,\n",
       " 2035,\n",
       " 2111,\n",
       " 1999,\n",
       " 5631,\n",
       " 1011,\n",
       " 27647,\n",
       " 2221,\n",
       " 7173,\n",
       " 2015,\n",
       " 2024,\n",
       " 10597,\n",
       " 5665,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2061,\n",
       " 1010,\n",
       " 2002,\n",
       " 2758,\n",
       " 1010,\n",
       " 1996,\n",
       " 11591,\n",
       " 3872,\n",
       " 2003,\n",
       " 10827,\n",
       " 1996,\n",
       " 2291,\n",
       " 1010,\n",
       " 1998,\n",
       " 1996,\n",
       " 2765,\n",
       " 2003,\n",
       " 2054,\n",
       " 2057,\n",
       " 2156,\n",
       " 2006,\n",
       " 1996,\n",
       " 6619,\n",
       " 2723,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1997,\n",
       " 2607,\n",
       " 1010,\n",
       " 2009,\n",
       " 2003,\n",
       " 1037,\n",
       " 7173,\n",
       " 1010,\n",
       " 2061,\n",
       " 2009,\n",
       " 1005,\n",
       " 1055,\n",
       " 2025,\n",
       " 4011,\n",
       " 2000,\n",
       " 2022,\n",
       " 4010,\n",
       " 1998,\n",
       " 16334,\n",
       " 1010,\n",
       " 2021,\n",
       " 1996,\n",
       " 4597,\n",
       " 10982,\n",
       " 1010,\n",
       " 1996,\n",
       " 4442,\n",
       " 2024,\n",
       " 4714,\n",
       " 1998,\n",
       " 2009,\n",
       " 1005,\n",
       " 102]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .',\n",
       " 'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robbery',\n",
       " 'he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .',\n",
       " \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .',\n",
       " \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed',\n",
       " '` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .',\n",
       " \"` he was in a coma and in critical condition for months . '\",\n",
       " 'paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .',\n",
       " 'she said he was was alone at the time of the alleged assault and personal items were stolen .',\n",
       " 'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .',\n",
       " 'mogni was a third-year finance major from glen ellyn , ill. , who was participating in a semester-long program at john cabot university .',\n",
       " \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni . '\",\n",
       " \"the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['src_txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "To start model training, we need to create a instance of BertSumExtractiveSummarizer, a wrapper for running BertSum-based finetuning. You can select any device ID on your machine, but make sure that you include the string version of the device ID in the gpu_ranks argument.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose which GPU device to use\n",
    "device_id = 1\n",
    "gpu_ranks = str(device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the encoder algorithm. There are four options:\n",
    "- baseline: it used a smaller transformer model to replace the bert model and with transformer summarization layer\n",
    "- classifier: it uses pretrained BERT and fine-tune BERT with **simple logistic classification** summarization layer\n",
    "- transformer: it uses pretrained BERT and fine-tune BERT with **transformer** summarization layer\n",
    "- RNN: it uses pretrained BERT and fine-tune BERT with **LSTM** summarization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = 'baseline'\n",
    "model_base_path = './models/'\n",
    "log_base_path = './logs/'\n",
    "result_base_path = './results'\n",
    "\n",
    "BERT_CONFIG_PATH = \"/dadendev/nlp/BertSum/bert_config_uncased_base.json\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(model_base_path):\n",
    "    os.makedirs(model_base_path)\n",
    "if not os.path.exists(log_base_path):\n",
    "    os.makedirs(log_base_path)\n",
    "if not os.path.exists(result_base_path):\n",
    "    os.makedirs(result_base_path)\n",
    "    \n",
    "from random import random\n",
    "random_number = random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1']\n",
      "{1: 0}\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.models.bert.extractive_text_summarization import BertSumExtractiveSummarizer\n",
    "bertsum_model = BertSumExtractiveSummarizer(encoder = 'baseline', \n",
    "                                            model_path = model_base_path+encoder+str(random_number),\n",
    "                                            log_file = log_base_path+encoder+str(random_number),\n",
    "                                            bert_config_path=BERT_CONFIG_PATH,\n",
    "                                            device_id = device_id,\n",
    "                                            gpu_ranks = gpu_ranks,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the fully processed CNN/DM dataset to train the model. During the training, you can stop any time and retrain from the previous saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PREPROCESSED_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PREPROCESSED_DATA is True:\n",
    "    PROCESSED_TRAIN_FILE = './bert_train_data_all_none_excluded'\n",
    "    training_data_files = [PROCESSED_TRAIN_FILE]\n",
    "else:    \n",
    "    import glob\n",
    "    BERT_DATA_PATH=\"/dadendev/nlp/examples/text_summarization/bertdata/train/\"\n",
    "    pts = sorted(glob.glob(BERT_DATA_PATH + 'train.bertdata' + '.[0-9]*'))\n",
    "    #pts = sorted(glob.glob(BERT_DATA_PATH + 'cnndm.train' + '.[0-9]*.pt'))\n",
    "    training_data_files = pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.0',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.10000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.100000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.110000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.120000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.130000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.140000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.150000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.160000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.170000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.180000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.190000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.20000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.200000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.210000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.220000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.230000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.240000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.250000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.260000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.270000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.280000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.30000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.40000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.50000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.60000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.70000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.80000',\n",
       " '/dadendev/nlp/examples/text_summarization/bertdata/train/train.bertdata.90000']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-10 20:27:30,812 INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "[2019-10-10 20:27:30,814 INFO] extracting archive file ./temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpha_v8quq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accum_count': 2, 'batch_size': 3000, 'beta1': 0.9, 'beta2': 0.999, 'block_trigram': True, 'decay_method': 'noam', 'dropout': 0.1, 'encoder': 'baseline', 'ff_size': 512, 'gpu_ranks': '1', 'heads': 4, 'hidden_size': 128, 'inter_layers': 2, 'lr': 0.002, 'max_grad_norm': 0, 'max_nsents': 100, 'max_src_ntokens': 200, 'min_nsents': 3, 'min_src_ntokens': 10, 'optim': 'adam', 'oracle_mode': 'combination', 'param_init': 0.0, 'param_init_glorot': True, 'recall_eval': False, 'report_every': 50, 'report_rouge': True, 'rnn_size': 512, 'save_checkpoint_steps': 500, 'seed': 42, 'temp_dir': './temp', 'test_all': False, 'test_from': '', 'train_from': '', 'use_interval': True, 'visible_gpus': '0', 'warmup_steps': 10000, 'world_size': 1, 'mode': 'train', 'model_path': './models/baseline0.4638002095122038', 'log_file': './logs/baseline0.4638002095122038', 'bert_config_path': '/dadendev/nlp/BertSum/bert_config_uncased_base.json', 'gpu_ranks_map': {1: 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-10 20:27:34,555 INFO] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2019-10-10 20:27:39,167 INFO] * number of parameters: 5179137\n",
      "[2019-10-10 20:27:39,169 INFO] Start training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 1\n",
      "gpu_rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-10 20:27:43,207 INFO] Step 50/50000; xent: 13.18; lr: 0.0000001; 300 docs/s;      3 sec\n",
      "[2019-10-10 20:27:46,300 INFO] Step 100/50000; xent: 12.94; lr: 0.0000002; 323 docs/s;      6 sec\n",
      "[2019-10-10 20:27:49,361 INFO] Step 150/50000; xent: 12.49; lr: 0.0000003; 326 docs/s;      9 sec\n",
      "[2019-10-10 20:27:52,455 INFO] Step 200/50000; xent: 11.62; lr: 0.0000004; 326 docs/s;     13 sec\n",
      "[2019-10-10 20:27:55,534 INFO] Step 250/50000; xent: 11.20; lr: 0.0000005; 324 docs/s;     16 sec\n",
      "[2019-10-10 20:27:58,596 INFO] Step 300/50000; xent: 10.23; lr: 0.0000006; 328 docs/s;     19 sec\n",
      "[2019-10-10 20:28:01,663 INFO] Step 350/50000; xent: 9.27; lr: 0.0000007; 326 docs/s;     22 sec\n",
      "[2019-10-10 20:28:04,759 INFO] Step 400/50000; xent: 8.30; lr: 0.0000008; 323 docs/s;     25 sec\n",
      "[2019-10-10 20:28:07,839 INFO] Step 450/50000; xent: 7.40; lr: 0.0000009; 321 docs/s;     28 sec\n",
      "[2019-10-10 20:28:10,906 INFO] Step 500/50000; xent: 6.70; lr: 0.0000010; 328 docs/s;     31 sec\n",
      "[2019-10-10 20:28:10,908 INFO] Saving checkpoint ./models/baseline0.4638002095122038/model_step_500.pt\n",
      "[2019-10-10 20:28:14,060 INFO] Step 550/50000; xent: 5.96; lr: 0.0000011; 317 docs/s;     34 sec\n",
      "[2019-10-10 20:28:17,169 INFO] Step 600/50000; xent: 5.46; lr: 0.0000012; 321 docs/s;     37 sec\n",
      "[2019-10-10 20:28:20,233 INFO] Step 650/50000; xent: 4.89; lr: 0.0000013; 325 docs/s;     40 sec\n",
      "[2019-10-10 20:28:23,314 INFO] Step 700/50000; xent: 4.67; lr: 0.0000014; 324 docs/s;     43 sec\n",
      "[2019-10-10 20:28:26,387 INFO] Step 750/50000; xent: 4.31; lr: 0.0000015; 328 docs/s;     47 sec\n",
      "[2019-10-10 20:28:29,452 INFO] Step 800/50000; xent: 4.18; lr: 0.0000016; 325 docs/s;     50 sec\n",
      "[2019-10-10 20:28:32,523 INFO] Step 850/50000; xent: 4.08; lr: 0.0000017; 325 docs/s;     53 sec\n",
      "[2019-10-10 20:28:35,584 INFO] Step 900/50000; xent: 4.08; lr: 0.0000018; 324 docs/s;     56 sec\n",
      "[2019-10-10 20:28:38,654 INFO] Step 950/50000; xent: 3.90; lr: 0.0000019; 327 docs/s;     59 sec\n",
      "[2019-10-10 20:28:41,693 INFO] Step 1000/50000; xent: 3.91; lr: 0.0000020; 329 docs/s;     62 sec\n",
      "[2019-10-10 20:28:41,695 INFO] Saving checkpoint ./models/baseline0.4638002095122038/model_step_1000.pt\n",
      "[2019-10-10 20:28:45,950 INFO] Step 1050/50000; xent: 3.91; lr: 0.0000021; 239 docs/s;     66 sec\n",
      "[2019-10-10 20:28:49,039 INFO] Step 1100/50000; xent: 3.94; lr: 0.0000022; 325 docs/s;     69 sec\n",
      "[2019-10-10 20:28:52,133 INFO] Step 1150/50000; xent: 3.93; lr: 0.0000023; 326 docs/s;     72 sec\n",
      "[2019-10-10 20:28:55,237 INFO] Step 1200/50000; xent: 3.90; lr: 0.0000024; 327 docs/s;     75 sec\n",
      "[2019-10-10 20:28:58,315 INFO] Step 1250/50000; xent: 3.84; lr: 0.0000025; 325 docs/s;     78 sec\n",
      "[2019-10-10 20:29:01,429 INFO] Step 1300/50000; xent: 3.93; lr: 0.0000026; 323 docs/s;     82 sec\n",
      "[2019-10-10 20:29:04,520 INFO] Step 1350/50000; xent: 3.79; lr: 0.0000027; 329 docs/s;     85 sec\n",
      "[2019-10-10 20:29:07,634 INFO] Step 1400/50000; xent: 4.00; lr: 0.0000028; 321 docs/s;     88 sec\n",
      "[2019-10-10 20:29:10,770 INFO] Step 1450/50000; xent: 3.79; lr: 0.0000029; 327 docs/s;     91 sec\n",
      "[2019-10-10 20:29:13,888 INFO] Step 1500/50000; xent: 3.88; lr: 0.0000030; 320 docs/s;     94 sec\n",
      "[2019-10-10 20:29:13,893 INFO] Saving checkpoint ./models/baseline0.4638002095122038/model_step_1500.pt\n",
      "[2019-10-10 20:29:17,057 INFO] Step 1550/50000; xent: 3.90; lr: 0.0000031; 322 docs/s;     97 sec\n",
      "[2019-10-10 20:29:20,209 INFO] Step 1600/50000; xent: 3.87; lr: 0.0000032; 318 docs/s;    100 sec\n",
      "[2019-10-10 20:29:23,305 INFO] Step 1650/50000; xent: 3.86; lr: 0.0000033; 326 docs/s;    103 sec\n",
      "[2019-10-10 20:29:26,419 INFO] Step 1700/50000; xent: 3.75; lr: 0.0000034; 328 docs/s;    107 sec\n",
      "[2019-10-10 20:29:29,523 INFO] Step 1750/50000; xent: 3.80; lr: 0.0000035; 327 docs/s;    110 sec\n",
      "[2019-10-10 20:29:32,616 INFO] Step 1800/50000; xent: 3.74; lr: 0.0000036; 328 docs/s;    113 sec\n"
     ]
    }
   ],
   "source": [
    "bertsum_model.fit(device_id, training_data_files, train_steps=50000, train_from=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_CONFIG_PATH=\"./bert_config_uncased_base.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1']\n",
      "{1: 0}\n"
     ]
    }
   ],
   "source": [
    "bertsum_model = BertSumExtractiveSummarizer(encoder = 'baseline', \n",
    "                                        model_path = model_base_path+encoder+str(random_number),\n",
    "                                        log_file = log_base_path+encoder+str(random_number),\n",
    "                                        bert_config_path=BERT_CONFIG_PATH,\n",
    "                                        device_id = device_id,\n",
    "                                        gpu_ranks = gpu_ranks,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    bertsum_model.fit(device_id, training_data_files, train_steps=50000, train_from=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)), or Recall-Oriented Understudy for Gisting Evaluation has been commonly used for evaluation text summerization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from bertsum.models.data_loader  import DataIterator,Batch,Dataloader\n",
    "import os\n",
    "\n",
    "USE_PREPROCESSED_DATA = False\n",
    "if USE_PREPROCESSED_DATA is True: \n",
    "    test_dataset=torch.load(PROCESSED_TEST_FILE)\n",
    "else:\n",
    "    test_dataset=[]\n",
    "    for i in range(0,6):\n",
    "        filename = os.path.join(BERT_DATA_PATH, \"cnndm.test.{0}.bert.pt\".format(i))\n",
    "        test_dataset.extend(torch.load(filename))\n",
    "\n",
    "    \n",
    "def get_data_iter(dataset,is_test=False, batch_size=3000):\n",
    "    args = Bunch({})\n",
    "    args.use_interval = True\n",
    "    args.batch_size = batch_size\n",
    "    test_data_iter = None\n",
    "    test_data_iter  = DataIterator(args, dataset, args.batch_size, 'cuda', is_test=is_test, shuffle=False, sort=False)\n",
    "    return test_data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-09 02:47:36,693 INFO] Device ID 1\n",
      "[2019-10-09 02:47:36,694 INFO] Loading checkpoint from ./models/baseline0.5550666171952351/model_step_50000.pt\n",
      "[2019-10-09 02:47:38,197 INFO] * number of parameters: 5179137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 1\n",
      "gpu_rank 0\n"
     ]
    }
   ],
   "source": [
    "model_for_test = \"./models/baseline0.5550666171952351/model_step_50000.pt\"\n",
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter(test_dataset),\n",
    "                                   test_from=model_for_test,\n",
    "                                   sentence_seperator='<q>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11489"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import random, seed\n",
    "from bertsum.others.utils import test_rouge\n",
    "\n",
    "def get_rouge(predictions, targets, temp_dir):\n",
    "    def _write_list_to_file(list_items, filename):\n",
    "        with open(filename, 'w') as filehandle:\n",
    "            #for cnt, line in enumerate(filehandle):\n",
    "            for item in list_items:\n",
    "                filehandle.write('%s\\n' % item)\n",
    "    seed(42)\n",
    "    random_number = random()\n",
    "    candidate_path = os.path.join(temp_dir, \"candidate\"+str(random_number))\n",
    "    gold_path = os.path.join(temp_dir, \"gold\"+str(random_number))\n",
    "    _write_list_to_file(predictions, candidate_path)\n",
    "    _write_list_to_file(targets, gold_path)\n",
    "    rouge = test_rouge(temp_dir, candidate_path, gold_path)\n",
    "    return rouge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11489\n",
      "11489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-09 02:49:06,778 [MainThread  ] [INFO ]  Writing summaries.\n",
      "[2019-10-09 02:49:06,778 INFO] Writing summaries.\n",
      "2019-10-09 02:49:06,781 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./results/tmpaikkoju5/system and model files to ./results/tmpaikkoju5/model.\n",
      "[2019-10-09 02:49:06,781 INFO] Processing summaries. Saving system files to ./results/tmpaikkoju5/system and model files to ./results/tmpaikkoju5/model.\n",
      "2019-10-09 02:49:06,782 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-10-09-02-49-05/candidate/.\n",
      "[2019-10-09 02:49:06,782 INFO] Processing files in ./results/rouge-tmp-2019-10-09-02-49-05/candidate/.\n",
      "2019-10-09 02:49:07,979 [MainThread  ] [INFO ]  Saved processed files to ./results/tmpaikkoju5/system.\n",
      "[2019-10-09 02:49:07,979 INFO] Saved processed files to ./results/tmpaikkoju5/system.\n",
      "2019-10-09 02:49:07,981 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-10-09-02-49-05/reference/.\n",
      "[2019-10-09 02:49:07,981 INFO] Processing files in ./results/rouge-tmp-2019-10-09-02-49-05/reference/.\n",
      "2019-10-09 02:49:09,181 [MainThread  ] [INFO ]  Saved processed files to ./results/tmpaikkoju5/model.\n",
      "[2019-10-09 02:49:09,181 INFO] Saved processed files to ./results/tmpaikkoju5/model.\n",
      "2019-10-09 02:49:09,267 [MainThread  ] [INFO ]  Written ROUGE configuration to ./results/tmp8_c210t6/rouge_conf.xml\n",
      "[2019-10-09 02:49:09,267 INFO] Written ROUGE configuration to ./results/tmp8_c210t6/rouge_conf.xml\n",
      "2019-10-09 02:49:09,268 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmp8_c210t6/rouge_conf.xml\n",
      "[2019-10-09 02:49:09,268 INFO] Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmp8_c210t6/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.52805 (95%-conf.int. 0.52503 - 0.53101)\n",
      "1 ROUGE-1 Average_P: 0.34902 (95%-conf.int. 0.34673 - 0.35137)\n",
      "1 ROUGE-1 Average_F: 0.40627 (95%-conf.int. 0.40407 - 0.40856)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.23266 (95%-conf.int. 0.22986 - 0.23553)\n",
      "1 ROUGE-2 Average_P: 0.15342 (95%-conf.int. 0.15150 - 0.15543)\n",
      "1 ROUGE-2 Average_F: 0.17858 (95%-conf.int. 0.17650 - 0.18072)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.47936 (95%-conf.int. 0.47650 - 0.48238)\n",
      "1 ROUGE-L Average_P: 0.31731 (95%-conf.int. 0.31511 - 0.31957)\n",
      "1 ROUGE-L Average_F: 0.36913 (95%-conf.int. 0.36697 - 0.37144)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.eval.evaluate_summarization import get_rouge\n",
    "rouge_baseline = get_rouge(prediction, target, \"./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11489"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .he was flown back to chicago via air ambulance on march 20 , but he died on sunday .a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nlp.models.bert.extractive_text_summarization import Bunch\n",
    "args=Bunch({\"max_nsents\": int(1e5), \n",
    "            \"max_src_ntokens\": int(2e6), \n",
    "            \"min_nsents\": -1, \n",
    "            \"min_src_ntokens\": -1,  \n",
    "            \"use_interval\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro.data_builder import BertData\n",
    "bertdata = BertData(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "#sys.path.insert(0, '../src')\n",
    "from others.utils import clean\n",
    "from multiprocess import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"]=\"/home/daden/stanfordnlp_resources/stanford-corenlp-full-2018-10-05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordnlp.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from utils_nlp.models.bert.extractive_text_summarization import tokenize_to_list, bertify\n",
    "import re\n",
    "\n",
    "def preprocess_target(line):\n",
    "    def _remove_ttags(line):\n",
    "        line = re.sub(r'<t>', '', line)\n",
    "        # change </t> to <q>\n",
    "        # pyrouge test requires <q> as  sentence splitter\n",
    "        line = re.sub(r'</t>', '<q>', line)\n",
    "        return line\n",
    "\n",
    "    return tokenize_to_list(client, _remove_ttags(line))\n",
    "def preprocess_source(line):\n",
    "    return tokenize_to_list(client, clean(line))\n",
    "\n",
    "def preprocess_cnndm(param):\n",
    "    source, target = param\n",
    "    return bertify(bertdata, source, target)\n",
    "\n",
    "def harvardnlp_cnndm_standfordnlp(client, source_file, target_file, n_cpus=2, top_n=-1):\n",
    "    source_list = []\n",
    "    i = 0\n",
    "    with open(source_file) as fd:\n",
    "        for line in fd:\n",
    "            source_list.append(line)\n",
    "            i +=1\n",
    "    \n",
    "    pool = Pool(n_cpus)\n",
    "    \n",
    "\n",
    "    tokenized_source_data =  pool.map(preprocess_source, source_list[0:top_n], int(len(source_list[0:top_n])/n_cpus))\n",
    "    pool.close()\n",
    "    pool.join\n",
    "    \n",
    "    i = 0\n",
    "    target_list = []\n",
    "    with open(target_file) as fd:\n",
    "        for line in fd:\n",
    "            target_list.append(line)\n",
    "            i +=1\n",
    "\n",
    "    pool = Pool(n_cpus)\n",
    "    tokenized_target_data =  pool.map(preprocess_target, target_list[0:top_n], int(len(target_list[0:top_n])/n_cpus))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "            \n",
    "\n",
    "    #return tokenized_source_data, tokenized_target_data\n",
    "\n",
    "    pool = Pool(n_cpus)\n",
    "    bertified_data =  pool.map(preprocess_cnndm, zip(tokenized_source_data[0:top_n], tokenized_target_data[0:top_n]), int(len(tokenized_source_data[0:top_n])/n_cpus))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return bertified_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx5G -cp /home/daden/stanfordnlp_resources/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e3b74cd551ba43f1.props -preload tokenize,ssplit\n",
      "Starting server with command: java -Xmx5G -cp /home/daden/stanfordnlp_resources/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e3b74cd551ba43f1.props -preload tokenize,ssplit\n",
      "Starting server with command: java -Xmx5G -cp /home/daden/stanfordnlp_resources/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e3b74cd551ba43f1.props -preload tokenize,ssplit\n",
      "Starting server with command: java -Xmx5G -cp /home/daden/stanfordnlp_resources/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e3b74cd551ba43f1.props -preload tokenize,ssplit\n",
      "CPU times: user 79.8 ms, sys: 85.4 ms, total: 165 ms\n",
      "Wall time: 7.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "source_file = './harvardnlp_cnndm/test.txt.src'\n",
    "target_file = './harvardnlp_cnndm/test.txt.tgt.tagged'\n",
    "client = CoreNLPClient(annotators=['tokenize','ssplit'])\n",
    "new_data = harvardnlp_cnndm_standfordnlp(client, source_file, target_file, n_cpus=2, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.data_loader  import DataIterator,Batch,Dataloader\n",
    "import os\n",
    "\n",
    "USE_PREPROCESSED_DATA = False\n",
    "if USE_PREPROCESSED_DATA is True: \n",
    "    test_dataset=torch.load(PROCESSED_TEST_FILE)\n",
    "else:\n",
    "    test_dataset=[]\n",
    "    for i in range(0,6):\n",
    "        filename = os.path.join(BERT_DATA_PATH, \"test/cnndm.test.{0}.bert.pt\".format(i))\n",
    "        test_dataset.extend(torch.load(filename))\n",
    "def get_data_iter(dataset,is_test=False, batch_size=3000):\n",
    "    args = Bunch({})\n",
    "    args.use_interval = True\n",
    "    args.batch_size = batch_size\n",
    "    test_data_iter = None\n",
    "    test_data_iter  = DataIterator(args, dataset, args.batch_size, 'cuda', is_test=is_test, shuffle=False, sort=False)\n",
    "    return test_data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_src = preprocess_source(\"\".join(test_dataset[0]['src_txt']))\n",
    "b_data = bertdata.preprocess(new_src, None, None)\n",
    "indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "b_data_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "               'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .',\n",
       " 'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robberyhe was taken to a medical facility in the chicago area , close to his family home in glen ellyn .',\n",
       " \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .',\n",
       " \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed ` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .',\n",
       " '` he was in a coma and in critical condition for months .',\n",
       " \"' paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .\",\n",
       " 'she said he was was alone at the time of the alleged assault and personal items were stolen .',\n",
       " 'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .',\n",
       " 'mogni was a third-year finance major from glen ellyn , ill .',\n",
       " ', who was participating in a semester-long program at john cabot university .',\n",
       " \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni .\",\n",
       " \"' the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_data_dict['src_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_data_dict['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-10-07 03:28:55,446 INFO] Device ID 1\n",
      "[2019-10-07 03:28:55,452 INFO] Loading checkpoint from ./models/baseline0.14344633695274556/model_step_30000.pt\n",
      "[2019-10-07 03:29:01,758 INFO] * number of parameters: 5179137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id 1\n",
      "gpu_rank 0\n"
     ]
    }
   ],
   "source": [
    "model_for_test = \"./models/baseline0.14344633695274556/model_step_30000.pt\"\n",
    "#get_data_iter(output,batch_size=30000)\n",
    "prediction = bertsum_model.predict(device_id, get_data_iter([b_data_dict], False),\n",
    "                                   test_from=model_for_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .he was flown back to chicago via air ambulance on march 20 , but he died on sunday .a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6 cm3",
   "language": "python",
   "name": "cm3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
