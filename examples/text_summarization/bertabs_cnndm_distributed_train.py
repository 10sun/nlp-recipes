# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.

import argparse
import os
import sys
import time
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
#torch.set_printoptions(threshold=5000)
from tempfile import TemporaryDirectory

nlp_path = os.path.abspath("../../")
if nlp_path not in sys.path:
    sys.path.insert(0, nlp_path)


from utils_nlp.models.transformers.abssum import AbsSum, AbsSumProcessor, validate
from utils_nlp.models.transformers.bertabs import model_builder

from utils_nlp.dataset.cnndm import CNNDMBertSumProcessedData, CNNDMSummarizationDataset
from utils_nlp.models.transformers.datasets import SummarizationNonIterableDataset
from utils_nlp.eval.evaluate_summarization import get_rouge

os.environ["NCCL_IB_DISABLE"] = "0"

def shorten_dataset(dataset, top_n=-1):
    if top_n == -1:
        return dataset
    return SummarizationNonIterableDataset(dataset.source[0:top_n], dataset.target[0:top_n])

parser = argparse.ArgumentParser()
parser.add_argument("--rank", type=int, default=0,
                    help="The rank of the current node in the cluster")
parser.add_argument("--dist_url", type=str, default="tcp://127.0.0.1:29500",
                    help="URL specifying how to initialize the process groupi.")
parser.add_argument("--node_count", type=int, default=1,
                    help="Number of nodes in the cluster.")

parser.add_argument("--cache_dir", type=str, default="./abstemp",
                    help="Directory to cache the tokenizer.")
parser.add_argument("--data_dir", type=str, default="./",
                    help="Directory to download the preprocessed data.")
parser.add_argument("--output_dir", type=str, default="./abstemp",
                    help="Directory to save the output model and prediction results.")
parser.add_argument("--quick_run", type=str.lower, default='false', choices=['true', 'false'],
                    help="Whether to have a quick run")
parser.add_argument("--model_name", type=str, default="bert-base-uncased",
                    help="Transformer model used in the summarization model, only \
                        \"bert-uncased\" is supported so far.")
parser.add_argument("--lr_bert", type=float, default=2e-3,
                    help="Learning rate for the BERT encoder.")
parser.add_argument("--lr_dec", type=float, default=2e-1,
                    help="Learning rate for the decoder.")
parser.add_argument("--batch_size", type=int, default=5,
                    help="batch size in terms of input token numbers in training")
parser.add_argument("--max_steps", type=int, default=5e4,
                    help="Maximum number of training steps run in training. If quick_run is set,\
                        it's not used.")
parser.add_argument("--warmup_steps_bert", type=int, default=2e4,
                    help="Warm-up number of training steps run in training for the encoder. If quick_run is set,\
                        it's not used.")
parser.add_argument("--warmup_steps_dec", type=int, default=1e4,
                    help="Warm-up number of training steps run in training for the decoder. If quick_run is set,\
                        it's not used.")
parser.add_argument("--summary_filename", type=str, default="generated_summaries.txt", 
                    help="Summary file name generated by prediction for evaluation.")
parser.add_argument("--model_filename", type=str, default="dist_extsum_model.pt", 
                    help="model file name saved for evaluation.")


def pretrained_model():
    return torch.load(os.path.join(MODEL_PATH, "model_step_148000_torch1.4.0.pt"))

def load_processed_cnndm_abs(data_path, 
        train_file= "train_abssum_dataset_full.pt", 
        test_file="test_abssum_dataset_full.pt" ):
    #TOP_N = -1
    train_data_path = os.path.join(data_path, train_file)
    test_data_path = os.path.join(data_path, test_file)
    train_sum_dataset = torch.load(train_data_path)
    test_sum_dataset = torch.load(test_data_path)
    return train_sum_dataset, test_sum_dataset


def main():

    args = parser.parse_args()

    print("NCCL_IB_DISABLE: {}".format(os.getenv("NCCL_IB_DISABLE")))
    print("quick_run is {}".format(args.quick_run))
    print("output_dir is {}".format(args.output_dir))
    print("data_dir is {}".format(args.data_dir))
    print("cache_dir is {}".format(args.cache_dir))

    ngpus_per_node = torch.cuda.device_count()
    processor = AbsSumProcessor(cache_dir=args.cache_dir)
    summarizer = AbsSum(
        processor, cache_dir=args.cache_dir
    )
    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, summarizer,  args))


def main_worker(local_rank, ngpus_per_node, summarizer, args):
    rank = args.rank * ngpus_per_node + local_rank
    world_size = args.node_count * ngpus_per_node
    print("world_size is {}".format(world_size))
    print("local_rank is {} and rank is {}".format(local_rank, rank))
    
    
    torch.distributed.init_process_group(
        backend="nccl",
        init_method=args.dist_url,
        world_size=world_size,
        rank=rank,
      )

    checkpoint = None
    train_sum_dataset, test_sum_dataset = load_processed_cnndm_abs(args.data_dir)
    def this_validate(class_obj):
        return validate(class_obj, test_sum_dataset, args.cache_dir)

    if rank not in [-1, 0]:
        save_every = -1
        this_validate = None
    else:
        save_every = 400

    # total number of steps for training
    MAX_STEPS = 400
    # number of steps for warm up
    WARMUP_STEPS_BERT = MAX_STEPS
    WARMUP_STEPS_DEC = MAX_STEPS
    if args.quick_run.lower() == "false":
        MAX_STEPS = args.max_steps
        WARMUP_STEPS_BERT = args.warmup_steps_bert
        WARMUP_STEPS_DEC= args.warmup_steps_dec 

    print("max steps is {}".format(MAX_STEPS))
    print("warmup steps for encoder bert is {}".format(WARMUP_STEPS_BERT))
    print("warmup steps for decoder is {}".format(WARMUP_STEPS_DEC))
    start = time.time()

    #summarizer.model.load_checkpoint(checkpoint['model'])
    summarizer.fit(
        train_sum_dataset,
        world_size=world_size,
        num_gpus=None,
        local_rank=local_rank,
        rank=rank,
        batch_size=args.batch_size,
        max_steps=MAX_STEPS/world_size,
        learning_rate_bert=args.lr_bert,
        learning_rate_dec=args.lr_dec,
        warmup_steps_bert=WARMUP_STEPS_BERT,
        warmup_steps_dec=WARMUP_STEPS_DEC,
        save_every=save_every,
        report_every=10,
        validation_function=this_validate,
        fp16=True,
        fp16_opt_level="O2",
        checkpoint=None
    )

    end = time.time()
    print("rank {0}, duration {1:.6f}s".format(rank, end - start))
    if rank == 0 or local_rank == -1:
        saved_model_path = os.path.join(args.output_dir, "summarizer_step{}_with_glocal_step.pt".format(MAX_STEPS))
        summarizer.save_model(MAX_STEPS, saved_model_path)  
        top_n = 8
        prediction = summarizer.predict(shorten_dataset(test_sum_dataset, top_n=top_n),
                batch_size=4, num_gpus=ngpus_per_node)
        print(prediction[0])
        def _write_list_to_file(list_items, filename):
            with open(filename, "w") as filehandle:
                # for cnt, line in enumerate(filehandle):
                for item in list_items:
                    filehandle.write("%s\n" % item)

        print("writing generated summaries")
        _write_list_to_file(prediction, os.path.join(args.output_dir, args.summary_filename))

    # only use the following line when you use your own cluster. 
    # AML distributed training run cleanup for you.
    dist.destroy_process_group()

if __name__ == "__main__":
    main()
