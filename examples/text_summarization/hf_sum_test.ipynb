{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 20:09:51.090050 140687649494848 file_utils.py:35] PyTorch version 1.3.0 available.\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/python3\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "#\n",
    "from transformers import BertTokenizer\n",
    "#from transformers import BertAbs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"/dadendev/transformers/examples/summarization\")\n",
    "from modeling_bertabs import BertAbs, build_predictor\n",
    "\n",
    "from utils_summarization import (\n",
    "    #SummarizationDataset,\n",
    "    build_mask,\n",
    "    compute_token_type_ids,\n",
    "    encode_for_summarization,\n",
    "    fit_to_block_size,\n",
    ")\n",
    "from run_summarization import format_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 20:09:52.932581 140687649494848 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/daden/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0108 20:09:53.102754 140687649494848 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization-config.json from cache at /home/daden/.cache/torch/transformers/7ebb4ac81007d10b400cb6c2968d4c8f1275a3e0cc3bab7f20f81913198b542c.df616398f4c84def6fca83d755543b01cb445db4ddd218d3efeded8ded68332f\n",
      "I0108 20:09:53.103896 140687649494848 configuration_utils.py:199] Model config {\n",
      "  \"dec_dropout\": 0.2,\n",
      "  \"dec_ff_size\": 2048,\n",
      "  \"dec_heads\": 8,\n",
      "  \"dec_hidden_size\": 768,\n",
      "  \"dec_layers\": 6,\n",
      "  \"enc_dropout\": 0.2,\n",
      "  \"enc_ff_size\": 512,\n",
      "  \"enc_heads\": 8,\n",
      "  \"enc_hidden_size\": 512,\n",
      "  \"enc_layers\": 6,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"max_pos\": 512,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0108 20:09:53.243465 140687649494848 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization-pytorch_model.bin from cache at /home/daden/.cache/torch/transformers/6f1af625ee57a9fbf093ef0863fb774fbdae89fa99fea7a213c08ad26f0724c0.ef06f4d767c6fad3c61125520f9dbb0f219834539c0369980ee5ecb9d1ef5542\n",
      "I0108 20:09:53.427680 140687649494848 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/daden/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I0108 20:09:53.428531 140687649494848 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "model = BertAbs.from_pretrained(\"bertabs-finetuned-cnndm\")\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "symbols = {\n",
    "    \"BOS\": tokenizer.vocab[\"[unused0]\"],\n",
    "    \"EOS\": tokenizer.vocab[\"[unused1]\"],\n",
    "    \"PAD\": tokenizer.vocab[\"[PAD]\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_path = os.path.abspath(\"../../\")\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "from utils_nlp.models.transformers.extractive_summarization import Bunch\n",
    "args = Bunch({\"block_trigram\":True, \"alpha\": 0.95, \"beam_size\": 5, \"min_length\": 20, \"max_length\": 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = build_predictor(args, tokenizer, symbols, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "import torch\n",
    "\n",
    "nlp_path = os.path.abspath(\"../../\")\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "\n",
    "from utils_nlp.common.pytorch_utils import get_device\n",
    "from utils_nlp.dataset.cnndm import CNNDMBertSumProcessedData, CNNDMSummarizationDataset\n",
    "from utils_nlp.eval.evaluate_summarization import get_rouge\n",
    "from utils_nlp.models.transformers.extractive_summarization import (\n",
    "    ExtractiveSummarizer,\n",
    "    ExtSumProcessedData,\n",
    "    ExtSumProcessor,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scrapbook as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils_nlp.models.transformers.datasets import SummarizationDataset\n",
    "from utils_nlp.dataset.cnndm import CNNDMAbsSumDataset, CNNDMSummarizationDataset\n",
    "#def build_data_iterator(args, tokenizer):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/tmp/tmpsh6mbj3g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK_RUN = True\n",
    "# the data path used to save the downloaded data file\n",
    "# DATA_PATH = TemporaryDirectory().name\n",
    "# The number of lines at the head of data file used for preprocessing. -1 means all the lines.\n",
    "TOP_N = 4\n",
    "CHUNK_SIZE=200\n",
    "if not QUICK_RUN:\n",
    "    TOP_N = -1\n",
    "    CHUNK_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, source, target=None):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 20:10:04.784991 140687649494848 utils.py:173] Opening tar file /tmp/tmpsh6mbj3g/cnndm.tar.gz.\n",
      "I0108 20:10:04.786256 140687649494848 utils.py:181] /tmp/tmpsh6mbj3g/test.txt.src already extracted.\n",
      "I0108 20:10:05.076648 140687649494848 utils.py:181] /tmp/tmpsh6mbj3g/test.txt.tgt.tagged already extracted.\n",
      "I0108 20:10:05.103137 140687649494848 utils.py:181] /tmp/tmpsh6mbj3g/train.txt.src already extracted.\n",
      "I0108 20:10:12.616743 140687649494848 utils.py:181] /tmp/tmpsh6mbj3g/train.txt.tgt.tagged already extracted.\n",
      "I0108 20:10:13.232445 140687649494848 utils.py:181] /tmp/tmpsh6mbj3g/val.txt.src already extracted.\n",
      "I0108 20:10:13.578413 140687649494848 utils.py:181] /tmp/tmpsh6mbj3g/val.txt.tgt.tagged already extracted.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = CNNDMAbsSumDataset(top_n=TOP_N, local_cache_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(test_dataset.get_source()), list(test_dataset.get_target())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sum_dataset = SummarizationDataset(data[0], data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainBatch = namedtuple(\"Batch\", [ \"batch_size\", \"src\", \"segs\", \"mask_src\", \"tgt\", \"tgt_segs\", \"mask_tgt\", \"tgt_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data, tokenizer, block_size, device):\n",
    "    \"\"\" Collate formats the data passed to the data loader.\n",
    "    In particular we tokenize the data batch after batch to avoid keeping them\n",
    "    all in memory. We output the data as a namedtuple to fit the original BertAbs's\n",
    "    API.\n",
    "    \"\"\"\n",
    "    data = [x for x in data if not len(x[1]) == 0]  # remove empty_files\n",
    "    #print(data)\n",
    "    #names = [name for name, _, _ in data]\n",
    "    # summaries = [\" \".join(summary_list) for _, _, summary_list in data]\n",
    "    summaries = [\" \".join(summary_list) for _, summary_list in data]\n",
    "  \n",
    "\n",
    "    encoded_text = [encode_for_summarization(story, summary, tokenizer) for story, summary in data]\n",
    "    \n",
    "    \n",
    "    #\"\"\"\"\"\"\n",
    "    encoded_stories = torch.tensor(\n",
    "        [fit_to_block_size(story, block_size, tokenizer.pad_token_id) for story, _ in encoded_text]\n",
    "    )\n",
    "    encoder_token_type_ids = compute_token_type_ids(encoded_stories, tokenizer.cls_token_id)\n",
    "    encoder_mask = build_mask(encoded_stories, tokenizer.pad_token_id)\n",
    "    #\"\"\"\n",
    "    print(len(encoded_stories))\n",
    "\n",
    "    batch = Batch(\n",
    "        #document_names=None,\n",
    "        batch_size=len(encoded_stories),\n",
    "        src=encoded_stories.to(device),\n",
    "        segs=encoder_token_type_ids.to(device),\n",
    "        mask_src=encoder_mask.to(device),\n",
    "        tgt_str=summaries,\n",
    "    )\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_for_summarization(story_lines, summary_lines, tokenizer, max_len=512):\n",
    "    \"\"\" Encode the story and summary lines, and join them\n",
    "    as specified in [1] by using `[SEP] [CLS]` tokens to separate\n",
    "    sentences.\n",
    "    \"\"\"\n",
    "    story_lines_token_ids = [tokenizer.encode(line, max_length=max_len) for line in story_lines]\n",
    "    story_token_ids = [token for sentence in story_lines_token_ids for token in sentence]\n",
    "    summary_lines_token_ids = [tokenizer.encode(line, max_length=max_len) for line in summary_lines]\n",
    "    summary_token_ids = [token for sentence in summary_lines_token_ids for token in sentence]\n",
    "\n",
    "    return story_token_ids, summary_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_iterator(dataset, tokenizer, batch_size=16, device='cuda'):\n",
    "   \n",
    "    sampler = SequentialSampler(dataset)\n",
    "\n",
    "    def collate_fn(data):\n",
    "        return collate(data, tokenizer, block_size=512, device=device)\n",
    "\n",
    "    iterator = DataLoader(dataset, sampler=sampler, batch_size=batch_size, collate_fn=collate_fn,)\n",
    "\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nlp.common.pytorch_utils import get_device\n",
    "device, num_gpus = get_device(num_gpus=4, local_rank=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = build_data_iterator(test_sum_dataset, tokenizer, batch_size=64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summaries = []\n",
    "generated_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(data_iterator):\n",
    "    \n",
    "    batch_data = predictor.translate_batch(batch)\n",
    "    translations = predictor.from_batch(batch_data)\n",
    "    summaries = [format_summary(t) for t in translations]\n",
    "    #save_summaries(summaries, args.summaries_output_dir, batch.document_names)\n",
    "\n",
    "    if True:\n",
    "        reference_summaries += batch.tgt_str\n",
    "        generated_summaries += summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prosecutor brice robin : \" so far no videos were used in the crash investigation \". robin \\'s comments follow claims by two magazines , german daily bild and french paris match. all 150 on board germanwings flight 9525 were killed'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' marseille prosecutor says \" so far no videos were used in the crash investigation \" despite media reports . <q>  journalists at bild and paris match are \" very confident \" the video clip is real , an editor says . <q>  andreas lubitz had informed his lufthansa training school of an episode of severe depression , airline says . <q>\\n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summaries = []\n",
    "for i in data[1]:\n",
    "    reference_summaries.append(i[0].replace('<q>', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def _write_list_to_file(list_items, filename):\n",
    "        with open(filename, \"w\") as filehandle:\n",
    "            # for cnt, line in enumerate(filehandle):\n",
    "            for item in list_items:\n",
    "                filehandle.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_write_list_to_file(generated_summaries, \"./generated_summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summaries = []\n",
    "with open(\"./generated_summaries\", \"r\") as filehandle:\n",
    "    for cnt, line in enumerate(filehandle):\n",
    "        generated_summaries.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11490"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prosecutor brice robin : \" so far no videos were used in the crash investigation \". robin \\'s comments follow claims by two magazines , german daily bild and french paris match. all 150 on board germanwings flight 9525 were killed\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' marseille prosecutor says \" so far no videos were used in the crash investigation \" despite media reports .   journalists at bild and paris match are \" very confident \" the video clip is real , an editor says .   andreas lubitz had informed his lufthansa training school of an episode of severe depression , airline says . \\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import rouge\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "rouge_evaluator = rouge.Rouge(\n",
    "    metrics=[\"rouge-n\", \"rouge-l\"],\n",
    "    max_n=2,\n",
    "    limit_length=True,\n",
    "    length_limit=args.beam_size,\n",
    "    length_limit_type=\"words\",\n",
    "    apply_avg=True,\n",
    "    apply_best=False,\n",
    "    alpha=0.5,  # Default F1_score\n",
    "    weight_factor=1.2,\n",
    "    stemming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores = rouge_evaluator.get_scores(generated_summaries, reference_summaries)\n",
    "#str_scores = format_rouge_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6 cm3",
   "language": "python",
   "name": "cm3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
