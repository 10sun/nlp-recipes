{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Summarization on CNN/DM Dataset using Transformer Version of BertSum\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook demonstrates how to fine tune Transformers for extractive text summarization. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, result postprocessing, and model evaluation.\n",
    "\n",
    "BertSum refers to  [Fine-tune BERT for Extractive Summarization (https://arxiv.org/pdf/1903.10318.pdf) with [published example](https://github.com/nlpyang/BertSum/). And the Transformer version of Bertsum refers to our modification of BertSum and the source code can be accessed at (https://github.com/daden-ms/BertSum/). \n",
    "\n",
    "Extractive summarization are usually used in document summarization where each input document consists of mutiple sentences. The preprocessing of the input training data involves assigning label 0 or 1 to the document sentences based on the give summary. The summarization problem is also simplfied to classifying whether each document sentence should be included in the summary. \n",
    "\n",
    "The figure below illustrates how BERTSum can be fine tuned for extractive summarization task. Each sentence is inserted with [CLS] token at the beginning and  [SEP] at the end. Interval segment embedding and positional embedding are added upon the token embedding before input the BERT model. The [CLS] token representation is used as sentence embedding and only the [CLS] tokens are used as input for the summarization model. The summarization layer predicts whether the probability of each each sentence token should be included in the summary or not. Techniques like trigram blocking can be used to improve model accuarcy.   \n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/BertSum.PNG\">\n",
    "\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "The running time shown in this notebook is on a Standard_NC24s_v3 Azure Deep Learning Virtual Machine with 4 NVIDIA Tesla V100 GPUs. \n",
    "> **Tip**: If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`** to run the notebook on a small subset of the data and a smaller number of epochs. \n",
    "\n",
    "The table below provides some reference running time on different machine configurations.  \n",
    "\n",
    "|QUICK_RUN|USE_PREPROCESSED_DATA|encoder|Machine Configurations|Running time|\n",
    "|:---------|:---------|:---------|:----------------------|:------------|\n",
    "|True|True|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 20 minutes |\n",
    "|False|True|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 60 minutes |\n",
    "|True|False|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 20 minutes |\n",
    "|True|True|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 80 minutes |\n",
    "|False|True|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 6.5hours |\n",
    "|True|False|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 80 minutes |\n",
    "|False|False|any| 1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| > 24 hours|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = True\n",
    "USE_PREPROCESSED_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Before we start the notebook, we should set the environment variable to make sure you can access the GPUs on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "I1210 21:34:47.924211 140261726336832 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1210 21:34:47.957383 140261726336832 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1210 21:34:47.963832 140261726336832 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "nlp_path = os.path.abspath(\"../../\")\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "sys.path.insert(0, \"./\")\n",
    "sys.path.insert(0, \"/dadendev/nlp/examples/text_summarization/BertSum/\")\n",
    "\n",
    "from utils_nlp.common.pytorch_utils import get_device\n",
    "from utils_nlp.dataset.cnndm import CNNDMBertSumProcessedData, CNNDMSummarization\n",
    "from utils_nlp.eval.evaluate_summarization import get_rouge\n",
    "from utils_nlp.models.transformers.extractive_summarization import (\n",
    "    get_dataloader,\n",
    "    get_data_iter,\n",
    "    ExtractiveSummarizer,\n",
    "    ExtSumProcessor,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to install the dependencies for pyrouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://azure.archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:2 http://azure.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:3 http://azure.archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Hit:4 https://packages.microsoft.com/repos/microsoft-ubuntu-xenial-prod xenial InRelease\n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \n",
      "Ign:6 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:7 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Fetched 252 kB in 1s (392 kB/s)                              \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "expat is already the newest version (2.2.5-3ubuntu0.2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  linux-azure-cloud-tools-5.0.0-1018 linux-azure-cloud-tools-5.0.0-1020\n",
      "  linux-azure-headers-5.0.0-1018 linux-azure-tools-5.0.0-1018\n",
      "  linux-azure-tools-5.0.0-1020\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Note, selecting 'libexpat1-dev' instead of 'libexpat-dev'\n",
      "libexpat1-dev is already the newest version (2.2.5-3ubuntu0.2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  linux-azure-cloud-tools-5.0.0-1018 linux-azure-cloud-tools-5.0.0-1020\n",
      "  linux-azure-headers-5.0.0-1018 linux-azure-tools-5.0.0-1018\n",
      "  linux-azure-tools-5.0.0-1020\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# dependencies for ROUGE-1.5.5.pl\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install expat\n",
    "!sudo apt-get install libexpat-dev -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in your terminal to install pre-requiste for using pyrouge.\n",
    "1. sudo cpan install XML::Parser\n",
    "1. sudo cpan install XML::Parser::PerlSAX\n",
    "1. sudo cpan install XML::DOM\n",
    "\n",
    "Download ROUGE-1.5.5 from https://github.com/andersjo/pyrouge/tree/master/tools/ROUGE-1.5.5.\n",
    "Run the following command in your terminal.\n",
    "* pyrouge_set_rouge_path $ABSOLUTE_DIRECTORY_TO_ROUGE-1.5.5.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprossing\n",
    "\n",
    "The dataset we used for this notebook is CNN/DM dataset which contains the documents and accompanying questions from the news articles of CNN and Daily mail. The highlights in each article are used as summary. The dataset consits of ~289K training examples, ~11K valiation and ~11K test dataset.  You can choose the [Option 1] below preprocess the data or [Option 2] to use the preprocessed version at [BERTSum published example](https://github.com/nlpyang/BertSum/). You don't need to manually download any of these two data sets as the code below will handle this part.  Since it takes up to 28 hours to preprocess the training data  to run on 10  Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz, we suggest you continue with set as True first and experiment with data preprocessing  with QUICKRUN set as True.\n",
    "\n",
    "##### Details of Data Preprocessing\n",
    "\n",
    "The purpose of preprocessing is to process the input articles to the format that BertSum takes.  Functions defined specific in harvardnlp_cnndm_preprocess function are unique to CNN/DM dataset that's processed by harvardnlp. However, it provides a skeleton of how to preprocessing data into the format that BertSum takes. Assuming you have all articles and target summery each in a file, line-breaker seperated, the steps to preprocess the data are:\n",
    "1. sentence tokenization\n",
    "2. word tokenization\n",
    "3. **label** the sentences in the article with 1 meaning the sentence is selected and 0 meaning the sentence is not selected. The options for the selection algorithms are \"greedy\" and \"combination\"\n",
    "3. convert each example to  BertSum format\n",
    "    - filter the sentences in the example based on the min_src_ntokens argument. If the lefted total sentence number is less than min_nsents, the example is discarded.\n",
    "    - truncate the sentences in the example if the length is greater than max_src_ntokens\n",
    "    - truncate the sentences in the example and the labels if the totle number of sentences is greater than max_nsents\n",
    "    - [CLS] and [SEP] are inserted before and after each sentence\n",
    "    - wordPiece tokenization\n",
    "    - truncate the example to 512 tokens\n",
    "    - convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary.\n",
    "    - segment ids are generated\n",
    "    - [CLS] token positions are logged\n",
    "    - [CLS] token labels are truncated if it's greater than 512, which is the maximum input length that can be taken by the BERT model.\n",
    "    \n",
    "    \n",
    "Note that the original BERTSum paper use Stanford CoreNLP for data proprocessing, here we'll first how to use NLTK version, and then we also provide instruction of how to set up Stanford NLP and code examples of how to use Standford CoreNLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Option 1] Preprocess  data\n",
    "The code in following cell will download the CNN/DM dataset listed at https://github.com/harvardnlp/sent-summary/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "I1210 18:30:56.902205 139830036641600 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1210 18:30:56.936184 139830036641600 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1210 18:30:56.943920 139830036641600 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1210 18:30:56.953987 139830036641600 utils.py:173] Opening tar file .data/cnndm.tar.gz.\n",
      "I1210 18:30:56.965388 139830036641600 utils.py:181] .data/test.txt.src already extracted.\n",
      "I1210 18:30:57.394748 139830036641600 utils.py:181] .data/test.txt.tgt.tagged already extracted.\n",
      "I1210 18:30:57.430505 139830036641600 utils.py:181] .data/train.txt.src already extracted.\n",
      "I1210 18:31:08.232069 139830036641600 utils.py:181] .data/train.txt.tgt.tagged already extracted.\n",
      "I1210 18:31:09.120292 139830036641600 utils.py:181] .data/val.txt.src already extracted.\n",
      "I1210 18:31:09.601161 139830036641600 utils.py:181] .data/val.txt.tgt.tagged already extracted.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = CNNDMSummarization(top_n=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data and save the data to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1206 16:51:11.971997 140019998881600 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpjzna7ttv\n",
      "100%|██████████| 231508/231508 [00:00<00:00, 1963987.24B/s]\n",
      "I1206 16:51:12.252732 140019998881600 file_utils.py:309] copying /tmp/tmpjzna7ttv to cache at ./26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1206 16:51:12.253886 140019998881600 file_utils.py:313] creating metadata file for ./26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I1206 16:51:12.255181 140019998881600 file_utils.py:322] removing temp file /tmp/tmpjzna7ttv\n",
      "I1206 16:51:12.255890 140019998881600 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "processor = ExtSumProcessor(model_name=\"distilbert-base-uncased\")\n",
    "ext_sum_train = processor.preprocess(train_dataset, train_dataset.get_target())\n",
    "ext_sum_test = processor.preprocess(test_dataset, test_dataset.get_target())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./temp_data4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "I1206 21:27:24.938656 139683323610944 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1206 21:27:24.971345 139683323610944 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1206 21:27:24.978915 139683323610944 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ext_sum_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aa82ff9053cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m train_files = CNNDMBertSumProcessedData.save_data(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mext_sum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_and_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m test_files = CNNDMBertSumProcessedData.save_data(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ext_sum_train' is not defined"
     ]
    }
   ],
   "source": [
    "train_files = CNNDMBertSumProcessedData.save_data(\n",
    "    ext_sum_train, is_test=False, path_and_prefix=data_path, chunk_size=25\n",
    ")\n",
    "test_files = CNNDMBertSumProcessedData.save_data(\n",
    "    ext_sum_test, is_test=True, path_and_prefix=data_path, chunk_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./temp_data4/_0_train',\n",
       " './temp_data4/_1_train',\n",
       " './temp_data4/_2_train',\n",
       " './temp_data4/_3_train']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./temp_data4/_0_test']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = CNNDMBertSumProcessedData().splits(root=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['src', 'labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "bert_format_data = torch.load(train_files[0])\n",
    "print(len(bert_format_data))\n",
    "bert_format_data[0].keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Option 2] Reuse Preprocessed  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PREPROCESSED_DATA:\n",
    "    data_path = \"./temp_data5/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    #CNNDMBertSumProcessedData.download(local_path=data_path)\n",
    "    train_iter, test_iter = CNNDMBertSumProcessedData().splits(root=data_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "To start model training, we need to create a instance of ExtractiveSummarizer.\n",
    "#### Choose the transformer model.\n",
    "Currently ExtractiveSummarizer support two models:\n",
    "- distilbert-base-uncase, \n",
    "- bert-base-uncase\n",
    "\n",
    "Potentionally, roberta-based model and xlnet can be supported but needs to be tested.\n",
    "#### Choose the encoder algorithm.\n",
    "There are four options:\n",
    "- baseline: it used a smaller transformer model to replace the bert model and with transformer summarization layer\n",
    "- classifier: it uses pretrained BERT and fine-tune BERT with **simple logistic classification** summarization layer\n",
    "- transformer: it uses pretrained BERT and fine-tune BERT with **transformer** summarization layer\n",
    "- RNN: it uses pretrained BERT and fine-tune BERT with **LSTM** summarization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "DATA_FOLDER = \"./temp\"\n",
    "CACHE_DIR = \"./temp\"\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 3000\n",
    "NUM_GPUS = 4\n",
    "encoder = \"transformer\"\n",
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 21:49:41.613120 140261726336832 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at ./temp/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I1210 21:49:41.615161 140261726336832 configuration_utils.py:168] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 0,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1210 21:49:41.755493 140261726336832 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin from cache at ./temp/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
      "I1210 21:49:42.957785 140261726336832 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at ./temp/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I1210 21:49:42.960213 140261726336832 configuration_utils.py:168] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1210 21:49:43.104863 140261726336832 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin from cache at ./temp/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n"
     ]
    }
   ],
   "source": [
    "summarizer = ExtractiveSummarizer(model_name, encoder, CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size is the number of tokens in a batch\n",
    "train_dataloader = get_dataloader(train_iter(), is_labeled=True, batch_size=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device, num_gpus = get_device(num_gpus=4, local_rank=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ffc3eaf4289a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepoch_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "epoch_iterator = tqdm(train_data_iterator)\n",
    "i = 0\n",
    "for step, batch in enumerate(epoch_iterator):\n",
    "    batch = batch.to(device)\n",
    "    i += 1\n",
    "    print(i)\n",
    "    if i > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.858079, time: 47.937159, number of examples: 5, step 100 out of total 50000\n",
      "loss: 5.761587, time: 36.250811, number of examples: 5, step 200 out of total 50000\n",
      "loss: 5.445527, time: 36.161208, number of examples: 5, step 300 out of total 50000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a11bcb7858c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mreport_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         )\n",
      "\u001b[0;32m/dadendev/nlp/utils_nlp/models/transformers/extractive_summarization.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataloader, num_gpus, local_rank, max_steps, optimization_method, lr, max_grad_norm, beta1, beta2, decay_method, warmup_steps, verbose, seed, gradient_accumulation_steps, report_every, clip_grad_norm, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mreport_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_every\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         )\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/nlp/utils_nlp/models/transformers/common.py\u001b[0m in \u001b[0;36mfine_tune\u001b[0;34m(self, train_dataloader, get_inputs, device, max_steps, num_train_epochs, max_grad_norm, gradient_accumulation_steps, n_gpu, move_batch_to_device, optimizer, scheduler, weight_decay, learning_rate, adam_epsilon, warmup_steps, fp16, fp16_opt_level, local_rank, verbose, seed, report_every, clip_grad_norm)\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmove_batch_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/anaconda3/envs/cm3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dadendev/anaconda3/envs/cm3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "summarizer.fit(\n",
    "            train_dataloader,\n",
    "            num_gpus=4,\n",
    "            gradient_accumulation_steps=2,\n",
    "            max_steps=5e4,\n",
    "            lr=2e-3,\n",
    "            warmup_steps=1e4*0.5,\n",
    "            verbose=True,\n",
    "            report_every=200,\n",
    "            clip_grad_norm=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "loss: 41.687942, time: 10.571247, number of examples: 5, step 100 out of total 50000\n",
      "loss: 27.198075, time: 10.414026, number of examples: 5, step 200 out of total 50000\n",
      "loss: 19.797057, time: 10.415758, number of examples: 5, step 300 out of total 50000\n",
      "loss: 18.425389, time: 10.781584, number of examples: 6, step 400 out of total 50000\n",
      "loss: 18.224794, time: 10.541868, number of examples: 5, step 500 out of total 50000\n",
      "loss: 17.613259, time: 10.589073, number of examples: 5, step 600 out of total 50000\n",
      "loss: 17.301366, time: 10.407273, number of examples: 5, step 700 out of total 50000\n",
      "loss: 17.202934, time: 10.942506, number of examples: 5, step 800 out of total 50000\n",
      "loss: 16.892723, time: 10.530933, number of examples: 5, step 900 out of total 50000\n",
      "loss: 16.507498, time: 10.463890, number of examples: 5, step 1000 out of total 50000\n",
      "loss: 16.465361, time: 10.396509, number of examples: 5, step 1100 out of total 50000\n",
      "loss: 16.058329, time: 11.084911, number of examples: 5, step 1200 out of total 50000\n",
      "loss: 16.257821, time: 10.393638, number of examples: 5, step 1300 out of total 50000\n",
      "loss: 16.388095, time: 10.345373, number of examples: 5, step 1400 out of total 50000\n",
      "loss: 16.286023, time: 10.365130, number of examples: 5, step 1500 out of total 50000\n",
      "loss: 16.051502, time: 10.817498, number of examples: 5, step 1600 out of total 50000\n",
      "loss: 16.168371, time: 10.377886, number of examples: 5, step 1700 out of total 50000\n",
      "loss: 15.836932, time: 10.311240, number of examples: 5, step 1800 out of total 50000\n",
      "loss: 15.930409, time: 10.416386, number of examples: 5, step 1900 out of total 50000\n",
      "loss: 16.144058, time: 10.919861, number of examples: 5, step 2000 out of total 50000\n",
      "loss: 15.958050, time: 10.503907, number of examples: 5, step 2100 out of total 50000\n",
      "loss: 15.989393, time: 10.336605, number of examples: 5, step 2200 out of total 50000\n",
      "loss: 16.184670, time: 10.369090, number of examples: 5, step 2300 out of total 50000\n",
      "loss: 15.845719, time: 10.783756, number of examples: 5, step 2400 out of total 50000\n",
      "loss: 15.995785, time: 10.476608, number of examples: 5, step 2500 out of total 50000\n",
      "loss: 15.956198, time: 10.584288, number of examples: 5, step 2600 out of total 50000\n",
      "loss: 15.698757, time: 10.363683, number of examples: 5, step 2700 out of total 50000\n",
      "loss: 16.062314, time: 10.870971, number of examples: 5, step 2800 out of total 50000\n",
      "loss: 16.330498, time: 10.406100, number of examples: 5, step 2900 out of total 50000\n",
      "loss: 16.146589, time: 10.311280, number of examples: 5, step 3000 out of total 50000\n",
      "loss: 15.714548, time: 10.306308, number of examples: 5, step 3100 out of total 50000\n",
      "loss: 15.486499, time: 10.973531, number of examples: 5, step 3200 out of total 50000\n",
      "loss: 16.004875, time: 10.477626, number of examples: 5, step 3300 out of total 50000\n",
      "loss: 15.624634, time: 10.465289, number of examples: 5, step 3400 out of total 50000\n",
      "loss: 15.008533, time: 10.357985, number of examples: 5, step 3500 out of total 50000\n",
      "loss: 14.941468, time: 10.786665, number of examples: 5, step 3600 out of total 50000\n",
      "loss: 15.690416, time: 10.349613, number of examples: 5, step 3700 out of total 50000\n",
      "loss: 15.473832, time: 10.488669, number of examples: 5, step 3800 out of total 50000\n",
      "loss: 15.507311, time: 10.463375, number of examples: 5, step 3900 out of total 50000\n",
      "loss: 15.229168, time: 10.962234, number of examples: 5, step 4000 out of total 50000\n",
      "loss: 15.370942, time: 10.260397, number of examples: 5, step 4100 out of total 50000\n",
      "loss: 16.097768, time: 10.322650, number of examples: 3, step 4200 out of total 50000\n",
      "loss: 15.122436, time: 10.378397, number of examples: 5, step 4300 out of total 50000\n",
      "loss: 15.237820, time: 10.712401, number of examples: 5, step 4400 out of total 50000\n",
      "loss: 15.357680, time: 10.279303, number of examples: 5, step 4500 out of total 50000\n",
      "loss: 15.554055, time: 10.379611, number of examples: 7, step 4600 out of total 50000\n",
      "loss: 15.103175, time: 10.420841, number of examples: 5, step 4700 out of total 50000\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\"\n",
    "summarizer.fit2(\n",
    "            train_iter,\n",
    "            device= DEVICE,\n",
    "            batch_size=3000,\n",
    "            num_gpus=NUM_GPUS,\n",
    "            gradient_accumulation_steps=2,\n",
    "            max_steps=5e4,\n",
    "            lr=2e-3,\n",
    "            warmup_steps=1e4*0.5,\n",
    "            verbose=True,\n",
    "            report_every=100,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(summarizer.model, model_name+'_newcommond'+\"_dataparallel\"+\"_bertsum_processed_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "summarizer.model = torch.load(\"cnndm_transformersum_distilbert-base-uncased_bertsum_processed_data.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)), or Recall-Oriented Understudy for Gisting Evaluation has been commonly used for evaluation text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=[]\n",
    "for i in test_iter():\n",
    "    test_dataset.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11489"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = summarizer.predict(get_data_iter(test_dataset), device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11489\n",
      "11489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-10 21:54:15,589 [MainThread  ] [INFO ]  Writing summaries.\n",
      "I1210 21:54:15.589290 140261726336832 pyrouge.py:525] Writing summaries.\n",
      "2019-12-10 21:54:15,591 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./results/tmpfmn7fdbj/system and model files to ./results/tmpfmn7fdbj/model.\n",
      "I1210 21:54:15.591191 140261726336832 pyrouge.py:518] Processing summaries. Saving system files to ./results/tmpfmn7fdbj/system and model files to ./results/tmpfmn7fdbj/model.\n",
      "2019-12-10 21:54:15,594 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-12-10-21-54-14/candidate/.\n",
      "I1210 21:54:15.594782 140261726336832 pyrouge.py:43] Processing files in ./results/rouge-tmp-2019-12-10-21-54-14/candidate/.\n",
      "2019-12-10 21:54:16,668 [MainThread  ] [INFO ]  Saved processed files to ./results/tmpfmn7fdbj/system.\n",
      "I1210 21:54:16.668018 140261726336832 pyrouge.py:53] Saved processed files to ./results/tmpfmn7fdbj/system.\n",
      "2019-12-10 21:54:16,669 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-12-10-21-54-14/reference/.\n",
      "I1210 21:54:16.669716 140261726336832 pyrouge.py:43] Processing files in ./results/rouge-tmp-2019-12-10-21-54-14/reference/.\n",
      "2019-12-10 21:54:17,775 [MainThread  ] [INFO ]  Saved processed files to ./results/tmpfmn7fdbj/model.\n",
      "I1210 21:54:17.775310 140261726336832 pyrouge.py:53] Saved processed files to ./results/tmpfmn7fdbj/model.\n",
      "2019-12-10 21:54:17,865 [MainThread  ] [INFO ]  Written ROUGE configuration to ./results/tmp4efxyz7b/rouge_conf.xml\n",
      "I1210 21:54:17.865489 140261726336832 pyrouge.py:354] Written ROUGE configuration to ./results/tmp4efxyz7b/rouge_conf.xml\n",
      "2019-12-10 21:54:17,867 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmp4efxyz7b/rouge_conf.xml\n",
      "I1210 21:54:17.867019 140261726336832 pyrouge.py:372] Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmp4efxyz7b/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.54208 (95%-conf.int. 0.53930 - 0.54484)\n",
      "1 ROUGE-1 Average_P: 0.36866 (95%-conf.int. 0.36651 - 0.37103)\n",
      "1 ROUGE-1 Average_F: 0.42466 (95%-conf.int. 0.42276 - 0.42672)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.24754 (95%-conf.int. 0.24499 - 0.25011)\n",
      "1 ROUGE-2 Average_P: 0.16856 (95%-conf.int. 0.16669 - 0.17049)\n",
      "1 ROUGE-2 Average_F: 0.19382 (95%-conf.int. 0.19190 - 0.19576)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.49419 (95%-conf.int. 0.49159 - 0.49685)\n",
      "1 ROUGE-L Average_P: 0.33667 (95%-conf.int. 0.33456 - 0.33889)\n",
      "1 ROUGE-L Average_F: 0.38754 (95%-conf.int. 0.38561 - 0.38960)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rouge_transformer = get_rouge(prediction, target, \"./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11489\n",
      "11489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 13:34:57,093 [MainThread  ] [INFO ]  Writing summaries.\n",
      "I1207 13:34:57.093963 139683323610944 pyrouge.py:525] Writing summaries.\n",
      "2019-12-07 13:34:57,095 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./results/tmpcferef5_/system and model files to ./results/tmpcferef5_/model.\n",
      "I1207 13:34:57.095779 139683323610944 pyrouge.py:518] Processing summaries. Saving system files to ./results/tmpcferef5_/system and model files to ./results/tmpcferef5_/model.\n",
      "2019-12-07 13:34:57,096 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-12-07-13-34-56/candidate/.\n",
      "I1207 13:34:57.096807 139683323610944 pyrouge.py:43] Processing files in ./results/rouge-tmp-2019-12-07-13-34-56/candidate/.\n",
      "2019-12-07 13:34:58,251 [MainThread  ] [INFO ]  Saved processed files to ./results/tmpcferef5_/system.\n",
      "I1207 13:34:58.251595 139683323610944 pyrouge.py:53] Saved processed files to ./results/tmpcferef5_/system.\n",
      "2019-12-07 13:34:58,253 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-12-07-13-34-56/reference/.\n",
      "I1207 13:34:58.253087 139683323610944 pyrouge.py:43] Processing files in ./results/rouge-tmp-2019-12-07-13-34-56/reference/.\n",
      "2019-12-07 13:34:59,402 [MainThread  ] [INFO ]  Saved processed files to ./results/tmpcferef5_/model.\n",
      "I1207 13:34:59.402208 139683323610944 pyrouge.py:53] Saved processed files to ./results/tmpcferef5_/model.\n",
      "2019-12-07 13:34:59,485 [MainThread  ] [INFO ]  Written ROUGE configuration to ./results/tmpuyc0ul2x/rouge_conf.xml\n",
      "I1207 13:34:59.485491 139683323610944 pyrouge.py:354] Written ROUGE configuration to ./results/tmpuyc0ul2x/rouge_conf.xml\n",
      "2019-12-07 13:34:59,486 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmpuyc0ul2x/rouge_conf.xml\n",
      "I1207 13:34:59.486500 139683323610944 pyrouge.py:372] Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmpuyc0ul2x/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.53608 (95%-conf.int. 0.53328 - 0.53883)\n",
      "1 ROUGE-1 Average_P: 0.37832 (95%-conf.int. 0.37594 - 0.38073)\n",
      "1 ROUGE-1 Average_F: 0.42902 (95%-conf.int. 0.42696 - 0.43104)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.24854 (95%-conf.int. 0.24592 - 0.25123)\n",
      "1 ROUGE-2 Average_P: 0.17574 (95%-conf.int. 0.17360 - 0.17792)\n",
      "1 ROUGE-2 Average_F: 0.19879 (95%-conf.int. 0.19676 - 0.20102)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.48980 (95%-conf.int. 0.48709 - 0.49253)\n",
      "1 ROUGE-L Average_P: 0.34635 (95%-conf.int. 0.34386 - 0.34873)\n",
      "1 ROUGE-L Average_F: 0.39242 (95%-conf.int. 0.39039 - 0.39448)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rouge_transformer = get_rouge(prediction, target, \"./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
      "QUICK_RUN = True\n",
      "USE_PREPROCESSED_DATA = True\n",
      "import os\n",
      "\n",
      "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
      "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "import sys\n",
      "import os\n",
      "\n",
      "nlp_path = os.path.abspath(\"../../\")\n",
      "if nlp_path not in sys.path:\n",
      "    sys.path.insert(0, nlp_path)\n",
      "sys.path.insert(0, \"./\")\n",
      "sys.path.insert(0, \"/dadendev/nlp/examples/text_summarization/BertSum/\")\n",
      "if USE_PREPROCESSED_DATA:\n",
      "    data_path = \"./temp_data5/\"\n",
      "    if not os.path.exists(data_path):\n",
      "        os.mkdir(data_path)\n",
      "    #CNNDMBertSumProcessedData.download(local_path=data_path)\n",
      "    train_iter, test_iter = CNNDMBertSumProcessedData().splits(root=data_path)\n",
      "data_path = \"./temp_data4/\"\n",
      "from utils_nlp.dataset.cnndm import CNNDMBertSumProcessedData\n",
      "\n",
      "train_files = CNNDMBertSumProcessedData.save_data(\n",
      "    ext_sum_train, is_test=False, path_and_prefix=data_path, chunk_size=25\n",
      ")\n",
      "test_files = CNNDMBertSumProcessedData.save_data(\n",
      "    ext_sum_test, is_test=True, path_and_prefix=data_path, chunk_size=None\n",
      ")\n",
      "if USE_PREPROCESSED_DATA:\n",
      "    data_path = \"./temp_data5/\"\n",
      "    if not os.path.exists(data_path):\n",
      "        os.mkdir(data_path)\n",
      "    #CNNDMBertSumProcessedData.download(local_path=data_path)\n",
      "    train_iter, test_iter = CNNDMBertSumProcessedData().splits(root=data_path)\n",
      "# notebook parameters\n",
      "DATA_FOLDER = \"./temp\"\n",
      "CACHE_DIR = \"./temp\"\n",
      "DEVICE = \"cuda\"\n",
      "BATCH_SIZE = 3000\n",
      "NUM_GPUS = 1\n",
      "encoder = \"transformer\"\n",
      "model_name = \"distilbert-base-uncased\"\n",
      "from utils_nlp.models.transformers.extractive_summarization import ExtractiveSummarizer\n",
      "summarizer = ExtractiveSummarizer(model_name, encoder, CACHE_DIR)\n",
      "from utils_nlp.models.transformers.extractive_summarization import get_dataloader\n",
      "train_dataloader = get_dataloader(train_iter(), is_labeled=True, batch_size=3000)\n",
      "from utils_nlp.common.pytorch_utils import get_device\n",
      "device, num_gpus = get_device(num_gpus=4, local_rank=-1)\n",
      "DEVICE = \"cuda\"\n",
      "summarizer.fit2(\n",
      "            train_iter,\n",
      "            device= DEVICE,\n",
      "            batch_size=3000,\n",
      "            num_gpus=NUM_GPUS,\n",
      "            gradient_accumulation_steps=2,\n",
      "            max_steps=5e4,\n",
      "            lr=2e-3,\n",
      "            warmup_steps=1e4*0.5,\n",
      "            verbose=True,\n",
      "            report_every=100,\n",
      "        )\n",
      "torch.save(summarizer.model, model_name+'1'+\"_bertsum_processed_data.pt\")\n",
      "import torch\n",
      "torch.save(summarizer.model, model_name+'1'+\"_bertsum_processed_data.pt\")\n",
      "import torch\n",
      "import os\n",
      "\n",
      "test_dataset=[]\n",
      "for i in test_iter():\n",
      "    test_dataset.extend(i)\n",
      "len(test_dataset)\n",
      "from utils_nlp.models.transformers.extractive_summarization import get_data_iter\n",
      "\n",
      "prediction = summarizer.predict(get_data_iter(test_dataset),\n",
      "                               device=DEVICE)\n",
      "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]\n",
      "from utils_nlp.eval.evaluate_summarization import get_rouge\n",
      "rouge_transformer = get_rouge(prediction, target, \"./results/\")\n",
      "from utils_nlp.models.transformers.extractive_summarization import ExtractiveSummarizer\n",
      "summarizer = ExtractiveSummarizer(model_name, encoder, CACHE_DIR)\n",
      "summarizer.fit(\n",
      "            train_dataloader,\n",
      "            num_gpus=NUM_GPUS,\n",
      "            gradient_accumulation_steps=2,\n",
      "            max_steps=5e4,\n",
      "            lr=2e-3,\n",
      "            warmup_steps=1e4*0.5,\n",
      "            verbose=True,\n",
      "            report_every=200,\n",
      "        )\n",
      "summarizer.fit(\n",
      "            train_dataloader,\n",
      "            num_gpus=NUM_GPUS,\n",
      "            gradient_accumulation_steps=2,\n",
      "            max_steps=5e4,\n",
      "            lr=2e-3,\n",
      "            warmup_steps=1e4*0.5,\n",
      "            verbose=True,\n",
      "            report_every=200,\n",
      "        )\n",
      "from utils_nlp.models.transformers.extractive_summarization import get_dataloader\n",
      "train_dataloader = get_dataloader(train_iter(), is_labeled=True, batch_size=3000)\n",
      "summarizer.fit(\n",
      "            train_dataloader,\n",
      "            num_gpus=NUM_GPUS,\n",
      "            gradient_accumulation_steps=2,\n",
      "            max_steps=5e4,\n",
      "            lr=2e-3,\n",
      "            warmup_steps=1e4*0.5,\n",
      "            verbose=True,\n",
      "            report_every=200,\n",
      "        )\n",
      "len(test_dataset)\n",
      "from utils_nlp.models.transformers.extractive_summarization import get_data_iter\n",
      "\n",
      "prediction = summarizer.predict(get_data_iter(test_dataset),\n",
      "                               device=DEVICE)\n",
      "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]\n",
      "from utils_nlp.eval.evaluate_summarization import get_rouge\n",
      "rouge_transformer = get_rouge(prediction, target, \"./results/\")\n",
      "import torch\n",
      "torch.save(summarizer.model, model_name+'newcommond'+\"_bertsum_processed_data.pt\")\n",
      "%history\n"
     ]
    }
   ],
   "source": [
    "%history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-31 17:59:07,705 [MainThread  ] [INFO ]  Writing summaries.\n",
      "I1031 17:59:07.705975 140308661311296 pyrouge.py:525] Writing summaries.\n",
      "2019-10-31 17:59:07,707 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./results/tmp2jh52kci/system and model files to ./results/tmp2jh52kci/model.\n",
      "I1031 17:59:07.707562 140308661311296 pyrouge.py:518] Processing summaries. Saving system files to ./results/tmp2jh52kci/system and model files to ./results/tmp2jh52kci/model.\n",
      "2019-10-31 17:59:07,708 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-10-31-17-59-07/candidate/.\n",
      "I1031 17:59:07.708495 140308661311296 pyrouge.py:43] Processing files in ./results/rouge-tmp-2019-10-31-17-59-07/candidate/.\n",
      "2019-10-31 17:59:07,719 [MainThread  ] [INFO ]  Saved processed files to ./results/tmp2jh52kci/system.\n",
      "I1031 17:59:07.719068 140308661311296 pyrouge.py:53] Saved processed files to ./results/tmp2jh52kci/system.\n",
      "2019-10-31 17:59:07,720 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-10-31-17-59-07/reference/.\n",
      "I1031 17:59:07.720107 140308661311296 pyrouge.py:43] Processing files in ./results/rouge-tmp-2019-10-31-17-59-07/reference/.\n",
      "2019-10-31 17:59:07,730 [MainThread  ] [INFO ]  Saved processed files to ./results/tmp2jh52kci/model.\n",
      "I1031 17:59:07.730343 140308661311296 pyrouge.py:53] Saved processed files to ./results/tmp2jh52kci/model.\n",
      "2019-10-31 17:59:07,732 [MainThread  ] [INFO ]  Written ROUGE configuration to ./results/tmp4zxruqq3/rouge_conf.xml\n",
      "I1031 17:59:07.732450 140308661311296 pyrouge.py:354] Written ROUGE configuration to ./results/tmp4zxruqq3/rouge_conf.xml\n",
      "2019-10-31 17:59:07,733 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmp4zxruqq3/rouge_conf.xml\n",
      "I1031 17:59:07.733434 140308661311296 pyrouge.py:372] Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmp4zxruqq3/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.41601 (95%-conf.int. 0.38468 - 0.44964)\n",
      "1 ROUGE-1 Average_P: 0.21124 (95%-conf.int. 0.19304 - 0.22896)\n",
      "1 ROUGE-1 Average_F: 0.27238 (95%-conf.int. 0.25137 - 0.29249)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.13888 (95%-conf.int. 0.11130 - 0.16833)\n",
      "1 ROUGE-2 Average_P: 0.06738 (95%-conf.int. 0.05454 - 0.08099)\n",
      "1 ROUGE-2 Average_F: 0.08825 (95%-conf.int. 0.07190 - 0.10610)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.37098 (95%-conf.int. 0.34192 - 0.40359)\n",
      "1 ROUGE-L Average_P: 0.18856 (95%-conf.int. 0.17234 - 0.20503)\n",
      "1 ROUGE-L Average_F: 0.24308 (95%-conf.int. 0.22405 - 0.26304)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from utils_nlp.eval.evaluate_summarization import get_rouge\n",
    "#rouge_transformer = get_rouge(prediction, target, \"./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .<q>he was flown back to chicago via air ambulance on march 20 , but he died on sunday .<q>a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .',\n",
       " 'he was flown back to chicago via air ambulance on march 20 , but he died on sunday .',\n",
       " 'andrew mogni , 20 , from glen ellyn , illinois , a university of iowa student has died nearly three months after a fall in rome in a suspected robbery',\n",
       " 'he was taken to a medical facility in the chicago area , close to his family home in glen ellyn .',\n",
       " \"he died on sunday at northwestern memorial hospital - medical examiner 's office spokesman frank shuftan says a cause of death wo n't be released until monday at the earliest .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed .',\n",
       " \"on sunday , his cousin abby wrote online : ` this morning my cousin andrew 's soul was lifted up to heaven .\",\n",
       " 'initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed',\n",
       " '` at the beginning of january he went to rome to study aboard and on the way home from a party he was brutally attacked and thrown off a 40ft bridge and hit the concrete below .',\n",
       " \"` he was in a coma and in critical condition for months . '\",\n",
       " 'paula barnett , who said she is a close family friend , told my suburban life , that mogni had only been in the country for six hours when the incident happened .',\n",
       " 'she said he was was alone at the time of the alleged assault and personal items were stolen .',\n",
       " 'she added that he was in a non-medically induced coma , having suffered serious infection and internal bleeding .',\n",
       " 'mogni was a third-year finance major from glen ellyn , ill. , who was participating in a semester-long program at john cabot university .',\n",
       " \"mogni belonged to the school 's chapter of the sigma nu fraternity , reports the chicago tribune who posted a sign outside a building reading ` pray for mogni . '\",\n",
       " \"the fraternity 's iowa chapter announced sunday afternoon via twitter that a memorial service will be held on campus to remember mogni .\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['src_txt']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6 cm3",
   "language": "python",
   "name": "cm3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
