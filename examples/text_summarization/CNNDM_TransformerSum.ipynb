{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "nlp_path = os.path.abspath(\"../../\")\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "sys.path.insert(0, \"./\")\n",
    "sys.path.insert(0, \"/dadendev/nlp/examples/text_summarization/BertSum/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/dadendev/nlp/examples/text_summarization/BertSum/', './', '/dadendev/nlp', '/dadendev/anaconda3/envs/cm3/lib/python36.zip', '/dadendev/anaconda3/envs/cm3/lib/python3.6', '/dadendev/anaconda3/envs/cm3/lib/python3.6/lib-dynload', '', '/home/daden/.local/lib/python3.6/site-packages', '/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages', '/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages/pyrouge-0.1.3-py3.6.egg', '/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages/IPython/extensions', '/home/daden/.ipython']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "0lines [00:00, ?lines/s]\n",
      "0lines [00:00, ?lines/s]\n",
      "0lines [00:00, ?lines/s]\n",
      "0lines [00:00, ?lines/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['marseille',\n",
       "  ',',\n",
       "  'france',\n",
       "  '(',\n",
       "  'cnn',\n",
       "  ')',\n",
       "  'the',\n",
       "  'french',\n",
       "  'prosecutor',\n",
       "  'leading',\n",
       "  'an',\n",
       "  'investigation',\n",
       "  'into',\n",
       "  'the',\n",
       "  'crash',\n",
       "  'of',\n",
       "  'germanwings',\n",
       "  'flight',\n",
       "  '9525',\n",
       "  'insisted',\n",
       "  'wednesday',\n",
       "  'that',\n",
       "  'he',\n",
       "  'was',\n",
       "  'not',\n",
       "  'aware',\n",
       "  'of',\n",
       "  'any',\n",
       "  'video',\n",
       "  'footage',\n",
       "  'from',\n",
       "  'on',\n",
       "  'board',\n",
       "  'the',\n",
       "  'plane',\n",
       "  '.'],\n",
       " ['marseille',\n",
       "  'prosecutor',\n",
       "  'brice',\n",
       "  'robin',\n",
       "  'told',\n",
       "  'cnn',\n",
       "  'that',\n",
       "  '``',\n",
       "  'so',\n",
       "  'far',\n",
       "  'no',\n",
       "  'videos',\n",
       "  'were',\n",
       "  'used',\n",
       "  'in',\n",
       "  'the',\n",
       "  'crash',\n",
       "  'investigation',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['he',\n",
       "  'added',\n",
       "  ',',\n",
       "  '``',\n",
       "  'a',\n",
       "  'person',\n",
       "  'who',\n",
       "  'has',\n",
       "  'such',\n",
       "  'a',\n",
       "  'video',\n",
       "  'needs',\n",
       "  'to',\n",
       "  'immediately',\n",
       "  'give',\n",
       "  'it',\n",
       "  'to',\n",
       "  'the',\n",
       "  'investigators',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['robin',\n",
       "  \"'s\",\n",
       "  'comments',\n",
       "  'follow',\n",
       "  'claims',\n",
       "  'by',\n",
       "  'two',\n",
       "  'magazines',\n",
       "  ',',\n",
       "  'german',\n",
       "  'daily',\n",
       "  'bild',\n",
       "  'and',\n",
       "  'french',\n",
       "  'paris',\n",
       "  'match',\n",
       "  ',',\n",
       "  'of',\n",
       "  'a',\n",
       "  'cell',\n",
       "  'phone',\n",
       "  'video',\n",
       "  'showing',\n",
       "  'the',\n",
       "  'harrowing',\n",
       "  'final',\n",
       "  'seconds',\n",
       "  'from',\n",
       "  'on',\n",
       "  'board',\n",
       "  'germanwings',\n",
       "  'flight',\n",
       "  '9525',\n",
       "  'as',\n",
       "  'it',\n",
       "  'crashed',\n",
       "  'into',\n",
       "  'the',\n",
       "  'french',\n",
       "  'alps',\n",
       "  '.'],\n",
       " ['all', '150', 'on', 'board', 'were', 'killed', '.'],\n",
       " ['paris',\n",
       "  'match',\n",
       "  'and',\n",
       "  'bild',\n",
       "  'reported',\n",
       "  'that',\n",
       "  'the',\n",
       "  'video',\n",
       "  'was',\n",
       "  'recovered',\n",
       "  'from',\n",
       "  'a',\n",
       "  'phone',\n",
       "  'at',\n",
       "  'the',\n",
       "  'wreckage',\n",
       "  'site',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'two',\n",
       "  'publications',\n",
       "  'described',\n",
       "  'the',\n",
       "  'supposed',\n",
       "  'video',\n",
       "  ',',\n",
       "  'but',\n",
       "  'did',\n",
       "  'not',\n",
       "  'post',\n",
       "  'it',\n",
       "  'on',\n",
       "  'their',\n",
       "  'websites',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'publications',\n",
       "  'said',\n",
       "  'that',\n",
       "  'they',\n",
       "  'watched',\n",
       "  'the',\n",
       "  'video',\n",
       "  ',',\n",
       "  'which',\n",
       "  'was',\n",
       "  'found',\n",
       "  'by',\n",
       "  'a',\n",
       "  'source',\n",
       "  'close',\n",
       "  'to',\n",
       "  'the',\n",
       "  'investigation',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['one',\n",
       "  'can',\n",
       "  'hear',\n",
       "  'cries',\n",
       "  'of',\n",
       "  '`',\n",
       "  'my',\n",
       "  'god',\n",
       "  \"'\",\n",
       "  'in',\n",
       "  'several',\n",
       "  'languages',\n",
       "  ',',\n",
       "  '``',\n",
       "  'paris',\n",
       "  'match',\n",
       "  'reported',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['metallic',\n",
       "  'banging',\n",
       "  'can',\n",
       "  'also',\n",
       "  'be',\n",
       "  'heard',\n",
       "  'more',\n",
       "  'than',\n",
       "  'three',\n",
       "  'times',\n",
       "  ',',\n",
       "  'perhaps',\n",
       "  'of',\n",
       "  'the',\n",
       "  'pilot',\n",
       "  'trying',\n",
       "  'to',\n",
       "  'open',\n",
       "  'the',\n",
       "  'cockpit',\n",
       "  'door',\n",
       "  'with',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'object',\n",
       "  '.'],\n",
       " ['towards',\n",
       "  'the',\n",
       "  'end',\n",
       "  ',',\n",
       "  'after',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'shake',\n",
       "  ',',\n",
       "  'stronger',\n",
       "  'than',\n",
       "  'the',\n",
       "  'others',\n",
       "  ',',\n",
       "  'the',\n",
       "  'screaming',\n",
       "  'intensifies',\n",
       "  '.'],\n",
       " ['then', 'nothing', '.', '``'],\n",
       " ['``',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'very',\n",
       "  'disturbing',\n",
       "  'scene',\n",
       "  ',',\n",
       "  '``',\n",
       "  'said',\n",
       "  'julian',\n",
       "  'reichelt',\n",
       "  ',',\n",
       "  'editor-in-chief',\n",
       "  'of',\n",
       "  'bild',\n",
       "  'online',\n",
       "  '.'],\n",
       " ['an',\n",
       "  'official',\n",
       "  'with',\n",
       "  'france',\n",
       "  \"'s\",\n",
       "  'accident',\n",
       "  'investigation',\n",
       "  'agency',\n",
       "  ',',\n",
       "  'the',\n",
       "  'bea',\n",
       "  ',',\n",
       "  'said',\n",
       "  'the',\n",
       "  'agency',\n",
       "  'is',\n",
       "  'not',\n",
       "  'aware',\n",
       "  'of',\n",
       "  'any',\n",
       "  'such',\n",
       "  'video',\n",
       "  '.'],\n",
       " ['lt.',\n",
       "  'col.',\n",
       "  'jean-marc',\n",
       "  'menichini',\n",
       "  ',',\n",
       "  'a',\n",
       "  'french',\n",
       "  'gendarmerie',\n",
       "  'spokesman',\n",
       "  'in',\n",
       "  'charge',\n",
       "  'of',\n",
       "  'communications',\n",
       "  'on',\n",
       "  'rescue',\n",
       "  'efforts',\n",
       "  'around',\n",
       "  'the',\n",
       "  'germanwings',\n",
       "  'crash',\n",
       "  'site',\n",
       "  ',',\n",
       "  'told',\n",
       "  'cnn',\n",
       "  'that',\n",
       "  'the',\n",
       "  'reports',\n",
       "  'were',\n",
       "  '``',\n",
       "  'completely',\n",
       "  'wrong',\n",
       "  '``',\n",
       "  'and',\n",
       "  '``',\n",
       "  'unwarranted',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['cell',\n",
       "  'phones',\n",
       "  'have',\n",
       "  'been',\n",
       "  'collected',\n",
       "  'at',\n",
       "  'the',\n",
       "  'site',\n",
       "  ',',\n",
       "  'he',\n",
       "  'said',\n",
       "  ',',\n",
       "  'but',\n",
       "  'that',\n",
       "  'they',\n",
       "  '``',\n",
       "  'had',\n",
       "  \"n't\",\n",
       "  'been',\n",
       "  'exploited',\n",
       "  'yet',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['menichini',\n",
       "  'said',\n",
       "  'he',\n",
       "  'believed',\n",
       "  'the',\n",
       "  'cell',\n",
       "  'phones',\n",
       "  'would',\n",
       "  'need',\n",
       "  'to',\n",
       "  'be',\n",
       "  'sent',\n",
       "  'to',\n",
       "  'the',\n",
       "  'criminal',\n",
       "  'research',\n",
       "  'institute',\n",
       "  'in',\n",
       "  'rosny',\n",
       "  'sous-bois',\n",
       "  ',',\n",
       "  'near',\n",
       "  'paris',\n",
       "  ',',\n",
       "  'in',\n",
       "  'order',\n",
       "  'to',\n",
       "  'be',\n",
       "  'analyzed',\n",
       "  'by',\n",
       "  'specialized',\n",
       "  'technicians',\n",
       "  'working',\n",
       "  'hand-in-hand',\n",
       "  'with',\n",
       "  'investigators',\n",
       "  '.'],\n",
       " ['but',\n",
       "  'none',\n",
       "  'of',\n",
       "  'the',\n",
       "  'cell',\n",
       "  'phones',\n",
       "  'found',\n",
       "  'so',\n",
       "  'far',\n",
       "  'have',\n",
       "  'been',\n",
       "  'sent',\n",
       "  'to',\n",
       "  'the',\n",
       "  'institute',\n",
       "  ',',\n",
       "  'menichini',\n",
       "  'said',\n",
       "  '.'],\n",
       " ['asked',\n",
       "  'whether',\n",
       "  'staff',\n",
       "  'involved',\n",
       "  'in',\n",
       "  'the',\n",
       "  'search',\n",
       "  'could',\n",
       "  'have',\n",
       "  'leaked',\n",
       "  'a',\n",
       "  'memory',\n",
       "  'card',\n",
       "  'to',\n",
       "  'the',\n",
       "  'media',\n",
       "  ',',\n",
       "  'menichini',\n",
       "  'answered',\n",
       "  'with',\n",
       "  'a',\n",
       "  'categorical',\n",
       "  '``',\n",
       "  'no',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['reichelt',\n",
       "  'told',\n",
       "  '``',\n",
       "  'erin',\n",
       "  'burnett',\n",
       "  ':',\n",
       "  'outfront',\n",
       "  '``',\n",
       "  'that',\n",
       "  'he',\n",
       "  'had',\n",
       "  'watched',\n",
       "  'the',\n",
       "  'video',\n",
       "  'and',\n",
       "  'stood',\n",
       "  'by',\n",
       "  'the',\n",
       "  'report',\n",
       "  ',',\n",
       "  'saying',\n",
       "  'bild',\n",
       "  'and',\n",
       "  'paris',\n",
       "  'match',\n",
       "  'are',\n",
       "  '``',\n",
       "  'very',\n",
       "  'confident',\n",
       "  '``',\n",
       "  'that',\n",
       "  'the',\n",
       "  'clip',\n",
       "  'is',\n",
       "  'real',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'noted',\n",
       "  'that',\n",
       "  'investigators',\n",
       "  'only',\n",
       "  'revealed',\n",
       "  'they',\n",
       "  \"'d\",\n",
       "  'recovered',\n",
       "  'cell',\n",
       "  'phones',\n",
       "  'from',\n",
       "  'the',\n",
       "  'crash',\n",
       "  'site',\n",
       "  'after',\n",
       "  'bild',\n",
       "  'and',\n",
       "  'paris',\n",
       "  'match',\n",
       "  'published',\n",
       "  'their',\n",
       "  'reports',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['that', 'is', 'something', 'we', 'did', 'not', 'know', 'before', '.'],\n",
       " ['...',\n",
       "  'overall',\n",
       "  'we',\n",
       "  'can',\n",
       "  'say',\n",
       "  'many',\n",
       "  'things',\n",
       "  'of',\n",
       "  'the',\n",
       "  'investigation',\n",
       "  'were',\n",
       "  \"n't\",\n",
       "  'revealed',\n",
       "  'by',\n",
       "  'the',\n",
       "  'investigation',\n",
       "  'at',\n",
       "  'the',\n",
       "  'beginning',\n",
       "  ',',\n",
       "  '``',\n",
       "  'he',\n",
       "  'said',\n",
       "  '.'],\n",
       " ['what', 'was', 'mental', 'state', 'of', 'germanwings', 'co-pilot', '?'],\n",
       " ['german',\n",
       "  'airline',\n",
       "  'lufthansa',\n",
       "  'confirmed',\n",
       "  'tuesday',\n",
       "  'that',\n",
       "  'co-pilot',\n",
       "  'andreas',\n",
       "  'lubitz',\n",
       "  'had',\n",
       "  'battled',\n",
       "  'depression',\n",
       "  'years',\n",
       "  'before',\n",
       "  'he',\n",
       "  'took',\n",
       "  'the',\n",
       "  'controls',\n",
       "  'of',\n",
       "  'germanwings',\n",
       "  'flight',\n",
       "  '9525',\n",
       "  ',',\n",
       "  'which',\n",
       "  'he',\n",
       "  \"'s\",\n",
       "  'accused',\n",
       "  'of',\n",
       "  'deliberately',\n",
       "  'crashing',\n",
       "  'last',\n",
       "  'week',\n",
       "  'in',\n",
       "  'the',\n",
       "  'french',\n",
       "  'alps',\n",
       "  '.'],\n",
       " ['lubitz',\n",
       "  'told',\n",
       "  'his',\n",
       "  'lufthansa',\n",
       "  'flight',\n",
       "  'training',\n",
       "  'school',\n",
       "  'in',\n",
       "  '2009',\n",
       "  'that',\n",
       "  'he',\n",
       "  'had',\n",
       "  'a',\n",
       "  '``',\n",
       "  'previous',\n",
       "  'episode',\n",
       "  'of',\n",
       "  'severe',\n",
       "  'depression',\n",
       "  ',',\n",
       "  '``',\n",
       "  'the',\n",
       "  'airline',\n",
       "  'said',\n",
       "  'tuesday',\n",
       "  '.'],\n",
       " ['email',\n",
       "  'correspondence',\n",
       "  'between',\n",
       "  'lubitz',\n",
       "  'and',\n",
       "  'the',\n",
       "  'school',\n",
       "  'discovered',\n",
       "  'in',\n",
       "  'an',\n",
       "  'internal',\n",
       "  'investigation',\n",
       "  ',',\n",
       "  'lufthansa',\n",
       "  'said',\n",
       "  ',',\n",
       "  'included',\n",
       "  'medical',\n",
       "  'documents',\n",
       "  'he',\n",
       "  'submitted',\n",
       "  'in',\n",
       "  'connection',\n",
       "  'with',\n",
       "  'resuming',\n",
       "  'his',\n",
       "  'flight',\n",
       "  'training',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'announcement',\n",
       "  'indicates',\n",
       "  'that',\n",
       "  'lufthansa',\n",
       "  ',',\n",
       "  'the',\n",
       "  'parent',\n",
       "  'company',\n",
       "  'of',\n",
       "  'germanwings',\n",
       "  ',',\n",
       "  'knew',\n",
       "  'of',\n",
       "  'lubitz',\n",
       "  \"'s\",\n",
       "  'battle',\n",
       "  'with',\n",
       "  'depression',\n",
       "  ',',\n",
       "  'allowed',\n",
       "  'him',\n",
       "  'to',\n",
       "  'continue',\n",
       "  'training',\n",
       "  'and',\n",
       "  'ultimately',\n",
       "  'put',\n",
       "  'him',\n",
       "  'in',\n",
       "  'the',\n",
       "  'cockpit',\n",
       "  '.'],\n",
       " ['lufthansa',\n",
       "  ',',\n",
       "  'whose',\n",
       "  'ceo',\n",
       "  'carsten',\n",
       "  'spohr',\n",
       "  'previously',\n",
       "  'said',\n",
       "  'lubitz',\n",
       "  'was',\n",
       "  '100',\n",
       "  '%',\n",
       "  'fit',\n",
       "  'to',\n",
       "  'fly',\n",
       "  ',',\n",
       "  'described',\n",
       "  'its',\n",
       "  'statement',\n",
       "  'tuesday',\n",
       "  'as',\n",
       "  'a',\n",
       "  '``',\n",
       "  'swift',\n",
       "  'and',\n",
       "  'seamless',\n",
       "  'clarification',\n",
       "  '``',\n",
       "  'and',\n",
       "  'said',\n",
       "  'it',\n",
       "  'was',\n",
       "  'sharing',\n",
       "  'the',\n",
       "  'information',\n",
       "  'and',\n",
       "  'documents',\n",
       "  '--',\n",
       "  'including',\n",
       "  'training',\n",
       "  'and',\n",
       "  'medical',\n",
       "  'records',\n",
       "  '--',\n",
       "  'with',\n",
       "  'public',\n",
       "  'prosecutors',\n",
       "  '.'],\n",
       " ['spohr',\n",
       "  'traveled',\n",
       "  'to',\n",
       "  'the',\n",
       "  'crash',\n",
       "  'site',\n",
       "  'wednesday',\n",
       "  ',',\n",
       "  'where',\n",
       "  'recovery',\n",
       "  'teams',\n",
       "  'have',\n",
       "  'been',\n",
       "  'working',\n",
       "  'for',\n",
       "  'the',\n",
       "  'past',\n",
       "  'week',\n",
       "  'to',\n",
       "  'recover',\n",
       "  'human',\n",
       "  'remains',\n",
       "  'and',\n",
       "  'plane',\n",
       "  'debris',\n",
       "  'scattered',\n",
       "  'across',\n",
       "  'a',\n",
       "  'steep',\n",
       "  'mountainside',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'saw',\n",
       "  'the',\n",
       "  'crisis',\n",
       "  'center',\n",
       "  'set',\n",
       "  'up',\n",
       "  'in',\n",
       "  'seyne-les-alpes',\n",
       "  ',',\n",
       "  'laid',\n",
       "  'a',\n",
       "  'wreath',\n",
       "  'in',\n",
       "  'the',\n",
       "  'village',\n",
       "  'of',\n",
       "  'le',\n",
       "  'vernet',\n",
       "  ',',\n",
       "  'closer',\n",
       "  'to',\n",
       "  'the',\n",
       "  'crash',\n",
       "  'site',\n",
       "  ',',\n",
       "  'where',\n",
       "  'grieving',\n",
       "  'families',\n",
       "  'have',\n",
       "  'left',\n",
       "  'flowers',\n",
       "  'at',\n",
       "  'a',\n",
       "  'simple',\n",
       "  'stone',\n",
       "  'memorial',\n",
       "  '.'],\n",
       " ['menichini',\n",
       "  'told',\n",
       "  'cnn',\n",
       "  'late',\n",
       "  'tuesday',\n",
       "  'that',\n",
       "  'no',\n",
       "  'visible',\n",
       "  'human',\n",
       "  'remains',\n",
       "  'were',\n",
       "  'left',\n",
       "  'at',\n",
       "  'the',\n",
       "  'site',\n",
       "  'but',\n",
       "  'recovery',\n",
       "  'teams',\n",
       "  'would',\n",
       "  'keep',\n",
       "  'searching',\n",
       "  '.'],\n",
       " ['french',\n",
       "  'president',\n",
       "  'francois',\n",
       "  'hollande',\n",
       "  ',',\n",
       "  'speaking',\n",
       "  'tuesday',\n",
       "  ',',\n",
       "  'said',\n",
       "  'that',\n",
       "  'it',\n",
       "  'should',\n",
       "  'be',\n",
       "  'possible',\n",
       "  'to',\n",
       "  'identify',\n",
       "  'all',\n",
       "  'the',\n",
       "  'victims',\n",
       "  'using',\n",
       "  'dna',\n",
       "  'analysis',\n",
       "  'by',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'the',\n",
       "  'week',\n",
       "  ',',\n",
       "  'sooner',\n",
       "  'than',\n",
       "  'authorities',\n",
       "  'had',\n",
       "  'previously',\n",
       "  'suggested',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'the',\n",
       "  'meantime',\n",
       "  ',',\n",
       "  'the',\n",
       "  'recovery',\n",
       "  'of',\n",
       "  'the',\n",
       "  'victims',\n",
       "  \"'\",\n",
       "  'personal',\n",
       "  'belongings',\n",
       "  'will',\n",
       "  'start',\n",
       "  'wednesday',\n",
       "  ',',\n",
       "  'menichini',\n",
       "  'said',\n",
       "  '.'],\n",
       " ['among',\n",
       "  'those',\n",
       "  'personal',\n",
       "  'belongings',\n",
       "  'could',\n",
       "  'be',\n",
       "  'more',\n",
       "  'cell',\n",
       "  'phones',\n",
       "  'belonging',\n",
       "  'to',\n",
       "  'the',\n",
       "  '144',\n",
       "  'passengers',\n",
       "  'and',\n",
       "  'six',\n",
       "  'crew',\n",
       "  'on',\n",
       "  'board',\n",
       "  '.'],\n",
       " ['check', 'out', 'the', 'latest', 'from', 'our', 'correspondents', '.'],\n",
       " ['the',\n",
       "  'details',\n",
       "  'about',\n",
       "  'lubitz',\n",
       "  \"'s\",\n",
       "  'correspondence',\n",
       "  'with',\n",
       "  'the',\n",
       "  'flight',\n",
       "  'school',\n",
       "  'during',\n",
       "  'his',\n",
       "  'training',\n",
       "  'were',\n",
       "  'among',\n",
       "  'several',\n",
       "  'developments',\n",
       "  'as',\n",
       "  'investigators',\n",
       "  'continued',\n",
       "  'to',\n",
       "  'delve',\n",
       "  'into',\n",
       "  'what',\n",
       "  'caused',\n",
       "  'the',\n",
       "  'crash',\n",
       "  'and',\n",
       "  'lubitz',\n",
       "  \"'s\",\n",
       "  'possible',\n",
       "  'motive',\n",
       "  'for',\n",
       "  'downing',\n",
       "  'the',\n",
       "  'jet',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'lufthansa',\n",
       "  'spokesperson',\n",
       "  'told',\n",
       "  'cnn',\n",
       "  'on',\n",
       "  'tuesday',\n",
       "  'that',\n",
       "  'lubitz',\n",
       "  'had',\n",
       "  'a',\n",
       "  'valid',\n",
       "  'medical',\n",
       "  'certificate',\n",
       "  ',',\n",
       "  'had',\n",
       "  'passed',\n",
       "  'all',\n",
       "  'his',\n",
       "  'examinations',\n",
       "  'and',\n",
       "  '``',\n",
       "  'held',\n",
       "  'all',\n",
       "  'the',\n",
       "  'licenses',\n",
       "  'required',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['earlier',\n",
       "  ',',\n",
       "  'a',\n",
       "  'spokesman',\n",
       "  'for',\n",
       "  'the',\n",
       "  'prosecutor',\n",
       "  \"'s\",\n",
       "  'office',\n",
       "  'in',\n",
       "  'dusseldorf',\n",
       "  ',',\n",
       "  'christoph',\n",
       "  'kumpa',\n",
       "  ',',\n",
       "  'said',\n",
       "  'medical',\n",
       "  'records',\n",
       "  'reveal',\n",
       "  'lubitz',\n",
       "  'suffered',\n",
       "  'from',\n",
       "  'suicidal',\n",
       "  'tendencies',\n",
       "  'at',\n",
       "  'some',\n",
       "  'point',\n",
       "  'before',\n",
       "  'his',\n",
       "  'aviation',\n",
       "  'career',\n",
       "  'and',\n",
       "  'underwent',\n",
       "  'psychotherapy',\n",
       "  'before',\n",
       "  'he',\n",
       "  'got',\n",
       "  'his',\n",
       "  'pilot',\n",
       "  \"'s\",\n",
       "  'license',\n",
       "  '.'],\n",
       " ['kumpa',\n",
       "  'emphasized',\n",
       "  'there',\n",
       "  \"'s\",\n",
       "  'no',\n",
       "  'evidence',\n",
       "  'suggesting',\n",
       "  'lubitz',\n",
       "  'was',\n",
       "  'suicidal',\n",
       "  'or',\n",
       "  'acting',\n",
       "  'aggressively',\n",
       "  'before',\n",
       "  'the',\n",
       "  'crash',\n",
       "  '.'],\n",
       " ['investigators',\n",
       "  'are',\n",
       "  'looking',\n",
       "  'into',\n",
       "  'whether',\n",
       "  'lubitz',\n",
       "  'feared',\n",
       "  'his',\n",
       "  'medical',\n",
       "  'condition',\n",
       "  'would',\n",
       "  'cause',\n",
       "  'him',\n",
       "  'to',\n",
       "  'lose',\n",
       "  'his',\n",
       "  'pilot',\n",
       "  \"'s\",\n",
       "  'license',\n",
       "  ',',\n",
       "  'a',\n",
       "  'european',\n",
       "  'government',\n",
       "  'official',\n",
       "  'briefed',\n",
       "  'on',\n",
       "  'the',\n",
       "  'investigation',\n",
       "  'told',\n",
       "  'cnn',\n",
       "  'on',\n",
       "  'tuesday',\n",
       "  '.'],\n",
       " ['while',\n",
       "  'flying',\n",
       "  'was',\n",
       "  '``',\n",
       "  'a',\n",
       "  'big',\n",
       "  'part',\n",
       "  'of',\n",
       "  'his',\n",
       "  'life',\n",
       "  ',',\n",
       "  '``',\n",
       "  'the',\n",
       "  'source',\n",
       "  'said',\n",
       "  ',',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'only',\n",
       "  'one',\n",
       "  'theory',\n",
       "  'being',\n",
       "  'considered',\n",
       "  '.'],\n",
       " ['another',\n",
       "  'source',\n",
       "  ',',\n",
       "  'a',\n",
       "  'law',\n",
       "  'enforcement',\n",
       "  'official',\n",
       "  'briefed',\n",
       "  'on',\n",
       "  'the',\n",
       "  'investigation',\n",
       "  ',',\n",
       "  'also',\n",
       "  'told',\n",
       "  'cnn',\n",
       "  'that',\n",
       "  'authorities',\n",
       "  'believe',\n",
       "  'the',\n",
       "  'primary',\n",
       "  'motive',\n",
       "  'for',\n",
       "  'lubitz',\n",
       "  'to',\n",
       "  'bring',\n",
       "  'down',\n",
       "  'the',\n",
       "  'plane',\n",
       "  'was',\n",
       "  'that',\n",
       "  'he',\n",
       "  'feared',\n",
       "  'he',\n",
       "  'would',\n",
       "  'not',\n",
       "  'be',\n",
       "  'allowed',\n",
       "  'to',\n",
       "  'fly',\n",
       "  'because',\n",
       "  'of',\n",
       "  'his',\n",
       "  'medical',\n",
       "  'problems',\n",
       "  '.'],\n",
       " ['lubitz',\n",
       "  \"'s\",\n",
       "  'girlfriend',\n",
       "  'told',\n",
       "  'investigators',\n",
       "  'he',\n",
       "  'had',\n",
       "  'seen',\n",
       "  'an',\n",
       "  'eye',\n",
       "  'doctor',\n",
       "  'and',\n",
       "  'a',\n",
       "  'neuropsychologist',\n",
       "  ',',\n",
       "  'both',\n",
       "  'of',\n",
       "  'whom',\n",
       "  'deemed',\n",
       "  'him',\n",
       "  'unfit',\n",
       "  'to',\n",
       "  'work',\n",
       "  'recently',\n",
       "  'and',\n",
       "  'concluded',\n",
       "  'he',\n",
       "  'had',\n",
       "  'psychological',\n",
       "  'issues',\n",
       "  ',',\n",
       "  'the',\n",
       "  'european',\n",
       "  'government',\n",
       "  'official',\n",
       "  'said',\n",
       "  '.'],\n",
       " ['but',\n",
       "  'no',\n",
       "  'matter',\n",
       "  'what',\n",
       "  'details',\n",
       "  'emerge',\n",
       "  'about',\n",
       "  'his',\n",
       "  'previous',\n",
       "  'mental',\n",
       "  'health',\n",
       "  'struggles',\n",
       "  ',',\n",
       "  'there',\n",
       "  \"'s\",\n",
       "  'more',\n",
       "  'to',\n",
       "  'the',\n",
       "  'story',\n",
       "  ',',\n",
       "  'said',\n",
       "  'brian',\n",
       "  'russell',\n",
       "  ',',\n",
       "  'a',\n",
       "  'forensic',\n",
       "  'psychologist',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['psychology',\n",
       "  'can',\n",
       "  'explain',\n",
       "  'why',\n",
       "  'somebody',\n",
       "  'would',\n",
       "  'turn',\n",
       "  'rage',\n",
       "  'inward',\n",
       "  'on',\n",
       "  'themselves',\n",
       "  'about',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'maybe',\n",
       "  'they',\n",
       "  'were',\n",
       "  \"n't\",\n",
       "  'going',\n",
       "  'to',\n",
       "  'keep',\n",
       "  'doing',\n",
       "  'their',\n",
       "  'job',\n",
       "  'and',\n",
       "  'they',\n",
       "  \"'re\",\n",
       "  'upset',\n",
       "  'about',\n",
       "  'that',\n",
       "  'and',\n",
       "  'so',\n",
       "  'they',\n",
       "  \"'re\",\n",
       "  'suicidal',\n",
       "  ',',\n",
       "  '``',\n",
       "  'he',\n",
       "  'said',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['but',\n",
       "  'there',\n",
       "  'is',\n",
       "  'no',\n",
       "  'mental',\n",
       "  'illness',\n",
       "  'that',\n",
       "  'explains',\n",
       "  'why',\n",
       "  'somebody',\n",
       "  'then',\n",
       "  'feels',\n",
       "  'entitled',\n",
       "  'to',\n",
       "  'also',\n",
       "  'take',\n",
       "  'that',\n",
       "  'rage',\n",
       "  'and',\n",
       "  'turn',\n",
       "  'it',\n",
       "  'outward',\n",
       "  'on',\n",
       "  '149',\n",
       "  'other',\n",
       "  'people',\n",
       "  'who',\n",
       "  'had',\n",
       "  'nothing',\n",
       "  'to',\n",
       "  'do',\n",
       "  'with',\n",
       "  'the',\n",
       "  'person',\n",
       "  \"'s\",\n",
       "  'problems',\n",
       "  '.',\n",
       "  '``'],\n",
       " ['germanwings', 'crash', 'compensation', ':', 'what', 'we', 'know', '.'],\n",
       " ['who', 'was', 'the', 'captain', 'of', 'germanwings', 'flight', '9525', '?'],\n",
       " ['cnn',\n",
       "  \"'s\",\n",
       "  'margot',\n",
       "  'haddad',\n",
       "  'reported',\n",
       "  'from',\n",
       "  'marseille',\n",
       "  'and',\n",
       "  'pamela',\n",
       "  'brown',\n",
       "  'from',\n",
       "  'dusseldorf',\n",
       "  ',',\n",
       "  'while',\n",
       "  'laura',\n",
       "  'smith-spark',\n",
       "  'wrote',\n",
       "  'from',\n",
       "  'london',\n",
       "  '.'],\n",
       " ['cnn',\n",
       "  \"'s\",\n",
       "  'frederik',\n",
       "  'pleitgen',\n",
       "  ',',\n",
       "  'pamela',\n",
       "  'boykoff',\n",
       "  ',',\n",
       "  'antonia',\n",
       "  'mortensen',\n",
       "  ',',\n",
       "  'sandrine',\n",
       "  'amiel',\n",
       "  'and',\n",
       "  'anna-maja',\n",
       "  'rappard',\n",
       "  'contributed',\n",
       "  'to',\n",
       "  'this',\n",
       "  'report',\n",
       "  '.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"\"\"\n",
    "from utils_nlp.dataset.cnndm import CNNDMSummarization\n",
    "\n",
    "train_dataset, test_dataset = CNNDMSummarization(top_n=5)\n",
    "\n",
    "len(train_dataset)\n",
    "\n",
    "len(test_dataset)\n",
    "\n",
    "test_dataset[0]\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1028 19:36:39.891576 139886807013184 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1028 19:36:39.924971 139886807013184 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1028 19:36:39.943137 139886807013184 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.models.transformers.extractive_summarization import ExtSumProcessor, ExtractiveSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1028 19:36:40.130957 139886807013184 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at ./5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "processor = ExtSumProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "ext_sum_train = processor.preprocess(list(train_dataset), list(train_dataset.get_target()))\n",
    "\n",
    "ext_sum_test = processor.preprocess(list(test_dataset), list(test_dataset.get_target()))\n",
    "\n",
    "#type(ext_sum_train)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "BERT_DATA_PATH = BERT_DATA_PATH=\"./bert_data/\"\n",
    "pts = sorted(glob.glob(BERT_DATA_PATH + 'cnndm.train' + '.[0-9]*.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./bert_data/cnndm.train.0.bert.pt', './bert_data/cnndm.train.1.bert.pt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_dataset(file_list):\n",
    "    #random.shuffle(file_list)\n",
    "    for file in file_list:\n",
    "        yield torch.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertsum.models.data_loader  import DataIterator\n",
    "from bertsum.models import  model_builder, data_loader\n",
    "class Bunch(object):\n",
    "    \"\"\" Class which convert a dictionary to an object \"\"\"\n",
    "\n",
    "    def __init__(self, adict):\n",
    "        self.__dict__.update(adict)\n",
    "def get_data_loader(data_files, device, is_labeled=False, batch_size=3000):\n",
    "    \"\"\"\n",
    "    Function to get data iterator over a list of data objects.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of objects): a list of data objects.\n",
    "        is_test (bool): it specifies whether the data objects are labeled data.\n",
    "        batch_size (int): number of tokens per batch.\n",
    "        \n",
    "    Returns:\n",
    "        DataIterator\n",
    "\n",
    "    \"\"\"\n",
    "    args = Bunch({})\n",
    "    args.use_interval = True\n",
    "    args.batch_size = batch_size\n",
    "    data_iter = None\n",
    "    data_iter  = data_loader.Dataloader(args, get_dataset(data_files), args.batch_size,device, shuffle=False, is_test=is_labeled)\n",
    "    return data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:{}\".format(0)) \n",
    "def train_iter_func():\n",
    "    return get_data_loader(pts, device, is_labeled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1028 19:36:40.696836 139886807013184 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at ./a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I1028 19:36:40.697976 139886807013184 configuration_utils.py:168] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1028 19:36:40.822206 139886807013184 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin from cache at ./7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedModel,  PretrainedConfig, DistilBertModel, BertModel, DistilBertConfig\n",
    "summarizer = None\n",
    "summarizer = ExtractiveSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "DATA_FOLDER = \"./temp\"\n",
    "CACHE_DIR = \"./temp\"\n",
    "DEVICE = \"cuda\"\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "NUM_GPUS = 1\n",
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loss: 0.434776, time: 0.33, step 0.000000 out of total 10000.000000\n",
      "loss: 52.902908, time: 11.08, step 100.000000 out of total 10000.000000\n",
      "loss: 38.612158, time: 11.01, step 200.000000 out of total 10000.000000\n",
      "loss: 35.420277, time: 11.03, step 300.000000 out of total 10000.000000\n",
      "loss: 34.828366, time: 11.95, step 400.000000 out of total 10000.000000\n",
      "loss: 34.409282, time: 11.49, step 500.000000 out of total 10000.000000\n",
      "loss: 33.686915, time: 11.44, step 600.000000 out of total 10000.000000\n",
      "loss: 33.543083, time: 11.21, step 700.000000 out of total 10000.000000\n",
      "loss: 33.193735, time: 11.82, step 800.000000 out of total 10000.000000\n",
      "loss: 32.814421, time: 11.23, step 900.000000 out of total 10000.000000\n",
      "loss: 32.306093, time: 11.26, step 1000.000000 out of total 10000.000000\n",
      "loss: 32.094086, time: 11.12, step 1100.000000 out of total 10000.000000\n",
      "loss: 33.199048, time: 11.75, step 1200.000000 out of total 10000.000000\n",
      "loss: 31.443077, time: 11.44, step 1300.000000 out of total 10000.000000\n",
      "loss: 31.991770, time: 11.31, step 1400.000000 out of total 10000.000000\n",
      "loss: 31.662769, time: 11.10, step 1500.000000 out of total 10000.000000\n",
      "loss: 31.764520, time: 11.67, step 1600.000000 out of total 10000.000000\n",
      "loss: 31.710532, time: 11.08, step 1700.000000 out of total 10000.000000\n",
      "loss: 31.700363, time: 11.01, step 1800.000000 out of total 10000.000000\n",
      "loss: 31.749142, time: 11.17, step 1900.000000 out of total 10000.000000\n",
      "loss: 31.710789, time: 11.73, step 2000.000000 out of total 10000.000000\n",
      "loss: 32.272130, time: 11.49, step 2100.000000 out of total 10000.000000\n",
      "loss: 32.114992, time: 11.46, step 2200.000000 out of total 10000.000000\n",
      "loss: 32.081605, time: 11.38, step 2300.000000 out of total 10000.000000\n",
      "loss: 32.550155, time: 11.64, step 2400.000000 out of total 10000.000000\n",
      "loss: 32.364637, time: 11.14, step 2500.000000 out of total 10000.000000\n",
      "loss: 30.473291, time: 11.19, step 2600.000000 out of total 10000.000000\n",
      "loss: 31.429663, time: 11.13, step 2700.000000 out of total 10000.000000\n",
      "loss: 30.424942, time: 11.40, step 2800.000000 out of total 10000.000000\n",
      "loss: 31.258406, time: 11.07, step 2900.000000 out of total 10000.000000\n",
      "loss: 30.718246, time: 11.19, step 3000.000000 out of total 10000.000000\n",
      "loss: 31.152926, time: 11.18, step 3100.000000 out of total 10000.000000\n",
      "loss: 31.191213, time: 11.39, step 3200.000000 out of total 10000.000000\n",
      "loss: 31.664712, time: 10.99, step 3300.000000 out of total 10000.000000\n",
      "loss: 31.407538, time: 11.03, step 3400.000000 out of total 10000.000000\n",
      "loss: 30.724196, time: 11.00, step 3500.000000 out of total 10000.000000\n",
      "loss: 30.603053, time: 11.29, step 3600.000000 out of total 10000.000000\n",
      "loss: 31.262879, time: 10.96, step 3700.000000 out of total 10000.000000\n",
      "loss: 30.602100, time: 11.02, step 3800.000000 out of total 10000.000000\n",
      "loss: 30.812468, time: 11.03, step 3900.000000 out of total 10000.000000\n",
      "loss: 30.118705, time: 11.28, step 4000.000000 out of total 10000.000000\n",
      "loss: 30.472731, time: 10.94, step 4100.000000 out of total 10000.000000\n",
      "loss: 30.939562, time: 10.99, step 4200.000000 out of total 10000.000000\n",
      "loss: 29.950871, time: 10.91, step 4300.000000 out of total 10000.000000\n",
      "loss: 31.281369, time: 11.34, step 4400.000000 out of total 10000.000000\n",
      "loss: 29.662618, time: 10.91, step 4500.000000 out of total 10000.000000\n",
      "loss: 30.060169, time: 11.00, step 4600.000000 out of total 10000.000000\n",
      "loss: 29.785620, time: 11.03, step 4700.000000 out of total 10000.000000\n",
      "loss: 30.156811, time: 11.47, step 4800.000000 out of total 10000.000000\n",
      "loss: 29.637551, time: 11.01, step 4900.000000 out of total 10000.000000\n",
      "loss: 30.674142, time: 11.39, step 5000.000000 out of total 10000.000000\n",
      "loss: 28.572815, time: 11.27, step 5100.000000 out of total 10000.000000\n",
      "loss: 29.803984, time: 11.50, step 5200.000000 out of total 10000.000000\n",
      "loss: 29.971485, time: 11.08, step 5300.000000 out of total 10000.000000\n",
      "loss: 29.791215, time: 11.24, step 5400.000000 out of total 10000.000000\n",
      "loss: 29.964834, time: 11.38, step 5500.000000 out of total 10000.000000\n",
      "loss: 30.322712, time: 11.75, step 5600.000000 out of total 10000.000000\n",
      "loss: 29.500406, time: 11.32, step 5700.000000 out of total 10000.000000\n",
      "loss: 29.795636, time: 11.30, step 5800.000000 out of total 10000.000000\n",
      "loss: 29.980351, time: 11.26, step 5900.000000 out of total 10000.000000\n",
      "loss: 29.034165, time: 11.56, step 6000.000000 out of total 10000.000000\n",
      "loss: 29.320281, time: 11.05, step 6100.000000 out of total 10000.000000\n",
      "loss: 28.547546, time: 11.13, step 6200.000000 out of total 10000.000000\n",
      "loss: 28.926620, time: 11.10, step 6300.000000 out of total 10000.000000\n",
      "loss: 29.557800, time: 11.63, step 6400.000000 out of total 10000.000000\n",
      "loss: 29.143100, time: 11.11, step 6500.000000 out of total 10000.000000\n",
      "loss: 29.671277, time: 11.02, step 6600.000000 out of total 10000.000000\n",
      "loss: 30.009365, time: 11.08, step 6700.000000 out of total 10000.000000\n",
      "loss: 28.840488, time: 11.41, step 6800.000000 out of total 10000.000000\n",
      "loss: 28.830560, time: 11.03, step 6900.000000 out of total 10000.000000\n",
      "loss: 29.287735, time: 11.09, step 7000.000000 out of total 10000.000000\n",
      "loss: 30.114177, time: 11.15, step 7100.000000 out of total 10000.000000\n",
      "loss: 28.591488, time: 11.58, step 7200.000000 out of total 10000.000000\n",
      "loss: 29.200069, time: 11.10, step 7300.000000 out of total 10000.000000\n",
      "loss: 29.515540, time: 11.08, step 7400.000000 out of total 10000.000000\n",
      "loss: 29.605279, time: 11.07, step 7500.000000 out of total 10000.000000\n",
      "loss: 28.578301, time: 11.50, step 7600.000000 out of total 10000.000000\n",
      "loss: 28.892440, time: 11.09, step 7700.000000 out of total 10000.000000\n",
      "loss: 28.933030, time: 10.96, step 7800.000000 out of total 10000.000000\n",
      "loss: 28.665656, time: 11.05, step 7900.000000 out of total 10000.000000\n",
      "loss: 29.566401, time: 11.80, step 8000.000000 out of total 10000.000000\n",
      "loss: 28.564517, time: 11.21, step 8100.000000 out of total 10000.000000\n",
      "loss: 28.593431, time: 11.01, step 8200.000000 out of total 10000.000000\n",
      "loss: 28.926263, time: 11.04, step 8300.000000 out of total 10000.000000\n",
      "loss: 28.860379, time: 11.44, step 8400.000000 out of total 10000.000000\n",
      "loss: 30.003897, time: 11.05, step 8500.000000 out of total 10000.000000\n",
      "loss: 29.035807, time: 11.06, step 8600.000000 out of total 10000.000000\n",
      "loss: 29.180781, time: 10.97, step 8700.000000 out of total 10000.000000\n",
      "loss: 28.666833, time: 11.44, step 8800.000000 out of total 10000.000000\n",
      "loss: 28.866304, time: 11.06, step 8900.000000 out of total 10000.000000\n",
      "loss: 27.299687, time: 10.98, step 9000.000000 out of total 10000.000000\n",
      "loss: 29.205652, time: 10.99, step 9100.000000 out of total 10000.000000\n",
      "loss: 29.167503, time: 11.35, step 9200.000000 out of total 10000.000000\n",
      "loss: 28.922509, time: 11.05, step 9300.000000 out of total 10000.000000\n",
      "loss: 28.488780, time: 11.10, step 9400.000000 out of total 10000.000000\n",
      "loss: 28.075319, time: 10.97, step 9500.000000 out of total 10000.000000\n",
      "loss: 28.700666, time: 11.36, step 9600.000000 out of total 10000.000000\n",
      "loss: 29.089914, time: 11.04, step 9700.000000 out of total 10000.000000\n",
      "loss: 29.031871, time: 10.99, step 9800.000000 out of total 10000.000000\n",
      "loss: 28.807646, time: 11.04, step 9900.000000 out of total 10000.000000\n",
      "loss: 28.629186, time: 11.33, step 10000.000000 out of total 10000.000000\n"
     ]
    }
   ],
   "source": [
    "### from utils_nlp.common.timer import Timer\n",
    "\n",
    "    summarizer.fit(\n",
    "            train_iter_func,\n",
    "            device= DEVICE,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_gpus=NUM_GPUS,\n",
    "            max_steps=1e4,\n",
    "            learning_rate=1e-4,\n",
    "            warmup_steps=1e4*0.5,\n",
    "            verbose=True,\n",
    "            report_every=100,\n",
    "        )\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(summarizer.model, \"cnndm_transformersum_distilbert-base-uncased_bertsum_processed_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils_nlp.models.bert.extractive_text_summarization import get_data_iter\n",
    "import os\n",
    "\n",
    "test_dataset=[]\n",
    "for i in range(0,6):\n",
    "    filename = os.path.join(BERT_DATA_PATH, \"cnndm.test.{0}.bert.pt\".format(i))\n",
    "    test_dataset.extend(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = summarizer.predict(get_data_iter(test_dataset),\n",
    "                               device=DEVICE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11489\n",
      "11489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 20:05:12,147 [MainThread  ] [INFO ]  Writing summaries.\n",
      "I1028 20:05:12.147566 139886807013184 pyrouge.py:525] Writing summaries.\n",
      "2019-10-28 20:05:12,149 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./results/tmpnapu1ceb/system and model files to ./results/tmpnapu1ceb/model.\n",
      "I1028 20:05:12.149479 139886807013184 pyrouge.py:518] Processing summaries. Saving system files to ./results/tmpnapu1ceb/system and model files to ./results/tmpnapu1ceb/model.\n",
      "2019-10-28 20:05:12,150 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-10-28-20-05-11/candidate/.\n",
      "I1028 20:05:12.150503 139886807013184 pyrouge.py:43] Processing files in ./results/rouge-tmp-2019-10-28-20-05-11/candidate/.\n",
      "2019-10-28 20:05:13,204 [MainThread  ] [INFO ]  Saved processed files to ./results/tmpnapu1ceb/system.\n",
      "I1028 20:05:13.204638 139886807013184 pyrouge.py:53] Saved processed files to ./results/tmpnapu1ceb/system.\n",
      "2019-10-28 20:05:13,206 [MainThread  ] [INFO ]  Processing files in ./results/rouge-tmp-2019-10-28-20-05-11/reference/.\n",
      "I1028 20:05:13.206232 139886807013184 pyrouge.py:43] Processing files in ./results/rouge-tmp-2019-10-28-20-05-11/reference/.\n",
      "2019-10-28 20:05:14,254 [MainThread  ] [INFO ]  Saved processed files to ./results/tmpnapu1ceb/model.\n",
      "I1028 20:05:14.254149 139886807013184 pyrouge.py:53] Saved processed files to ./results/tmpnapu1ceb/model.\n",
      "2019-10-28 20:05:14,614 [MainThread  ] [INFO ]  Written ROUGE configuration to ./results/tmph6tt42ke/rouge_conf.xml\n",
      "I1028 20:05:14.614730 139886807013184 pyrouge.py:354] Written ROUGE configuration to ./results/tmph6tt42ke/rouge_conf.xml\n",
      "2019-10-28 20:05:14,616 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmph6tt42ke/rouge_conf.xml\n",
      "I1028 20:05:14.616016 139886807013184 pyrouge.py:372] Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./results/tmph6tt42ke/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.53918 (95%-conf.int. 0.53643 - 0.54191)\n",
      "1 ROUGE-1 Average_P: 0.36463 (95%-conf.int. 0.36234 - 0.36693)\n",
      "1 ROUGE-1 Average_F: 0.42091 (95%-conf.int. 0.41880 - 0.42302)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.24454 (95%-conf.int. 0.24200 - 0.24735)\n",
      "1 ROUGE-2 Average_P: 0.16532 (95%-conf.int. 0.16341 - 0.16740)\n",
      "1 ROUGE-2 Average_F: 0.19061 (95%-conf.int. 0.18852 - 0.19274)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.49144 (95%-conf.int. 0.48881 - 0.49416)\n",
      "1 ROUGE-L Average_P: 0.33284 (95%-conf.int. 0.33057 - 0.33513)\n",
      "1 ROUGE-L Average_F: 0.38398 (95%-conf.int. 0.38192 - 0.38601)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.eval.evaluate_summarization import get_rouge\n",
    "rouge_baseline = get_rouge(prediction, target, \"./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program in italy when the incident happened in january .<q>he was flown back to chicago via air ambulance on march 20 , but he died on sunday .<q>a university of iowa student has died nearly three months after a fall in rome in a suspected robbery attack in rome .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrew mogni , 20 , from glen ellyn , illinois , had only just arrived for a semester program when the incident happened in january<q>he was flown back to chicago via air on march 20 but he died on sunday<q>initial police reports indicated the fall was an accident but authorities are investigating the possibility that mogni was robbed<q>his cousin claims he was attacked and thrown 40ft from a bridge'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['tgt_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6 cm3",
   "language": "python",
   "name": "cm3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
