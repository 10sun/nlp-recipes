{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Summarization on CNN/DM Dataset using Transformer Version of BertSum\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook demonstrates how to fine tune Transformers for extractive text summarization. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, result postprocessing, and model evaluation.\n",
    "\n",
    "BertSum refers to  [Fine-tune BERT for Extractive Summarization (https://arxiv.org/pdf/1903.10318.pdf) with [published example](https://github.com/nlpyang/BertSum/). And the Transformer version of Bertsum refers to our modification of BertSum and the source code can be accessed at (https://github.com/daden-ms/BertSum/). \n",
    "\n",
    "Extractive summarization are usually used in document summarization where each input document consists of mutiple sentences. The preprocessing of the input training data involves assigning label 0 or 1 to the document sentences based on the give summary. The summarization problem is also simplfied to classifying whether each document sentence should be included in the summary. \n",
    "\n",
    "The figure below illustrates how BERTSum can be fine tuned for extractive summarization task. Each sentence is inserted with [CLS] token at the beginning and  [SEP] at the end. Interval segment embedding and positional embedding are added upon the token embedding before input the BERT model. The [CLS] token representation is used as sentence embedding and only the [CLS] tokens are used as input for the summarization model. The summarization layer predicts whether the probability of each each sentence token should be included in the summary or not. Techniques like trigram blocking can be used to improve model accuarcy.   \n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/BertSum.PNG\">\n",
    "\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "The running time shown in this notebook is on a Standard_NC24s_v3 Azure Deep Learning Virtual Machine with 4 NVIDIA Tesla V100 GPUs. \n",
    "> **Tip**: If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`** to run the notebook on a small subset of the data and a smaller number of epochs. \n",
    "\n",
    "The table below provides some reference running time on different machine configurations.  \n",
    "\n",
    "|QUICK_RUN|USE_PREPROCESSED_DATA|encoder|Machine Configurations|Running time|\n",
    "|:---------|:---------|:---------|:----------------------|:------------|\n",
    "|True|True|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 20 minutes |\n",
    "|False|True|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 60 minutes |\n",
    "|True|False|baseline|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 20 minutes |\n",
    "|True|True|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 80 minutes |\n",
    "|False|True|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 6.5hours |\n",
    "|True|False|transformer|1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| ~ 80 minutes |\n",
    "|False|False|any| 1 NVIDIA Tesla V100 GPUs, 16GB GPU memory| > 24 hours|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = True\n",
    "USE_PREPROCESSED_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Before we start the notebook, we should set the environment variable to make sure you can access the GPUs on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "nlp_path = os.path.abspath(\"../../\")\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "sys.path.insert(0, \"./\")\n",
    "sys.path.insert(0, \"/dadendev/nlp/examples/text_summarization/BertSum/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/dadendev/nlp/examples/text_summarization/BertSum/', './', '/dadendev/nlp', '/dadendev/anaconda3/envs/cm3/lib/python36.zip', '/dadendev/anaconda3/envs/cm3/lib/python3.6', '/dadendev/anaconda3/envs/cm3/lib/python3.6/lib-dynload', '', '/home/daden/.local/lib/python3.6/site-packages', '/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages', '/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages/pyrouge-0.1.3-py3.6.egg', '/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages/IPython/extensions', '/home/daden/.ipython']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to install the dependencies for pyrouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://azure.archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:2 http://azure.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:3 http://azure.archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Hit:4 https://packages.microsoft.com/repos/microsoft-ubuntu-xenial-prod xenial InRelease\n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \n",
      "Ign:6 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:7 http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Fetched 252 kB in 1s (392 kB/s)                              \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "expat is already the newest version (2.2.5-3ubuntu0.2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  linux-azure-cloud-tools-5.0.0-1018 linux-azure-cloud-tools-5.0.0-1020\n",
      "  linux-azure-headers-5.0.0-1018 linux-azure-tools-5.0.0-1018\n",
      "  linux-azure-tools-5.0.0-1020\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Note, selecting 'libexpat1-dev' instead of 'libexpat-dev'\n",
      "libexpat1-dev is already the newest version (2.2.5-3ubuntu0.2).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  linux-azure-cloud-tools-5.0.0-1018 linux-azure-cloud-tools-5.0.0-1020\n",
      "  linux-azure-headers-5.0.0-1018 linux-azure-tools-5.0.0-1018\n",
      "  linux-azure-tools-5.0.0-1020\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# dependencies for ROUGE-1.5.5.pl\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install expat\n",
    "!sudo apt-get install libexpat-dev -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in your terminal to install pre-requiste for using pyrouge.\n",
    "1. sudo cpan install XML::Parser\n",
    "1. sudo cpan install XML::Parser::PerlSAX\n",
    "1. sudo cpan install XML::DOM\n",
    "\n",
    "Download ROUGE-1.5.5 from https://github.com/andersjo/pyrouge/tree/master/tools/ROUGE-1.5.5.\n",
    "Run the following command in your terminal.\n",
    "* pyrouge_set_rouge_path $ABSOLUTE_DIRECTORY_TO_ROUGE-1.5.5.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprossing\n",
    "\n",
    "The dataset we used for this notebook is CNN/DM dataset which contains the documents and accompanying questions from the news articles of CNN and Daily mail. The highlights in each article are used as summary. The dataset consits of ~289K training examples, ~11K valiation and ~11K test dataset.  You can choose the [Option 1] below preprocess the data or [Option 2] to use the preprocessed version at [BERTSum published example](https://github.com/nlpyang/BertSum/). You don't need to manually download any of these two data sets as the code below will handle this part.  Since it takes up to 28 hours to preprocess the training data  to run on 10  Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz, we suggest you continue with set as True first and experiment with data preprocessing  with QUICKRUN set as True.\n",
    "\n",
    "##### Details of Data Preprocessing\n",
    "\n",
    "The purpose of preprocessing is to process the input articles to the format that BertSum takes.  Functions defined specific in harvardnlp_cnndm_preprocess function are unique to CNN/DM dataset that's processed by harvardnlp. However, it provides a skeleton of how to preprocessing data into the format that BertSum takes. Assuming you have all articles and target summery each in a file, line-breaker seperated, the steps to preprocess the data are:\n",
    "1. sentence tokenization\n",
    "2. word tokenization\n",
    "3. label the sentences in the article with 1 meaning the sentence is selected and 0 meaning the sentence is not selected. The options for the selection algorithms are \"greedy\" and \"combination\"\n",
    "3. convert each example to  BertSum format\n",
    "    - filter the sentences in the example based on the min_src_ntokens argument. If the lefted total sentence number is less than min_nsents, the example is discarded.\n",
    "    - truncate the sentences in the example if the length is greater than max_src_ntokens\n",
    "    - truncate the sentences in the example and the labels if the totle number of sentences is greater than max_nsents\n",
    "    - [CLS] and [SEP] are inserted before and after each sentence\n",
    "    - wordPiece tokenization\n",
    "    - truncate the example to 512 tokens\n",
    "    - convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary.\n",
    "    - segment ids are generated\n",
    "    - [CLS] token positions are logged\n",
    "    - [CLS] token labels are truncated if it's greater than 512, which is the maximum input length that can be taken by the BERT model.\n",
    "    \n",
    "    \n",
    "Note that the original BERTSum paper use Stanford CoreNLP for data proprocessing, here we'll first how to use NLTK version, and then we also provide instruction of how to set up Stanford NLP and code examples of how to use Standford CoreNLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Option 1] Preprocess  data\n",
    "The code in following cell will download the CNN/DM dataset listed at https://github.com/harvardnlp/sent-summary/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daden/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "I1030 19:42:44.432886 140491949197120 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1030 19:42:44.466703 140491949197120 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1030 19:42:44.483136 140491949197120 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1030 19:42:44.489104 140491949197120 utils.py:173] Opening tar file .data/cnndm.tar.gz.\n",
      "I1030 19:42:44.490219 140491949197120 utils.py:181] .data/test.txt.src already extracted.\n",
      "I1030 19:42:44.778111 140491949197120 utils.py:181] .data/test.txt.tgt.tagged already extracted.\n",
      "I1030 19:42:44.804572 140491949197120 utils.py:181] .data/train.txt.src already extracted.\n",
      "I1030 19:42:52.298821 140491949197120 utils.py:181] .data/train.txt.tgt.tagged already extracted.\n",
      "I1030 19:42:52.910166 140491949197120 utils.py:181] .data/val.txt.src already extracted.\n",
      "I1030 19:42:53.247699 140491949197120 utils.py:181] .data/val.txt.tgt.tagged already extracted.\n"
     ]
    }
   ],
   "source": [
    "if QUICK_RUN:\n",
    "    top_n = 100\n",
    "from utils_nlp.dataset.cnndm import CNNDMSummarization\n",
    "\n",
    "train_dataset, test_dataset = CNNDMSummarization(top_n=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data and save the data to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1030 19:42:54.824942 140491949197120 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ./26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.models.transformers.extractive_summarization import ExtSumProcessor\n",
    "\n",
    "processor = ExtSumProcessor(model_name=\"distilbert-base-uncased\")\n",
    "ext_sum_train = processor.preprocess(train_dataset, train_dataset.get_target())\n",
    "ext_sum_test = processor.preprocess(test_dataset, test_dataset.get_target())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./temp_data4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nlp.dataset.cnndm import CNNDMBertSumProcessedData\n",
    "\n",
    "train_files = CNNDMBertSumProcessedData.save_data(\n",
    "    ext_sum_train, is_test=False, path_and_prefix=data_path, chunk_size=25\n",
    ")\n",
    "test_files = CNNDMBertSumProcessedData.save_data(\n",
    "    ext_sum_test, is_test=True, path_and_prefix=data_path, chunk_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./temp_data4/_0_train',\n",
       " './temp_data4/_1_train',\n",
       " './temp_data4/_2_train',\n",
       " './temp_data4/_3_train']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./temp_data4/_0_test']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = CNNDMBertSumProcessedData().splits(root=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Option 2] Reuse Preprocess  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1030 19:42:57.929424 140491949197120 utils.py:88] Downloading from Google Drive; may take a few minutes\n",
      "I1030 19:42:58.700399 140491949197120 utils.py:60] File ./temp_data5/bertsum_data.zip already exists.\n"
     ]
    }
   ],
   "source": [
    "if USE_PREPROCESSED_DATA:\n",
    "    data_path = \"./temp_data5/\"\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    CNNDMBertSumProcessedData.download(local_path=data_path)\n",
    "    train_iter, test_iter = CNNDMBertSumProcessedData().splits(root=data_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_dataset at 0x7fc616e03f68>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./temp_data4/_0_train',\n",
       " './temp_data4/_1_train',\n",
       " './temp_data4/_2_train',\n",
       " './temp_data4/_3_train']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['src', 'labels', 'segs', 'clss', 'src_txt', 'tgt_txt'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "bert_format_data = torch.load(train_files[0])\n",
    "print(len(bert_format_data))\n",
    "bert_format_data[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_format_data[0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "To start model training, we need to create a instance of ExtractiveSummarizer.\n",
    "#### Choose the transformer model.\n",
    "Currently ExtractiveSummarizer support two models:\n",
    "- distilbert-base-uncase, \n",
    "- bert-base-uncase\n",
    "\n",
    "Potentionally, roberta-based model and xlnet can be supported but needs to be tested.\n",
    "#### Choose the encoder algorithm.\n",
    "There are four options:\n",
    "- baseline: it used a smaller transformer model to replace the bert model and with transformer summarization layer\n",
    "- classifier: it uses pretrained BERT and fine-tune BERT with **simple logistic classification** summarization layer\n",
    "- transformer: it uses pretrained BERT and fine-tune BERT with **transformer** summarization layer\n",
    "- RNN: it uses pretrained BERT and fine-tune BERT with **LSTM** summarization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "DATA_FOLDER = \"./temp\"\n",
    "CACHE_DIR = \"./temp\"\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 3000\n",
    "NUM_GPUS = 1\n",
    "encoder = \"transformer\"\n",
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1030 19:43:17.238647 140491949197120 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at ./temp/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I1030 19:43:17.239963 140491949197120 configuration_utils.py:168] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1030 19:43:17.363380 140491949197120 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin from cache at ./temp/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp.models.transformers.extractive_summarization import ExtractiveSummarizer\n",
    "summarizer = ExtractiveSummarizer(model_name, encoder, CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./temp_data4/_0_train',\n",
       " './temp_data4/_1_train',\n",
       " './temp_data4/_2_train',\n",
       " './temp_data4/_3_train']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils_nlp.models.transformers.extractive_summarization import  get_dataset, get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "loss: 80.055855, time: 21.066953, examples number: 5.000000, step 100.000000 out of total 10000.000000\n",
      "loss: 0.210719, time: 0.097609, examples number: 5.000000, step 100.000000 out of total 10000.000000\n",
      "loss: 37.006704, time: 21.019811, examples number: 7.000000, step 200.000000 out of total 10000.000000\n",
      "loss: 0.189547, time: 0.085794, examples number: 6.000000, step 200.000000 out of total 10000.000000\n",
      "loss: 34.051729, time: 20.893883, examples number: 5.000000, step 300.000000 out of total 10000.000000\n",
      "loss: 0.154256, time: 0.098439, examples number: 5.000000, step 300.000000 out of total 10000.000000\n",
      "loss: 33.769536, time: 21.259850, examples number: 5.000000, step 400.000000 out of total 10000.000000\n",
      "loss: 0.194510, time: 0.098374, examples number: 5.000000, step 400.000000 out of total 10000.000000\n",
      "loss: 33.363134, time: 20.839859, examples number: 5.000000, step 500.000000 out of total 10000.000000\n",
      "loss: 0.127341, time: 0.104356, examples number: 5.000000, step 500.000000 out of total 10000.000000\n",
      "loss: 32.484810, time: 21.217096, examples number: 5.000000, step 600.000000 out of total 10000.000000\n",
      "loss: 0.158462, time: 0.099393, examples number: 5.000000, step 600.000000 out of total 10000.000000\n",
      "loss: 32.207327, time: 20.770870, examples number: 5.000000, step 700.000000 out of total 10000.000000\n",
      "loss: 0.139624, time: 0.098159, examples number: 5.000000, step 700.000000 out of total 10000.000000\n",
      "loss: 31.975883, time: 21.158446, examples number: 5.000000, step 800.000000 out of total 10000.000000\n",
      "loss: 0.126678, time: 0.097939, examples number: 5.000000, step 800.000000 out of total 10000.000000\n",
      "loss: 31.714358, time: 20.730140, examples number: 5.000000, step 900.000000 out of total 10000.000000\n",
      "loss: 0.136615, time: 0.098182, examples number: 5.000000, step 900.000000 out of total 10000.000000\n",
      "loss: 32.101234, time: 20.998002, examples number: 5.000000, step 1000.000000 out of total 10000.000000\n",
      "loss: 0.146306, time: 0.099846, examples number: 5.000000, step 1000.000000 out of total 10000.000000\n",
      "loss: 31.611188, time: 20.601194, examples number: 5.000000, step 1100.000000 out of total 10000.000000\n",
      "loss: 0.117829, time: 0.097640, examples number: 5.000000, step 1100.000000 out of total 10000.000000\n",
      "loss: 31.863980, time: 20.959873, examples number: 5.000000, step 1200.000000 out of total 10000.000000\n",
      "loss: 0.152569, time: 0.097665, examples number: 5.000000, step 1200.000000 out of total 10000.000000\n"
     ]
    }
   ],
   "source": [
    "### from utils_nlp.common.timer import Timer\n",
    "#\"\"\"\n",
    "summarizer.fit(\n",
    "            train_iter,\n",
    "            device= DEVICE,\n",
    "            batch_size=3000,\n",
    "            num_gpus=NUM_GPUS,\n",
    "            gradient_accumulation_steps=2,\n",
    "            max_steps=1e4,\n",
    "            lr=2e-3,\n",
    "            warmup_steps=1e4*0.5,\n",
    "            verbose=True,\n",
    "            report_every=100,\n",
    "        )\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(summarizer.model, \"cnndm_transformersum_bert-base-uncased_bertsum_processed_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "summarizer.model = torch.load(\"cnndm_transformersum_distilbert-base-uncased_bertsum_processed_data.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "[ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)), or Recall-Oriented Understudy for Gisting Evaluation has been commonly used for evaluation text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils_nlp.models.bert.extractive_text_summarization import get_data_iter\n",
    "import os\n",
    "\n",
    "test_dataset=[]\n",
    "for i in range(0,6):\n",
    "    filename = os.path.join(BERT_DATA_PATH, \"cnndm.test.{0}.bert.pt\".format(i))\n",
    "    test_dataset.extend(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = summarizer.predict(get_data_iter(test_dataset),\n",
    "                               device=DEVICE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [test_dataset[i]['tgt_txt'] for i in range(len(test_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils_nlp.eval.evaluate_summarization import get_rouge\n",
    "rouge_transformer = get_rouge(prediction, target, \"./results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]['tgt_txt']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6 cm3",
   "language": "python",
   "name": "cm3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
