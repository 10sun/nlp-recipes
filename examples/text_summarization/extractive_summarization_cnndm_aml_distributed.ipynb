{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training For Extractive Summarization on CNN/DM Dataset\n",
    "\n",
    "## Summary\n",
    "This notebook demonstrates how to use Azure Machine Learning to run distributed training using Distributed Data Parallel in Pytorch. For more detailed model related information, please see [extractive_summarization_cnndm_transformer.ipynb](extractive_summarization_cnndm_transformer.ipynb)\n",
    "\n",
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, refer to the [Configuration Notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) first if you haven't already to establish your connection to the AzureML Workspace. Prerequisites are:\n",
    "\n",
    "- Azure subscription\n",
    "- Azure Machine Learning Workspace\n",
    "- Azure Machine Learning SDK\n",
    "\n",
    "To run rouge evaluation, please refer to the section of compute_rouge_perl in [summarization_evaluation.ipynb](summarization_evaluation.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML Workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core:PipelineRun._from_dto with exception (azureml-core 1.0.83 (/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages), Requirement.parse('azureml-core==1.0.57.*')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core:StepRun._from_reused_dto with exception (azureml-core 1.0.83 (/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages), Requirement.parse('azureml-core==1.0.57.*')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core:StepRun._from_dto with exception (azureml-core 1.0.83 (/dadendev/anaconda3/envs/cm3/lib/python3.6/site-packages), Requirement.parse('azureml-core==1.0.57.*')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.83\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "nlp_path = os.path.abspath('../../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "    \n",
    "from utils_nlp.azureml import azureml_utils\n",
    "\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.dnn import Nccl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace the following constants with your own \n",
    "WORKSPACE_NAME = \"daden1amlws\"\n",
    "SUBSRIPTION_ID = \"9086b59a-02d7-4687-b3fd-e39fa5e0fd9b\" \n",
    "RESOURCE_GROUP = \"daden1aml\"\n",
    "LOCATION = \"eastus2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the workspace using the specified parameters\n",
    "ws = Workspace.create(name = WORKSPACE_NAME,\n",
    "                      subscription_id = SUBSRIPTION_ID,\n",
    "                      resource_group = RESOURCE_GROUP, \n",
    "                      location = LOCATION,\n",
    "                      create_resource_group = False,\n",
    "                      exist_ok = True)\n",
    "ws.get_details()\n",
    "\n",
    "# write the details of the workspace to a configuration file to the notebook library\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "setup() is now deprecated. Instead, please use create() to create a new workspace, or get()/from_config() to retrieve an existing one\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: daden1amlws\n",
      "Workspace region: eastus2\n",
      "Subscription id: 9086b59a-02d7-4687-b3fd-e39fa5e0fd9b\n",
      "Resource group: daden1aml\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the workspace\n",
    "ws = Workspace.setup()\n",
    "\n",
    "# Print the workspace attributes\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Workspace region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace the following constants with your own \n",
    "AMLCOMPUTE_CLUSTER_NAME = \"extsum5\"\n",
    "NODE_COUNT = 4\n",
    "VM_SIZE = 'STANDARD_NC6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 4, 'targetNodeCount': 4, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 4, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-01-30T05:01:54.385000+00:00', 'errors': None, 'creationTime': '2020-01-23T04:50:26.160743+00:00', 'modifiedTime': '2020-01-23T20:31:35.349184+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT1200S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gpu_compute_target = ComputeTarget(workspace=ws, name=AMLCOMPUTE_CLUSTER_NAME)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=VM_SIZE,\n",
    "                                                           max_nodes=NODE_COUNT,\n",
    "                                                           NodeIdleTimeBeforeScaleDown='PT1200S')\n",
    "\n",
    "    # create the cluster\n",
    "    gpu_compute_target = ComputeTarget.create(ws, AMLCOMPUTE_CLUSTER_NAME, compute_config)\n",
    "\n",
    "    gpu_compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute. \n",
    "print(gpu_compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace the following constants with your own \n",
    "EXPERIMENT_NAME = 'NLP-ExtSum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, name=EXPERIMENT_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset to local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## local folder to save the downloaded data\n",
    "LOCAL_DATA_FOLDER = '/dadendev/bertsumdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {LOCAL_DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bertsum_data.zip: 869MB [00:25, 33.5MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/dadendev/bertsumdata/'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_nlp.dataset.cnndm import CNNDMBertSumProcessedData, CNNDMSummarizationDataset\n",
    "CNNDMBertSumProcessedData.download(local_path=LOCAL_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the downloaded dataset to AML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## folder in the workspace where the data is uploaded to\n",
    "TARGET_DATA_FOLDER = '/bertsumdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 158 files\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.22.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.78.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.88.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.test.1.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.51.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.5.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.120.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.114.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.140.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.87.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.104.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.94.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.59.bert.pt\n",
      "Uploading /dadendev/bertsumdata/bertsum_data.zip\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.30.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.44.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.73.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.56.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.58.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.83.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.test.4.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.41.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.124.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.125.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.valid.4.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.18.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.86.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.78.bert.pt, 1 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.88.bert.pt, 2 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.test.2.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.142.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.22.bert.pt, 3 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.131.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.test.1.bert.pt, 4 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.test.0.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.110.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.111.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.5.bert.pt, 5 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.66.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.97.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.120.bert.pt, 6 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.27.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.33.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.114.bert.pt, 7 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.39.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.43.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.140.bert.pt, 8 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.8.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.104.bert.pt, 9 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.87.bert.pt, 10 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.51.bert.pt, 11 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.136.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.35.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.94.bert.pt, 12 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.67.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.89.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.59.bert.pt, 13 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.99.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.30.bert.pt, 14 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.10.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.73.bert.pt, 15 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.101.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.44.bert.pt, 16 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.117.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.56.bert.pt, 17 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.118.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.58.bert.pt, 18 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.141.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.83.bert.pt, 19 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.20.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.test.2.bert.pt, 20 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.123.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.test.4.bert.pt, 21 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.25.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.86.bert.pt, 22 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.138.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.41.bert.pt, 23 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.47.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.test.0.bert.pt, 24 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.108.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.124.bert.pt, 25 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.50.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.131.bert.pt, 26 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.52.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.125.bert.pt, 27 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.65.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.valid.4.bert.pt, 28 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.132.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.66.bert.pt, 29 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.71.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.18.bert.pt, 30 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.77.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.97.bert.pt, 31 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.28.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.142.bert.pt, 32 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.81.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.33.bert.pt, 33 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.42.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.110.bert.pt, 34 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.113.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.8.bert.pt, 35 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.6.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.111.bert.pt, 36 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.75.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.39.bert.pt, 37 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.0.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.67.bert.pt, 38 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.43.bert.pt, 39 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.121.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.14.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.99.bert.pt, 40 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.84.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.89.bert.pt, 41 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.85.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.27.bert.pt, 42 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.9.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.35.bert.pt, 43 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.136.bert.pt, 44 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.119.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.129.bert.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded /dadendev/bertsumdata/cnndm.train.10.bert.pt, 45 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.130.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.101.bert.pt, 46 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.116.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.117.bert.pt, 47 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.13.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.118.bert.pt, 48 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.15.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.20.bert.pt, 49 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.19.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.25.bert.pt, 50 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.141.bert.pt, 51 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.2.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.23.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.123.bert.pt, 52 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.115.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.138.bert.pt, 53 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.126.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.47.bert.pt, 54 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.50.bert.pt, 55 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.133.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.34.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.108.bert.pt, 56 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.29.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.132.bert.pt, 57 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.53.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.52.bert.pt, 58 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.60.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.65.bert.pt, 59 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.102.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.71.bert.pt, 60 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.3.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.77.bert.pt, 61 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.76.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.28.bert.pt, 62 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.26.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.113.bert.pt, 63 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.107.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.42.bert.pt, 64 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.68.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.81.bert.pt, 65 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.55.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.75.bert.pt, 66 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.128.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.6.bert.pt, 67 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.143.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.84.bert.pt, 68 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.12.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.14.bert.pt, 69 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.122.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.9.bert.pt, 70 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.127.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.121.bert.pt, 71 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.134.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.0.bert.pt, 72 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.46.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.85.bert.pt, 73 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.63.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.119.bert.pt, 74 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.7.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.129.bert.pt, 75 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.61.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.130.bert.pt, 76 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.90.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.13.bert.pt, 77 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.100.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.2.bert.pt, 78 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.137.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.116.bert.pt, 79 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.139.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.19.bert.pt, 80 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.24.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.23.bert.pt, 81 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.37.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.15.bert.pt, 82 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.57.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.143.bert.pt, 83 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.103.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.126.bert.pt, 84 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.17.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.115.bert.pt, 85 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.test.3.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.133.bert.pt, 86 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.64.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.60.bert.pt, 87 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.72.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.53.bert.pt, 88 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.34.bert.pt, 89 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.109.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.80.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.29.bert.pt, 90 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.91.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.76.bert.pt, 91 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.40.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.102.bert.pt, 92 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.92.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.3.bert.pt, 93 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.11.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.68.bert.pt, 94 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.105.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.26.bert.pt, 95 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.45.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.107.bert.pt, 96 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.49.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.55.bert.pt, 97 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.1.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.128.bert.pt, 98 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.112.bert.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded /dadendev/bertsumdata/cnndm.train.46.bert.pt, 99 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.test.5.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.127.bert.pt, 100 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.134.bert.pt, 101 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.32.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.122.bert.pt, 102 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.48.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.62.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.12.bert.pt, 103 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.69.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.63.bert.pt, 104 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.4.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.90.bert.pt, 105 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.7.bert.pt, 106 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.70.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.74.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.61.bert.pt, 107 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.31.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.57.bert.pt, 108 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.106.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.139.bert.pt, 109 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.135.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.100.bert.pt, 110 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.16.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.24.bert.pt, 111 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.21.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.37.bert.pt, 112 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.36.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.64.bert.pt, 113 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.38.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.17.bert.pt, 114 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.54.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.test.3.bert.pt, 115 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.79.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.103.bert.pt, 116 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.82.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.72.bert.pt, 117 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.93.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.80.bert.pt, 118 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.95.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.91.bert.pt, 119 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.96.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.92.bert.pt, 120 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.train.98.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.109.bert.pt, 121 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.40.bert.pt, 122 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.valid.0.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.valid.1.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.105.bert.pt, 123 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.11.bert.pt, 124 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.valid.2.bert.pt\n",
      "Uploading /dadendev/bertsumdata/cnndm.valid.3.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.49.bert.pt, 125 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.valid.5.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.137.bert.pt, 126 files out of an estimated total of 158\n",
      "Uploading /dadendev/bertsumdata/cnndm.valid.6.bert.pt\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.45.bert.pt, 127 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.112.bert.pt, 128 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.1.bert.pt, 129 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.test.5.bert.pt, 130 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.48.bert.pt, 131 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.4.bert.pt, 132 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.62.bert.pt, 133 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.69.bert.pt, 134 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.70.bert.pt, 135 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.32.bert.pt, 136 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.31.bert.pt, 137 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.106.bert.pt, 138 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.16.bert.pt, 139 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.36.bert.pt, 140 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.38.bert.pt, 141 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.valid.6.bert.pt, 142 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.93.bert.pt, 143 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.79.bert.pt, 144 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.135.bert.pt, 145 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.82.bert.pt, 146 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.95.bert.pt, 147 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.valid.1.bert.pt, 148 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.valid.0.bert.pt, 149 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.74.bert.pt, 150 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.valid.2.bert.pt, 151 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.21.bert.pt, 152 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.54.bert.pt, 153 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.98.bert.pt, 154 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.valid.3.bert.pt, 155 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.train.96.bert.pt, 156 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/cnndm.valid.5.bert.pt, 157 files out of an estimated total of 158\n",
      "Uploaded /dadendev/bertsumdata/bertsum_data.zip, 158 files out of an estimated total of 158\n",
      "Uploaded 158 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_91f544a88b9a404d90ccc25479e1d77a"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.upload(src_dir=LOCAL_DATA_FOLDER, target_path=TARGET_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the local project folder which is mirror to the workspace for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## local folder to store all the related files to be copied to the workspace\n",
    "PROJECT_FOLDER = './azureml_exp'\n",
    "## conda environment name, the yaml file will be copied to the workspace\n",
    "CONDA_ENV_NAME = \"nlp_gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTRY_SCRIPT = \"extractive_summarization_cnndm_distributed_train.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated conda file: nlp_gpu.yaml\r\n",
      "\r\n",
      "To create the conda environment:\r\n",
      "$ conda env create -f nlp_gpu.yaml\r\n",
      "\r\n",
      "To update the conda environment:\r\n",
      "$ conda env update -f nlp_gpu.yaml\r\n",
      "\r\n",
      "To register the conda environment in Jupyter:\r\n",
      "$ conda activate nlp_gpu\r\n",
      "$ python -m ipykernel install --user --name nlp_gpu --display-name \"Python (nlp_gpu)\"\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p {PROJECT_FOLDER}\n",
    "!python ../../tools/generate_conda_file.py --gpu --name {CONDA_ENV_NAME}\n",
    "!cp ./nlp_gpu.yaml {PROJECT_FOLDER}\n",
    "!cp {ENTRY_SCRIPT} {PROJECT_FOLDER}\n",
    "!cp -r ../../utils_nlp {PROJECT_FOLDER}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AZUREML_CONFIG_PATH = \"./.azureml\"\n",
    "## output dir in the workspace\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "ENCODER = \"transformer\"\n",
    "TARGET_OUTPUT_DIR = f'output/{EXPERIMENT_NAME}/'\n",
    "## cache dir in the workspace\n",
    "TARGET_CACHE_DIR = f'cache/{EXPERIMENT_NAME}/'\n",
    "## file name for saving the prediction\n",
    "SUMMARY_FILENAME = \"generated_summaries.txt\"\n",
    "MODEL_FILENAME = \"dist_extsum.pt\"\n",
    "\n",
    "## local path to download the output from the cluster\n",
    "LOCAL_OUTPUT_DIR = './output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(LOCAL_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(LOCAL_OUTPUT_DIR, EXPERIMENT_NAME), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:azureml.train.estimator._framework_base_estimator:If environment_definition or conda_dependencies_file is specified, Azure ML will not install any framework related packages on behalf of the user.\n",
      "WARNING:azureml.train.estimator._framework_base_estimator:framework_version is not specified, defaulting to version 1.3.\n"
     ]
    }
   ],
   "source": [
    "NcclConfig=Nccl()\n",
    "estimator = PyTorch(source_directory=PROJECT_FOLDER,\n",
    "                    compute_target=gpu_compute_target,\n",
    "                    script_params={\n",
    "                        \"--dist_url\": \"$AZ_BATCHAI_PYTORCH_INIT_METHOD\",\n",
    "                        \"--rank\": \"$AZ_BATCHAI_TASK_INDEX\",\n",
    "                        \"--node_count\": NODE_COUNT,\n",
    "                        \"--data_dir\":ds.path(f'{TARGET_DATA_FOLDER}').as_mount(),\n",
    "                        \"--cache_dir\": ds.path(f'{TARGET_CACHE_DIR}').as_mount(),\n",
    "                        '--output_dir':ds.path(f'{TARGET_OUTPUT_DIR}').as_mount(),\n",
    "                        \"--quick_run\": 'true',\n",
    "                        \"--use_preprocessed_data\": 'true',\n",
    "                        \"--summary_filename\": f'{SUMMARY_FILENAME}',\n",
    "                        \"--model_filename\": f'{MODEL_FILENAME}',\n",
    "                        \"--model_name\": MODEL_NAME,\n",
    "                        \"--encoder\": ENCODER\n",
    "                    },\n",
    "                    entry_script= ENTRY_SCRIPT,\n",
    "                    node_count=NODE_COUNT,\n",
    "                    distributed_training=NcclConfig,\n",
    "                    conda_dependencies_file=f'{CONDA_ENV_NAME}.yaml',\n",
    "                    use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee66480919f4c5da0672f2f5e668983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Generated Summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you stop the notebook and come back, \n",
    "you'll need to use the run_id in the output of the previous cell \n",
    "to get run details.\n",
    "\"\"\n",
    "# fetched_run = Run(experiment, \"NLP-ExtSum_1579816237_ea238f69\")\n",
    "# RunDetails(fetched_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to clear the local output dir as the ds.download worn't download is the path exists\n",
    "!rm -rf {LOCAL_OUTPUT_DIR}/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.download(target_path=LOCAL_OUTPUT_DIR,\n",
    "                   prefix=f'{TARGET_OUTPUT_DIR}{SUMMARY_FILE}',\n",
    "                   show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nlp.eval.evaluate_summarization import get_rouge\n",
    "from utils_nlp.models.transformers.extractive_summarization import ExtSumProcessedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset, test_dataset = ExtSumProcessedData().splits(root=LOCAL_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [i['tgt_txt'] for i in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "with open(os.path.join(LOCAL_OUTPUT_DIR, f'{TARGET_OUTPUT_DIR}{SUMMARY_FILENAME}'), \"r\") as filehandle:\n",
    "    for cnt, line in enumerate(filehandle):\n",
    "        prediction.append(line[0:-1]) # remove the ending \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in case quick run and not use preprocessed data, download the saved model and run prediction\n",
    "import pickle\n",
    "from utils_nlp.models.transformers.extractive_summarization import ExtractiveSummarizer\n",
    "if len(prediction) !=len(target):\n",
    "    ds.download(target_path=LOCAL_OUTPUT_DIR,\n",
    "                   prefix=f'{TARGET_OUTPUT_DIR}{MODEL_FILENAME}',\n",
    "                   show_progress=True)\n",
    "    model = pickle.load(open(os.path.join(LOCAL_OUTPUT_DIR, f'{TARGET_OUTPUT_DIR}{MODEL_FILENAME}'), \"rb\"))\n",
    "    summarizer = ExtractiveSummarizer(\"distilbert-base-uncased\", \"transformer\", \"./output\")\n",
    "    summarizer.model = model\n",
    "    prediction = summarizer.predict(test_dataset, num_gpus=4, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading output/NLP-ExtSum/dist_extsum_model.pt\n",
      "Downloaded output/NLP-ExtSum/dist_extsum_model.pt, 1 files out of an estimated total of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.download(target_path=LOCAL_OUTPUT_DIR,\n",
    "            prefix=\"output/NLP-ExtSum/dist_extsum_model.pt\",\n",
    "            show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring: 100%|██████████| 90/90 [00:41<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['turkey has blocked access to twitter and youtube after they refused a request to remove pictures of a prosecutor held during an armed siege last week .',\n",
       " \"a turkish court imposed the blocks because images of the deadly siege were being shared on social media and ` deeply upset ' the wife and children of mehmet selim kiraz , the hostage who was killed .\",\n",
       " \"the 46-year-old turkish prosecutor died in hospital when members of the revolutionary people 's liberation party-front ( dhkp-c ) stormed a courthouse and took him hostage .\",\n",
       " 'the dhkp-c is considered a terrorist group by turkey , the european union and us .',\n",
       " 'a turkish court has blocked access to twitter and youtube after they refused a request to remove pictures of prosecutor mehmet selim kiraz held during an armed siege last week',\n",
       " 'grief : the family of mehmet selim kiraz grieve over his coffin during his funeral at eyup sultan mosque in istanbul , turkey .',\n",
       " 'he died in hospital after he was taken hostage by the far-left organisation',\n",
       " 'two of his captors were killed when security forces took back the building where the far-left group was holding him .',\n",
       " 'gunshots were heard and smoke could be seen rising from the scene at the end of the six-hour stand-off .',\n",
       " 'mr kiraz , a father-of-two married to a judge who also worked at the courthouse , was targeted for his part in an investigation into the death of berkin elvan .',\n",
       " 'the 15-year-old was severely wounded after being hit on the head by a tear-gas canister fired by a police officer during anti-government protests in istanbul in june 2013 .',\n",
       " 'after spending 269 days in a coma , elvan eventually died on march 11 last year .',\n",
       " \"his death , and the subsequent investigation , have since become a rallying point for the country 's far-left .\",\n",
       " 'gathering : prosecutors , lawyers and judges stand near a statue of lady justice during the funeral ceremony',\n",
       " \"a british national , of polish origin but who has not been named , was arrested on saturday as part of an operation against the revolutionary people 's liberation party-front , according to reports .\",\n",
       " \"a foreign office spokeswoman said this morning : ' i can confirm that a british national has been arrested in turkey and that we are offering consular assistance . '\",\n",
       " 'before imposing the blocks on the websites , turkish authorities had tried to prevent newspapers printing images taken during the siege last week .',\n",
       " \"the newspapers were accused by the government of ` spreading terrorist propaganda ' in sharing the images of the hostage-taking .\",\n",
       " \"presidential spokesman ibrahim kalin said : ` this has to do with the publishing of the prosecutor 's\",\n",
       " \"what happened in the aftermath ( of the prosecutor 's\",\n",
       " 'killing ) is as grim as the incident itself .',\n",
       " \"` the demand from the prosecutor 's office is that this image\",\n",
       " 'not be used anywhere in electronic platforms .',\n",
       " '` the wife and children of prosecutor kiraz have been deeply',\n",
       " \"the images are everywhere . '\",\n",
       " \"he added : ' a request has been made to both twitter and youtube for the\",\n",
       " 'removal of the images and posts but they have not accepted it',\n",
       " 'and no response has been given .',\n",
       " \"this decision has been taken through a court in istanbul . '\",\n",
       " 'critical : prosecutor mehmet selim kiraz was taken to hospital with gunshot wounds but died of his injuries',\n",
       " 'strength of feeling : elvan has since become an icon for the turkish far-left and his supporters accuse the authorities of covering up the circumstances and perpetrators of his death',\n",
       " 'google said it was working to restore service to the youtube',\n",
       " 'video-sharing site , which it owns .',\n",
       " 'working to restore access for its users .',\n",
       " 'facebook said it had complied with a turkish court order requiring it to restrict access to some content or face a block on its service .',\n",
       " 'a company spokesman said it would appeal the order .',\n",
       " \"turkey 's telecoms regulator could not immediately be reached\",\n",
       " 'and there was no statement on its website .',\n",
       " 'this is not the first time that turkish authorities have imposed blocks on social media sites and networks .',\n",
       " 'in the run-up to local elections in march 2014 blocks were imposed after recordings circulated allegedly revealing corruption among senior officials .',\n",
       " 'figures provided by twitter revealed that turkey filed more requests to remove content from the social network than any other nation between july and december 2014 .']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['src_txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"after spending 269 days in a coma , elvan eventually died on march 11 last year .<q>the 15-year-old was severely wounded after being hit on the head by a tear-gas canister fired by a police officer during anti-government protests in istanbul in june 2013 .<q>his death , and the subsequent investigation , have since become a rallying point for the country 's far-left .\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"turkish court imposed blocks as images of siege shared on social media<q>images ` deeply upset ' wife and children of hostage mehmet selim kiraz<q>prosecutor , 46 , died in hospital after hostages stormed a courthouse<q>two of his captors were killed when security forces took back the building\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = './testrouge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11489\n",
      "11489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 06:42:07,875 [MainThread  ] [INFO ]  Writing summaries.\n",
      "INFO:global:Writing summaries.\n",
      "2020-01-30 06:42:07,877 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ./testrouge/tmpml_l__bp/system and model files to ./testrouge/tmpml_l__bp/model.\n",
      "INFO:global:Processing summaries. Saving system files to ./testrouge/tmpml_l__bp/system and model files to ./testrouge/tmpml_l__bp/model.\n",
      "2020-01-30 06:42:07,878 [MainThread  ] [INFO ]  Processing files in ./testrouge/rouge-tmp-2020-01-30-06-42-06/candidate/.\n",
      "INFO:global:Processing files in ./testrouge/rouge-tmp-2020-01-30-06-42-06/candidate/.\n",
      "2020-01-30 06:42:09,028 [MainThread  ] [INFO ]  Saved processed files to ./testrouge/tmpml_l__bp/system.\n",
      "INFO:global:Saved processed files to ./testrouge/tmpml_l__bp/system.\n",
      "2020-01-30 06:42:09,030 [MainThread  ] [INFO ]  Processing files in ./testrouge/rouge-tmp-2020-01-30-06-42-06/reference/.\n",
      "INFO:global:Processing files in ./testrouge/rouge-tmp-2020-01-30-06-42-06/reference/.\n",
      "2020-01-30 06:42:10,235 [MainThread  ] [INFO ]  Saved processed files to ./testrouge/tmpml_l__bp/model.\n",
      "INFO:global:Saved processed files to ./testrouge/tmpml_l__bp/model.\n",
      "2020-01-30 06:42:10,317 [MainThread  ] [INFO ]  Written ROUGE configuration to ./testrouge/tmpn6chepdc/rouge_conf.xml\n",
      "INFO:global:Written ROUGE configuration to ./testrouge/tmpn6chepdc/rouge_conf.xml\n",
      "2020-01-30 06:42:10,318 [MainThread  ] [INFO ]  Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./testrouge/tmpn6chepdc/rouge_conf.xml\n",
      "INFO:global:Running ROUGE with command /dadendev/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /dadendev/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ./testrouge/tmpn6chepdc/rouge_conf.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "1 ROUGE-1 Average_R: 0.32121 (95%-conf.int. 0.31865 - 0.32394)\n",
      "1 ROUGE-1 Average_P: 0.26093 (95%-conf.int. 0.25874 - 0.26303)\n",
      "1 ROUGE-1 Average_F: 0.27434 (95%-conf.int. 0.27243 - 0.27641)\n",
      "---------------------------------------------\n",
      "1 ROUGE-2 Average_R: 0.08593 (95%-conf.int. 0.08408 - 0.08778)\n",
      "1 ROUGE-2 Average_P: 0.06877 (95%-conf.int. 0.06732 - 0.07025)\n",
      "1 ROUGE-2 Average_F: 0.07264 (95%-conf.int. 0.07112 - 0.07414)\n",
      "---------------------------------------------\n",
      "1 ROUGE-L Average_R: 0.28730 (95%-conf.int. 0.28497 - 0.28973)\n",
      "1 ROUGE-L Average_P: 0.23408 (95%-conf.int. 0.23206 - 0.23607)\n",
      "1 ROUGE-L Average_F: 0.24574 (95%-conf.int. 0.24392 - 0.24768)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rouge_score = get_rouge(prediction, target, RESULT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_gpu)",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
