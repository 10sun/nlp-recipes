{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Baseline Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a baseline model? \n",
    "\n",
    "Producing a baseline model is crucial for evaluating your model's performance on any machine learning problem. A baseline model is a basic solution that serves as a point of reference for comparing other models to. The baseline model's performance gives us an indication of how much better our models can perform relative to a naive approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are good baselines for sentence similarity?\n",
    "\n",
    "For sentence similarity problems, we have to first produce a vector representation of each sentence in the sentence pair, known as an **embedding**. Then, we need to compute the similarity between these two sentence embeddings (we'll use Cosine Similarity).\n",
    "\n",
    "For producing representations of sentences, there are some common baseline approaches: \n",
    "1. Create word embeddings for each word in a sentence\n",
    "    1. word2vec word embeddings\n",
    "    3. fastText word embeddings\n",
    "    \n",
    "2. Create sentence embeddings\n",
    "    2. TF-IDF embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Data Loading and Preprocessing](#Data-Loading-and-Preprocessing)\n",
    "    * [Load STS Benchmark Dataset](#Load-STS-Benchmark-Dataset)\n",
    "    - [Preprocess / Tokenize](#Data-Preprocessing-/-Tokenization)\n",
    "    - [Document Frequency Calculation](#Document-Frequency-Calculation)\n",
    "* [Baseline Models](#Baseline-Models)\n",
    "    - [Baseline #1: word2vec and cosine similarity](#Baseline-#1:-Word2vec-Embeddings-with-Cosine-Similarity)\n",
    "    - [Baseline #2: fastText and cosine similarity](#Baseline-#2:-fastText-Embeddings-with-Cosine-Similarity)\n",
    "\n",
    "    * [Baseline #3: TF-IDF and cosine similarity](#Baseline-#3:-TF-IDF-Embeddings-with-Cosine-Similarity)\n",
    "\n",
    "* [Comparison of Baseline Models](#Comparison-of-Baseline-Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "#Import Packages\n",
    "import sys\n",
    "# Set the environment path\n",
    "sys.path.append(\"../../\")  \n",
    "import os\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "from tempfile import TemporaryDirectory\n",
    "import scrapbook as sb\n",
    "import scipy\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Import utility functions\n",
    "from utils_nlp.dataset import stsbenchmark\n",
    "from utils_nlp.dataset.preprocess import (\n",
    "    to_lowercase,\n",
    "    to_spacy_tokens,\n",
    "    rm_spacy_stopwords,\n",
    ")\n",
    "from utils_nlp.models.pretrained_embeddings import word2vec\n",
    "from utils_nlp.models.pretrained_embeddings import fasttext\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path where you datasets are located\n",
    "tmp_dir = TemporaryDirectory()\n",
    "BASE_DATA_PATH = tmp_dir.name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1.61M/1.61M [03:22<00:00, 7.92kKB/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2.56M/2.56M [05:17<00:00, 8.05kKB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download pretrained word vectors\n",
    "word2vec_model = word2vec.load_pretrained_vectors(dir_path=BASE_DATA_PATH)\n",
    "fastText_model = fasttext.load_pretrained_vectors(dest_path=BASE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load STS Benchmark Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we utilize the [STS Benchmark dataset](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark#STS_benchmark_dataset_and_companion_dataset) which contains a selection of English datasets that were used in Semantic Textual Similarity (STS) tasks 2012-2017. The datasets include text from image captions, news headlines, and user forums. The dataset contains 8,628 sentence pairs with a human-labeled integer representing the sentences' similarity (ranging from 0, for no meaning overlap, to 5, meaning equivalence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 401/401 [00:01<00:00, 268KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to C:\\Users\\cocochra\\AppData\\Local\\Temp\\tmpg6q71ioj\\raw\\stsbenchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 401/401 [00:01<00:00, 219KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to C:\\Users\\cocochra\\AppData\\Local\\Temp\\tmpg6q71ioj\\raw\\stsbenchmark\n"
     ]
    }
   ],
   "source": [
    "# Produce a pandas dataframe for the training and test sets\n",
    "train_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"train\")\n",
    "test_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"test\")\n",
    "\n",
    "# Clean the sts dataset\n",
    "sts_train = stsbenchmark.clean_sts(train_raw)\n",
    "sts_test = stsbenchmark.clean_sts(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 5749 sentences\n",
      "Testing set has 1379 sentences\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has {} sentences\".format(len(sts_train)))\n",
    "print(\"Testing set has {} sentences\".format(len(sts_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.500</td>\n",
       "      <td>A girl is styling her hair.</td>\n",
       "      <td>A girl is brushing her hair.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.600</td>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.000</td>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.500</td>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.800</td>\n",
       "      <td>A woman is cutting onions.</td>\n",
       "      <td>A woman is cutting tofu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.500</td>\n",
       "      <td>A man is riding an electric bicycle.</td>\n",
       "      <td>A man is riding a bicycle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.200</td>\n",
       "      <td>A man is playing the drums.</td>\n",
       "      <td>A man is playing the guitar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.200</td>\n",
       "      <td>A man is playing guitar.</td>\n",
       "      <td>A lady is playing the guitar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.714</td>\n",
       "      <td>A man is playing a guitar.</td>\n",
       "      <td>A man is playing a trumpet.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0  2.500                    A girl is styling her hair.   \n",
       "1  3.600       A group of men play soccer on the beach.   \n",
       "2  5.000  One woman is measuring another woman's ankle.   \n",
       "3  4.200                A man is cutting up a cucumber.   \n",
       "4  1.500                       A man is playing a harp.   \n",
       "5  1.800                     A woman is cutting onions.   \n",
       "6  3.500           A man is riding an electric bicycle.   \n",
       "7  2.200                    A man is playing the drums.   \n",
       "8  2.200                       A man is playing guitar.   \n",
       "9  1.714                     A man is playing a guitar.   \n",
       "\n",
       "                                          sentence2  \n",
       "0                      A girl is brushing her hair.  \n",
       "1  A group of boys are playing soccer on the beach.  \n",
       "2           A woman measures another woman's ankle.  \n",
       "3                      A man is slicing a cucumber.  \n",
       "4                      A man is playing a keyboard.  \n",
       "5                          A woman is cutting tofu.  \n",
       "6                        A man is riding a bicycle.  \n",
       "7                      A man is playing the guitar.  \n",
       "8                     A lady is playing the guitar.  \n",
       "9                       A man is playing a trumpet.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing / Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline models will expect that each sentence is represented by a list of **tokens**. Tokens are linguistic units like words, punctuation marks, numbers, etc. We'll use our util functions which utilize the spaCy package, a popular package for performing tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also common to remove high-frequency words which do not help distinguish one sentence from another, so called **stop words**. For example, \"the\", \"and\", \"a\", etc. are typical stop words although each tokenization package may differ in the words they consider to be stop words. We'll tokenize our corpus with and without stop words so that we can compare our methods with and without stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase\n",
    "df_low = to_lowercase(sts_train)  \n",
    "# Tokenize text\n",
    "sts_tokenize = to_spacy_tokens(df_low) \n",
    "# Tokenize with removal of stopwords\n",
    "sts_train_stop = rm_spacy_stopwords(sts_tokenize) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each row in our dataframe contains:  \n",
    "- The similarity score of the sentence pair\n",
    "- The 2 original sentences from our datasets  \n",
    "- A column for each sentence's tokenization with stop words  \n",
    "- A column for each sentence's tokenization without stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence1_tokens</th>\n",
       "      <th>sentence2_tokens</th>\n",
       "      <th>sentence1_tokens_rm_stopwords</th>\n",
       "      <th>sentence2_tokens_rm_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.00</td>\n",
       "      <td>a plane is taking off.</td>\n",
       "      <td>an air plane is taking off.</td>\n",
       "      <td>[a, plane, is, taking, off, .]</td>\n",
       "      <td>[an, air, plane, is, taking, off, .]</td>\n",
       "      <td>[plane, taking, .]</td>\n",
       "      <td>[air, plane, taking, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.80</td>\n",
       "      <td>a man is playing a large flute.</td>\n",
       "      <td>a man is playing a flute.</td>\n",
       "      <td>[a, man, is, playing, a, large, flute, .]</td>\n",
       "      <td>[a, man, is, playing, a, flute, .]</td>\n",
       "      <td>[man, playing, large, flute, .]</td>\n",
       "      <td>[man, playing, flute, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.80</td>\n",
       "      <td>a man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>a man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>[a, man, is, spreading, shreded, cheese, on, a...</td>\n",
       "      <td>[a, man, is, spreading, shredded, cheese, on, ...</td>\n",
       "      <td>[man, spreading, shreded, cheese, pizza, .]</td>\n",
       "      <td>[man, spreading, shredded, cheese, uncooked, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.60</td>\n",
       "      <td>three men are playing chess.</td>\n",
       "      <td>two men are playing chess.</td>\n",
       "      <td>[three, men, are, playing, chess, .]</td>\n",
       "      <td>[two, men, are, playing, chess, .]</td>\n",
       "      <td>[men, playing, chess, .]</td>\n",
       "      <td>[men, playing, chess, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.25</td>\n",
       "      <td>a man is playing the cello.</td>\n",
       "      <td>a man seated is playing the cello.</td>\n",
       "      <td>[a, man, is, playing, the, cello, .]</td>\n",
       "      <td>[a, man, seated, is, playing, the, cello, .]</td>\n",
       "      <td>[man, playing, cello, .]</td>\n",
       "      <td>[man, seated, playing, cello, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0   5.00                         a plane is taking off.   \n",
       "1   3.80                a man is playing a large flute.   \n",
       "2   3.80  a man is spreading shreded cheese on a pizza.   \n",
       "3   2.60                   three men are playing chess.   \n",
       "4   4.25                    a man is playing the cello.   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0                        an air plane is taking off.   \n",
       "1                          a man is playing a flute.   \n",
       "2  a man is spreading shredded cheese on an uncoo...   \n",
       "3                         two men are playing chess.   \n",
       "4                 a man seated is playing the cello.   \n",
       "\n",
       "                                    sentence1_tokens  \\\n",
       "0                     [a, plane, is, taking, off, .]   \n",
       "1          [a, man, is, playing, a, large, flute, .]   \n",
       "2  [a, man, is, spreading, shreded, cheese, on, a...   \n",
       "3               [three, men, are, playing, chess, .]   \n",
       "4               [a, man, is, playing, the, cello, .]   \n",
       "\n",
       "                                    sentence2_tokens  \\\n",
       "0               [an, air, plane, is, taking, off, .]   \n",
       "1                 [a, man, is, playing, a, flute, .]   \n",
       "2  [a, man, is, spreading, shredded, cheese, on, ...   \n",
       "3                 [two, men, are, playing, chess, .]   \n",
       "4       [a, man, seated, is, playing, the, cello, .]   \n",
       "\n",
       "                 sentence1_tokens_rm_stopwords  \\\n",
       "0                           [plane, taking, .]   \n",
       "1              [man, playing, large, flute, .]   \n",
       "2  [man, spreading, shreded, cheese, pizza, .]   \n",
       "3                     [men, playing, chess, .]   \n",
       "4                     [man, playing, cello, .]   \n",
       "\n",
       "                       sentence2_tokens_rm_stopwords  \n",
       "0                            [air, plane, taking, .]  \n",
       "1                           [man, playing, flute, .]  \n",
       "2  [man, spreading, shredded, cheese, uncooked, p...  \n",
       "3                           [men, playing, chess, .]  \n",
       "4                   [man, seated, playing, cello, .]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train_stop.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase\n",
    "df_low = to_lowercase(sts_test)\n",
    "# Tokenize text\n",
    "sts_tokenize = to_spacy_tokens(df_low)\n",
    "# Tokenize with removal of stopwords\n",
    "sts_test_stop = rm_spacy_stopwords(sts_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many baseline models we explore will require calculation of how frequently a word appears in our corpus. To calculate this, we iterate through the sentences in our training set and count the number of sentences that contain each word. Note that \"document\" refers to some larger chunk of multiple tokens/words. In our case, our documents will actually be individual sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_frequency(df):\n",
    "    \"\"\"Iterate through all sentences in dataframe and create a dictionary \n",
    "    mapping tokens to the number of sentences in our corpus they appear in\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): dataframe of sentence pairs with their similarity scores\n",
    "        \n",
    "    Returns:\n",
    "        document_frequency_dict (dictionary): mapping from tokens to number of sentences they appear in\n",
    "        n (int): number of sentences in the corpus\n",
    "    \"\"\"\n",
    "    document_frequency_dict = {}\n",
    "    all_sentences =  df[[\"sentence1_tokens\", \"sentence2_tokens\"]]\n",
    "    sentences = all_sentences.values.flatten().tolist()\n",
    "    n = len(sentences)\n",
    "\n",
    "    for s in sentences:\n",
    "        for token in set(s):\n",
    "            document_frequency_dict[token] = document_frequency_dict.get(token, 0) + 1\n",
    "\n",
    "    return document_frequency_dict, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we need to calculate these values on our training set so that we don't \"peek at\" our test set until test time\n",
    "document_frequencies, num_documents = get_document_frequency(sts_train_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11498"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we consider each of the baseline models, we'll save all model predictions in a dictionary and will evaluate the results at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #1: Word2vec Embeddings with Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline first constructs word embeddings using word2vec. Once we have a word embedding (vector) for each word in the sentence, we calculate an embedding for the full sentence by taking the (weighted) average of all the word embeddings. The weights will be calculated using TF-IDF. Lastly, in order to compare the two sentence embeddings we use the cosine similarity metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Word2Vec?\n",
    "Word2vec is a predictive model for learning word embeddings from text (see [original research paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)). Word embeddings are learned such that words that share common contexts in the corpus will be close together in the vector space. There are two different model architectures that can be used to produce word2vec embeddings: continuous bag-of-words (CBOW) or continuous skip-gram. The former uses a window of surrounding words (the \"context\") to predict the current word and the latter uses the current word to predict the surrounding context words. See this [tutorial](https://www.guru99.com/word-embedding-word2vec.html#3) on word2vec for more detailed information about the model.\n",
    "\n",
    "For our purposes, we use pretrained word2vec word embeddings. These embeddings were trained on a Google News corpus and provide 300-dimensional embeddings (vectors) for 3 million English words. See this [link](https://code.google.com/archive/p/word2vec/) for the original location of the embeddings and see the code below to load these word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TF-IDF?\n",
    "\n",
    "TF-IDF or term frequency-inverse document frequency is a weighting scheme intended to measure how important a word is to the document (or sentence in our case) within the broader corpus (our dataset). The weight \"increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus\" ([tutorial link](http://www.tfidf.com/)). When we're averaging together many different word vectors to get a sentence embedding, it makes sense to give stronger weights to words that are more distinct relative to the corpus and that have a high frequency in the sentence. The TF-IDF weights capture this intution, with the weight increasing as term frequency increases and/or as the inverse document frequency increases.\n",
    "\n",
    "For a term $t$ in sentence $s$ in corpus $c$, then the TF-IDF weight is \n",
    "$$w_{t,s} = TF_{t,s} * \\log{\\frac{N}{df_t}}$$\n",
    "where:  \n",
    "$TF_{t,s}$ = the number of times term $t$ appears in sentence $s$  \n",
    "$df_t$ = the number of sentences containing term $t$  \n",
    "$N$ = the size of the corpus.  \n",
    "\n",
    "In these baselines, we calculate the TF-IDF weighted average of all the word embeddings. The code below implements this weighted average given a list of tokens and an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_embedding(tokens, embedding_model):\n",
    "    \"\"\"Calculate TF-IDF weighted average embedding for a sentence\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): list of tokens in a sentence\n",
    "        embedding_model: model to use for word embedding (word2vec, glove, fastText, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        list: vector representing the sentence\n",
    "    \"\"\"\n",
    "    # Throw away tokens that are not in the embedding model\n",
    "    tokens = [i for i in tokens if i in embedding_model]\n",
    "\n",
    "    if len(tokens) == 0:\n",
    "        return []\n",
    "\n",
    "    # We will weight by TF-IDF. The TF part is calculated by:\n",
    "    # (# of times term appears / total terms in sentence)\n",
    "    count = Counter(tokens)\n",
    "    token_list = list(count)\n",
    "    term_frequency = [count[i] / len(tokens) for i in token_list]\n",
    "\n",
    "    # Now for the IDF part: LOG(# documents / # documents with term in it)\n",
    "    inv_doc_frequency = [\n",
    "        math.log(num_documents / (document_frequencies.get(i, 0) + 1)) for i in count\n",
    "    ]\n",
    "\n",
    "    # Put the TF-IDF together and produce the weighted average of vector embeddings\n",
    "    word_embeddings = [embedding_model[token] for token in token_list]\n",
    "    weights = [term_frequency[i] * inv_doc_frequency[i] for i in range(len(token_list))]\n",
    "    return list(np.average(word_embeddings, weights=weights, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Cosine Similarity?\n",
    "\n",
    "Cosine similarity is a common similarity metric between vectors. Intuitively it measures the cosine of the angle between any two vectors. With vectors $a$ and $b$, the cosine similarity is: cosine similarity($a$,$b$) = $\\frac{\\vec{a} \\cdot \\vec{b} }{||\\vec{a}|| ||\\vec{b}||}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(embedding1, embedding2):\n",
    "    \"\"\"Calculate cosine similarity between two embedding vectors\n",
    "    \n",
    "    Args:\n",
    "        embedding1 (list): embedding for the first sentence\n",
    "        embedding2 (list): embedding for the second sentence\n",
    "    \n",
    "    Returns:\n",
    "        list: cosine similarity value between the two embeddings\n",
    "    \"\"\"\n",
    "    # distance.cosine calculates cosine DISTANCE, so we need to\n",
    "    # return 1 - distance to get cosine similarity\n",
    "    cosine_similarity = 1 - distance.cosine(embedding1, embedding2)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Sentence Similarity Predictions for Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate predictions for each sentence pair found in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_embedding_cosine_similarity(df, embedding_model, rm_stopwords=False):\n",
    "    \"\"\"Calculate the cosine similarity between TF-IDF weighted averaged embeddings\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): dataframe as provided by the nlp_utils\n",
    "        embedding_model: word embedding model\n",
    "        rm_stopwords (bool): whether to remove stop words (True) or not (False)\n",
    "    \n",
    "    Returns:\n",
    "        list: predicted values for sentence similarity of test set examples\n",
    "    \"\"\"\n",
    "    if rm_stopwords:\n",
    "        df['sentence1_embedding'] = df.apply(lambda x: average_sentence_embedding(x.sentence1_tokens_rm_stopwords, embedding_model), axis=1)\n",
    "        df['sentence2_embedding'] = df.apply(lambda x: average_sentence_embedding(x.sentence2_tokens_rm_stopwords, embedding_model), axis=1)\n",
    "    else:\n",
    "        df['sentence1_embedding'] = df.apply(lambda x: average_sentence_embedding(x.sentence1_tokens, embedding_model), axis=1)\n",
    "        df['sentence2_embedding'] = df.apply(lambda x: average_sentence_embedding(x.sentence2_tokens, embedding_model), axis=1)\n",
    "\n",
    "    df['predictions'] = df.apply(lambda x: calculate_cosine_similarity(x.sentence1_embedding, x.sentence2_embedding) if \n",
    "                                 (sum(x.sentence1_embedding) != 0 and sum(x.sentence2_embedding) != 0) else 0, axis=1)\n",
    "    \n",
    "    return df['predictions'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions using average word2vec embeddings both with and without stop words\n",
    "baselines[\"Word2vec Cosine\"] = average_word_embedding_cosine_similarity(\n",
    "    sts_test_stop, word2vec_model, rm_stopwords=True\n",
    ")\n",
    "baselines[\"Word2vec Cosine with Stop Words\"] = average_word_embedding_cosine_similarity(\n",
    "    sts_test_stop, word2vec_model, rm_stopwords=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #2: fastText Embeddings with Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline first constructs word embeddings using fastText. Once we have a word embedding (vector) for each word in the sentence, we calculate an embedding for the full sentence by taking the (weighted) average of all the word embeddings. The weights will be calculated using TF-IDF. Lastly, in order to compare the two sentence embeddings we use the cosine similarity metric (for an introduction to the cosine similarity metric, see [Background on Cosine Similarity](#What-is-Cosine-Similarity?)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is fastText?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText is an unsupervised algorithm created by Facebook Research for efficiently learning word embeddings (see [original research paper](https://arxiv.org/pdf/1607.04606.pdf)). fastText is significantly different than word2vec or GloVe in that these two algorithms we saw earlier treat each word as the smallest possible unit to find an embedding for. Conversely, fastText assumes that words are formed by an n-gram of characters (i.e. 2-grams of the word \"language\" would be {la, an, ng, gu, ua, ag, ge}). The embedding for a word is then composed of the sum of these character n-grams. This has advantages when finding word embeddings for rare words and words not present in the dictionary, as these words can still be broken down into character n-grams. Typically, for smaller datasets, fastText performs better than word2vec or GloVe. See this [tutorial](https://fasttext.cc/docs/en/unsupervised-tutorial.html) on fastText for more detail. We will use the pretrained word embeddings for the English language (wiki.en.bin; these embeddings as well as embeddings for 156 other languages can be found [here](https://fasttext.cc/docs/en/english-vectors.html)). These are 300-dimensional embeddings (vectors) trained on Wikipedia data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cocochra\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_cpu\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\cocochra\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_cpu\\lib\\site-packages\\ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# Get predictions using fastText embeddings and cosine similarity both with and without stop words\n",
    "baselines[\"fastText Cosine\"] = average_word_embedding_cosine_similarity(\n",
    "    sts_test_stop, fastText_model, rm_stopwords=True\n",
    ")\n",
    "baselines[\"fastText Cosine with Stop Words\"] = average_word_embedding_cosine_similarity(\n",
    "    sts_test_stop, fastText_model, rm_stopwords=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline #3: TF-IDF Embeddings with Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline first constructs a document embedding based on bag-of-words with TF-IDF weighting (for an introduction to TF-IDF, see [Background on TF-IDF](#What-is-TF-IDF?). Then we apply cosine similarity between the two embeddings in the sentence pair (for an introduction to the cosine similarity metric, see [Background on Cosine Similarity](#What-is-Cosine-Similarity?))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words\n",
    "\n",
    "The most basic approach for document embeddings is called Bag-of-Words. This method first determines the vocabulary across the entire corpus and then, for each document, creates a vector containing the number of times each vocabulary word appeared in the given document. These vectors are obviously very sparse and typical bag-of-words implementations ignore terms whose document frequency is less than some threshold in order to reduce sparsity. We also often ignore stop words as they add little semantic information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_cosine_similarity(df, rm_stopwords=False):\n",
    "    \"\"\"Calculate cosine similarity between TF-IDF document embeddings\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): dataframe as provided by the nlp_utils\n",
    "        rm_stopwords (bool): whether to remove stop words (True) or not (False)\n",
    "    \n",
    "    Returns:\n",
    "        list: predicted values for sentence similarity of test set examples\n",
    "    \"\"\"\n",
    "    stop_word_param = \"english\" if rm_stopwords else None\n",
    "\n",
    "    tf = TfidfVectorizer(\n",
    "        input=\"content\",\n",
    "        analyzer=\"word\",\n",
    "        min_df=0,\n",
    "        stop_words=stop_word_param,\n",
    "        sublinear_tf=True,\n",
    "    )\n",
    "    all_sentences = df[[\"sentence1\", \"sentence2\"]]\n",
    "    corpus = np.concatenate([df[\"sentence1\"].values, df[\"sentence2\"].values])\n",
    "    tfidf_matrix = np.array(tf.fit_transform(corpus).todense())\n",
    "    num_samples = len(df.index)\n",
    "    \n",
    "    # calculate the cosine similarity between pairs of tfidf embeddings\n",
    "    # first pair at index 0 and n in tfidf_matrix, second pair at 1 and n+1, etc.\n",
    "    df[\"predictions\"] = df.apply(\n",
    "        lambda x: calculate_cosine_similarity(\n",
    "            tfidf_matrix[int(x.name), :], tfidf_matrix[num_samples + int(x.name), :]\n",
    "        )\n",
    "        if (\n",
    "            sum(tfidf_matrix[int(x.name), :]) != 0\n",
    "            and sum(tfidf_matrix[num_samples + int(x.name), :]) != 0\n",
    "        )\n",
    "        else 0,\n",
    "        axis=1,\n",
    "    )\n",
    "    return df[\"predictions\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baselines[\"TF-IDF Cosine\"] = tfidf_cosine_similarity(sts_test_stop, rm_stopwords=True)\n",
    "baselines[\"TF-IDF Cosine with Stop Words\"] = tfidf_cosine_similarity(\n",
    "    sts_test_stop, rm_stopwords=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation metric is Pearson correlation ($\\rho$) which is a measure of the linear correlation between two variables. The formula for calculating Pearson correlation is as follows:  \n",
    "\n",
    "$$\\rho_{X,Y} = \\frac{E[(X-\\mu_X)(Y-\\mu_Y)]}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "This metric takes a value in [-1,1] where -1 represents a perfect negative correlation, 1 represents a perfect positive correlation, and 0 represents no correlation. We utilize the Pearson correlation metric as this is the metric that [SentEval](http://nlpprogress.com/english/semantic_textual_similarity.html), a widely-used evaluation toolkit for evaluation sentence representations, uses for the STS Benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(df, prediction):\n",
    "    \"\"\"Calculate the Pearson correlation between two vectors\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): dataframe of sentences and their similarity scores\n",
    "        prediction (list): predicted similarity scores for each value in test set\n",
    "        \n",
    "    Returns:\n",
    "        float: pearson correlation value between the actual and predicted score lists\n",
    "    \"\"\"\n",
    "    pearson_correlation = scipy.stats.pearsonr(prediction, list(df[\"score\"]))[0]\n",
    "    return pearson_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Word2vec Cosine': 0.6476611994922243,\n",
       " 'Word2vec Cosine with Stop Words': 0.6683811402044397,\n",
       " 'fastText Cosine': 0.6707291805647921,\n",
       " 'fastText Cosine with Stop Words': 0.677108697276809,\n",
       " 'TF-IDF Cosine': 0.6749213786510484,\n",
       " 'TF-IDF Cosine with Stop Words': 0.7118087132257669}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get metrics on predictions from all models\n",
    "results = dict((model, pearson_correlation(sts_test_stop, baselines[model])) for model in baselines)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Example data\n",
    "x = list(baselines.keys())\n",
    "x_pos = np.arange(len(x))\n",
    "y = list(results.values())\n",
    "\n",
    "ax.barh(x_pos, y, align='center')\n",
    "ax.set_yticks(x_pos)\n",
    "ax.set_yticklabels(x)\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Perason Correlation')\n",
    "ax.set_title('Performance of Baseline Models')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up data\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": {
        "TF-IDF Cosine": 0.6749213786510484,
        "TF-IDF Cosine with Stop Words": 0.7118087132257669,
        "Word2vec Cosine": 0.6476611994922243,
        "Word2vec Cosine with Stop Words": 0.6683811402044397,
        "fastText Cosine": 0.6707291805647921,
        "fastText Cosine with Stop Words": 0.677108697276809
       },
       "encoder": "json",
       "name": "results",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "results"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record results with scrapbook for tests\n",
    "sb.glue(\"results\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
