{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureML Pipeline, AutoML, AKS Deployment for Sentence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/nlp/examples/sentence_similarity/automl_with_pipelines_deployment_aks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds off of the [AutoML Local Deployment ACI](automl_local_deployment_aci.ipynb) notebook and demonstrates how to use [Azure Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning-service/\n",
    ") pipelines and Automated Machine Learning ([AutoML](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-automated-ml\n",
    ")) to streamline the creation of a machine learning workflow for predicting sentence similarity. The pipeline contains two steps:   \n",
    "1. PythonScriptStep: embeds sentences using a popular sentence embedding model, Google Universal Sentence Encoder\n",
    "2. AutoMLStep: demonstrates how to use Automated Machine Learning (AutoML) to automate model selection for predicting sentence similarity (regression)\n",
    "\n",
    "After creating the pipeline, the notebook demonstrates the deployment of our sentence similarity model using Azure Kubernetes Service ([AKS](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes\n",
    ")).\n",
    "\n",
    "This notebook showcases how to use the following AzureML features:  \n",
    "- AzureML Pipelines (PythonScriptStep and AutoMLStep)\n",
    "- Automated Machine Learning\n",
    "- AmlCompute\n",
    "- Datastore\n",
    "- Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#1.-Introduction)  \n",
    "    * 1.1 [What are AzureML Pipelines?](#1.1-What-are-AzureML-Pipelines?)  \n",
    "    * 1.2 [What is Azure AutoML?](#1.2-What-is-Azure-AutoML?)  \n",
    "    * 1.3 [Modeling Problem](#1.3-Modeling-Problem)  \n",
    "2. [Data Preparation](#2.-Data-Preparation)  \n",
    "3. [AzureML Setup](#3.-AzureML-Setup)  \n",
    "    * 3.1 [Link to or create a `Workspace`](#3.1-Link-to-or-create-a-Workspace)  \n",
    "    * 3.2 [Set up an `Experiment` and Logging](#3.2-Set-up-an-Experiment-and-Logging)  \n",
    "    * 3.3 [Link `AmlCompute` compute target](#3.3-Link-AmlCompute-compute-target)  \n",
    "    * 3.4 [Upload data to `Datastore`](#3.4-Upload-data-to-Datastore)  \n",
    "4. [Create AzureML Pipeline](#4.-Create-AzureML-Pipeline)  \n",
    "    * 4.1 [Set up run configuration file](#4.1-Set-up-run-configuration-file)  \n",
    "    * 4.2 [PythonScriptStep](#4.2-PythonScriptStep)  \n",
    "        * 4.2.1 [Define python script to run](#4.2.1-Define-python-script-to-run)\n",
    "        * 4.2.2 [Create PipelineData object](#4.2.2-Create-PipelineData-object)\n",
    "        * 4.2.3 [Create PythonScriptStep](#4.2.3-Create-PythonScriptStep)\n",
    "    * 4.3 [AutoMLStep](#4.3-AutoMLStep)\n",
    "        * 4.3.1 [Define get_data script to load data](#4.3.1-Define-get_data-script-to-load-data)\n",
    "        * 4.3.2 [Create AutoMLConfig object](#4.3.2-Create-AutoMLConfig-object)\n",
    "        * 4.3.3 [Create AutoMLStep](#4.3.3-Create-AutoMLStep)    \n",
    "5. [Run Pipeline](#5.-Run-Pipeline)  \n",
    "6. [Deploy Sentence Similarity Model](#6.-Deploy-Sentence-Similarity-Model)\n",
    "    * 6.1 [Register/Retrieve AutoML and Google Universal Sentence Encoder Models for Deployment](#6.1-Register/Retrieve-AutoML-and-Google-Universal-Sentence-Encoder-Models-for-Deployment)  \n",
    "    * 6.2 [Create Scoring Script](#6.2-Create-Scoring-Script)\n",
    "    * 6.3 [Create a YAML File for the Environment](#6.3-Create-a-YAML-File-for-the-Environment)   \n",
    "    * 6.4 [Image Creation](#6.4-Image-Creation) \n",
    "    * 6.5 [Provision the AKS Cluster](#6.5-Provision-the-AKS-Cluster)   \n",
    "    * 6.6 [Deploy the image as a Web Service to Azure Kubernetes Service](#6.6-Deploy-the-image-as-a-Web-Service-to-Azure-Kubernetes-Service)  \n",
    "    * 6.7 [Test Deployed Model](#6.7-Test-Deployed-Webservice)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What are AzureML Pipelines?\n",
    "\n",
    "[AzureML Pipelines](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines) define reusable machine learning workflows that can be used as a template for your machine learning scenarios. Pipelines allow you to optimize your workflow and spend time on machine learning rather than infrastructure. A Pipeline is defined by a series of steps; the following steps are available: AdlaStep, AutoMLStep, AzureBatchStep, DataTransferStep, DatabricksStep, EstimatorStep, HyperDriveStep, ModuleStep, MpiStep, and PythonScriptStep (see [here](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/?view=azure-ml-py) for details of each step). When the pipeline is run, cached results are used for all steps that have not changed, optimizing the run time. Data sources and intermediate data can be used across multiple steps in a pipeline, saving time and resources. Below we see an example of an AzureML pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://nlpbp.blob.core.windows.net/images/pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 What is Azure AutoML?\n",
    "\n",
    "Automated machine learning ([AutoML](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-automated-ml)) is a capability of Microsoft's [Azure Machine Learning service](https://azure.microsoft.com/en-us/services/machine-learning-service/\n",
    "). The goal of AutoML is to improve the productivity of data scientists and democratize AI by allowing for the rapid development and deployment of machine learning models. To acheive this goal, AutoML automates the process of selecting a ML model and tuning the model. All the user is required to provide is a dataset (suitable for a classification, regression, or time-series forecasting problem) and a metric to optimize in choosing the model and hyperparameters. The user is also given the ability to set time and cost constraints for the model selection and tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](automl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoML model selection and tuning process can be easily tracked through the Azure portal or directly in python notebooks through the use of widgets. AutoML quickly selects a high quality machine learning model tailored for your prediction problem. In this notebook, we walk through the steps of preparing data, setting up an AutoML experiment, and evaluating the results of our best model. More information about running AutoML experiments in Python can be found [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Modeling Problem\n",
    "\n",
    "The regression problem we will demonstrate is predicting sentence similarity scores on the STS Benchmark dataset. The [STS Benchmark dataset](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark#STS_benchmark_dataset_and_companion_dataset) contains a selection of English datasets that were used in Semantic Textual Similarity (STS) tasks 2012-2017. The dataset contains 8,628 sentence pairs with a human-labeled integer representing the sentences' similarity (ranging from 0, for no meaning overlap, to 5, meaning equivalence).\n",
    "\n",
    "For each sentence in the sentence pair, we will use Google's pretrained Universal Sentence Encoder (details provided below) to generate a $512$-dimensional embedding. Both embeddings in the sentence pair will be concatenated and the resulting $1024$-dimensional vector will be used as features in our regression problem. Our target variable is the sentence similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning diagnostics collection on. \n",
      "System version: 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]\n",
      "Azure ML SDK Version: 1.0.53\n",
      "Pandas version: 0.23.4\n"
     ]
    }
   ],
   "source": [
    "# Set the environment path to find NLP\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial import distance\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "\n",
    "# Import utils\n",
    "from utils_nlp.azureml import azureml_utils\n",
    "from utils_nlp.dataset import stsbenchmark\n",
    "from utils_nlp.dataset.preprocess import (\n",
    "    to_lowercase,\n",
    "    to_spacy_tokens,\n",
    "    rm_spacy_stopwords,\n",
    ")\n",
    "from utils_nlp.common.timer import Timer\n",
    "\n",
    "# Google Universal Sentence Encoder loader\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# AzureML packages\n",
    "import azureml as aml\n",
    "import logging\n",
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)\n",
    "from azureml.core import Datastore, Experiment, Workspace\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.webservice import AksWebservice, Webservice\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.model import Model\n",
    "from azureml.train.automl import AutoMLStep, AutoMLStepRun, AutoMLConfig\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Azure ML SDK Version:\", aml.core.VERSION)\n",
    "print(\"Pandas version: {}\".format(pd.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = \"../../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"task\": 'regression',  # type of task: classification, regression or forecasting\n",
    "    \"iteration_timeout_minutes\": 15,  # How long each iteration can take before moving on\n",
    "    \"iterations\": 2,  # Number of algorithm options to try\n",
    "    \"primary_metric\": \"spearman_correlation\",  # Metric to optimize\n",
    "    \"preprocess\": True,  # Whether dataset preprocessing should be applied\n",
    "    \"verbosity\": logging.INFO,\n",
    "    \"blacklist_models\": ['XGBoostRegressor'] #this model is blacklisted due to installation issues\n",
    "}\n",
    "\n",
    "config_path = (\n",
    "    \"./.azureml\"\n",
    ")  # Path to the directory containing config.json with azureml credentials\n",
    "\n",
    "# Azure resources\n",
    "subscription_id = '15ae9cb6-95c1-483d-a0e3-b1a1a3b06324'#\"YOUR_SUBSCRIPTION_ID\"\n",
    "resource_group = \"nlprg\"#\"YOUR_RESOURCE_GROUP_NAME\"  \n",
    "workspace_name = 'MAIDAPTest'#\"YOUR_WORKSPACE_NAME\"  \n",
    "workspace_region = 'eastus2'#\"YOUR_WORKSPACE_REGION\" #Possible values eastus, eastus2 and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STS Benchmark Dataset**\n",
    "\n",
    "As described above, the STS Benchmark dataset contains 8.6K sentence pairs along with a human-annotated score for how similar the two sentences are. We will load the training, development (validation), and test sets provided by STS Benchmark and preprocess the data (lowercase the text, drop irrelevant columns, and rename the remaining columns) using the utils contained in this repo. Each dataset will ultimately have three columns: _sentence1_ and _sentence2_ which contain the text of the sentences in the sentence pair, and _score_ which contains the human-annotated similarity score of the sentence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [00:02<00:00, 171KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to ../../data\\raw\\stsbenchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [00:01<00:00, 369KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to ../../data\\raw\\stsbenchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 401/401 [00:01<00:00, 202KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to ../../data\\raw\\stsbenchmark\n"
     ]
    }
   ],
   "source": [
    "# Load in the raw datasets as pandas dataframes\n",
    "train_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"train\")\n",
    "dev_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"dev\")\n",
    "test_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean each dataset by lowercasing text, removing irrelevant columns,\n",
    "# and renaming the remaining columns\n",
    "train_clean = stsbenchmark.clean_sts(train_raw)\n",
    "dev_clean = stsbenchmark.clean_sts(dev_raw)\n",
    "test_clean = stsbenchmark.clean_sts(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase\n",
    "train = to_lowercase(train_clean)\n",
    "dev = to_lowercase(dev_clean)\n",
    "test = to_lowercase(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 5749 sentences\n",
      "Development set has 1500 sentences\n",
      "Testing set has 1379 sentences\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has {} sentences\".format(len(train)))\n",
    "print(\"Development set has {} sentences\".format(len(dev)))\n",
    "print(\"Testing set has {} sentences\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.00</td>\n",
       "      <td>a plane is taking off.</td>\n",
       "      <td>an air plane is taking off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.80</td>\n",
       "      <td>a man is playing a large flute.</td>\n",
       "      <td>a man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.80</td>\n",
       "      <td>a man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>a man is spreading shredded cheese on an uncoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.60</td>\n",
       "      <td>three men are playing chess.</td>\n",
       "      <td>two men are playing chess.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.25</td>\n",
       "      <td>a man is playing the cello.</td>\n",
       "      <td>a man seated is playing the cello.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0   5.00                         a plane is taking off.   \n",
       "1   3.80                a man is playing a large flute.   \n",
       "2   3.80  a man is spreading shreded cheese on a pizza.   \n",
       "3   2.60                   three men are playing chess.   \n",
       "4   4.25                    a man is playing the cello.   \n",
       "\n",
       "                                           sentence2  \n",
       "0                        an air plane is taking off.  \n",
       "1                          a man is playing a flute.  \n",
       "2  a man is spreading shredded cheese on an uncoo...  \n",
       "3                         two men are playing chess.  \n",
       "4                 a man seated is playing the cello.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "train.to_csv(\"data/train.csv\", index=False)\n",
    "test.to_csv(\"data/test.csv\", index=False)\n",
    "dev.to_csv(\"data/dev.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AzureML Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up the necessary components for running this as an AzureML experiment\n",
    "1. Create or link to an existing `Workspace`\n",
    "2. Set up an `Experiment` with `logging`\n",
    "3. Create or attach existing `AmlCompute`\n",
    "4. Upload our data to a `Datastore`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Link to or create a Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell looks to set up the connection to your [Azure Machine Learning service Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace). You can choose to connect to an existing workspace or create a new one. \n",
    "\n",
    "**To access an existing workspace:**\n",
    "1. If you have a `config.json` file, you do not need to provide the workspace information; you will only need to update the `config_path` variable that is defined above which contains the file.\n",
    "2. Otherwise, you will need to supply the following:\n",
    "    * The name of your workspace\n",
    "    * Your subscription id\n",
    "    * The resource group name\n",
    "\n",
    "**To create a new workspace:**\n",
    "\n",
    "Set the following information:\n",
    "* A name for your workspace\n",
    "* Your subscription id\n",
    "* The resource group name\n",
    "* [Azure region](https://azure.microsoft.com/en-us/global-infrastructure/regions/) to create the workspace in, such as `eastus2`. \n",
    "\n",
    "This will automatically create a new resource group for you in the region provided if a resource group with the name given does not already exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = azureml_utils.get_or_create_workspace(\n",
    "    config_path=config_path,\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group=resource_group,\n",
    "    workspace_name=workspace_name,\n",
    "    workspace_region=workspace_region,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: MAIDAPTest\n",
      "Azure region: eastus2\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: nlprg\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Workspace name: \" + ws.name,\n",
    "    \"Azure region: \" + ws.location,\n",
    "    \"Subscription id: \" + ws.subscription_id,\n",
    "    \"Resource group: \" + ws.resource_group,\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Set up an Experiment and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a folder for the project\n",
    "project_folder = \"./automl-sentence-similarity\"\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "\n",
    "# Set up an experiment\n",
    "experiment_name = \"NLP-SS-googleUSE\"\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "# Add logging to our experiment\n",
    "run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Link AmlCompute Compute Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use AzureML Pipelines we need to link a compute target as they can not be run locally. The different options include AmlCompute, Azure Databricks, Remote VMs, etc. All [compute options](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#supported-compute-targets) can be found in this table with details about whether the given options work with automated ML, pipelines, and GPU. For the following example, we will use an AmlCompute target because it supports Azure Pipelines and GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 1, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-10-24T00:30:13.694000+00:00', 'errors': None, 'creationTime': '2019-06-25T18:13:14.313025+00:00', 'modifiedTime': '2019-10-24T00:29:03.116033+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 1, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "# choose your cluster\n",
    "cluster_name = \"eval-gpu\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC6\", max_nodes=4\n",
    "    )\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute.\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Upload data to Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step uploads our local data to a `Datastore` so that the data is accessible from the remote compute target and creates a `DataReference` to point to the location of the data on the Datastore. A DataStore is backed either by a Azure File Storage (default option) or Azure Blob Storage ([how to decide between these options](https://docs.microsoft.com/en-us/azure/storage/common/storage-decide-blobs-files-disks)) and data is made accessible by mounting or copying data to the compute target. `ws.datastores` lists all options for datastores and `ds.account_name` gets the name of the datastore that can be used to find it in the Azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 3 files\n",
      "Uploading ./data\\dev.csv\n",
      "Uploading ./data\\test.csv\n",
      "Uploading ./data\\train.csv\n",
      "Uploaded ./data\\dev.csv, 1 files out of an estimated total of 3\n",
      "Uploaded ./data\\test.csv, 2 files out of an estimated total of 3\n",
      "Uploaded ./data\\train.csv, 3 files out of an estimated total of 3\n",
      "Uploaded 3 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_63862e3b039547be88846da34c3a0492"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a specific datastore or you can call ws.get_default_datastore()\n",
    "datastore_name = \"workspacefilestore\"\n",
    "ds = ws.datastores[datastore_name]\n",
    "\n",
    "# Upload files in data folder to the datastore\n",
    "ds.upload(\n",
    "    src_dir=\"./data\",\n",
    "    target_path=\"stsbenchmark_data\",\n",
    "    overwrite=True,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set up a `DataReference` object that points to the data we just uploaded into the stsbenchmark_data folder. DataReference objects point to data that is accessible from a datastore and will be used an an input into our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = DataReference(\n",
    "    datastore=ds,\n",
    "    data_reference_name=\"stsbenchmark\",\n",
    "    path_on_datastore=\"stsbenchmark_data/\",\n",
    "    overwrite=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create AzureML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up our pipeline which is made of two steps:  \n",
    "1. `PythonScriptStep`: takes each sentence pair from the data in the `Datastore` and concatenates the Google USE embeddings for each sentence into one vector. This step saves the embedding feature matrix back to our `Datastore` and uses a `PipelineData` object to represent this intermediate data.  \n",
    "2. `AutoMLStep`: takes the intermediate data produced by the previous step and passes it to an `AutoMLConfig` which performs the automatic model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Set up run configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up a `RunConfiguration` object which configures the execution environment for an experiment (sets up the conda dependencies, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "format": "row"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run config is ready\n"
     ]
    }
   ],
   "source": [
    "# create a new RunConfig object\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute\n",
    "conda_run_config.target = compute_target\n",
    "\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "conda_run_config.environment.docker.base_image = aml.core.runconfig.DEFAULT_CPU_IMAGE\n",
    "\n",
    "# Specify our own conda dependencies for the execution environment\n",
    "conda_run_config.environment.python.user_managed_dependencies = False\n",
    "conda_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    pip_packages=[\n",
    "        \"azureml-sdk[automl]==1.0.53\",\n",
    "        \"azureml-dataprep==1.1.8\",\n",
    "        \"azureml-train-automl==1.0.53\",\n",
    "    ],\n",
    "    conda_packages=[\n",
    "        \"numpy\",\n",
    "        \"py-xgboost<=0.80\",\n",
    "        \"pandas\",\n",
    "        \"tensorflow\",\n",
    "        \"tensorflow-hub\",\n",
    "        \"scikit-learn\",\n",
    "    ],\n",
    "    pin_sdk_version=False,\n",
    ")\n",
    "\n",
    "print(\"run config is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 PythonScriptStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PythonScriptStep` is a step which runs a user-defined Python script ([documentation](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py) here). In this `PythonScriptStep`, we will convert our sentences into a numerical representation in order to use them in our machine learning model. We will embed both sentences using the Google Universal Sentence Encoder (provided by tensorflow-hub) and concatenate their representations into a $1024$-dimensional vector to use as features for AutoML.\n",
    "\n",
    "**Google Universal Sentence Encoder:**\n",
    "We'll use a popular sentence encoder called Google Universal Sentence Encoder (see [original paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf)). Google provides two pretrained models based on different design goals: a Transformer model (targets high accuracy even if this reduces model complexity) and a Deep Averaging Network model (DAN; targets efficient inference). Both models are trained on a variety of web sources (Wikipedia, news, question-answers pages, and discussion forums) and produced 512-dimensional embeddings. This notebook utilizes the Transformer-based encoding model which can be downloaded [here](https://tfhub.dev/google/universal-sentence-encoder-large/3) because of its better performance relative to the DAN model on the STS Benchmark dataset (see Table 2 in Google Research's [paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf)). The Transformer model produces sentence embeddings using the \"encoding sub-graph of the transformer architecture\" (original architecture introduced [here](https://arxiv.org/abs/1706.03762)). \"This sub-graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other workds. The context aware word representations are converted to a fixed length sentence encoding vector by computing the element-wise sum of the representations at each word position.\" The input to the model is lowercase PTB-tokenized strings and the model is designed to be useful for multiple different tasks by using multi-task learning. More details about the model can be found in the [paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf) by Google Research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Define python script to run\n",
    "\n",
    "Define the script (called embed.py) that the `PythonScriptStep` will execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./automl-sentence-similarity/embed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_folder/embed.py\n",
    "import argparse\n",
    "import os\n",
    "import azureml.core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # reduce logging output\n",
    "\n",
    "\n",
    "def google_encoder(dataset):\n",
    "    \"\"\" Function that embeds sentences using the Google Universal\n",
    "    Sentence Encoder pretrained model\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataset: pandas dataframe with sentences and scores\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    emb1: 512-dimensional representation of sentence1\n",
    "    emb2: 512-dimensional representation of sentence2\n",
    "    \"\"\"\n",
    "    sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "    sts_input2 = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "    # Apply embedding model and normalize the input\n",
    "    sts_encode1 = tf.nn.l2_normalize(embedding_model(sts_input1), axis=1)\n",
    "    sts_encode2 = tf.nn.l2_normalize(embedding_model(sts_input2), axis=1)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(tf.tables_initializer())\n",
    "        emb1, emb2 = session.run(\n",
    "            [sts_encode1, sts_encode2],\n",
    "            feed_dict={\n",
    "                sts_input1: dataset[\"sentence1\"],\n",
    "                sts_input2: dataset[\"sentence2\"],\n",
    "            },\n",
    "        )\n",
    "    return emb1, emb2\n",
    "\n",
    "\n",
    "def feature_engineering(dataset):\n",
    "    \"\"\"Extracts embedding features from the dataset and returns\n",
    "    features and target in a dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataset: pandas dataframe with sentences and scores\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    df: pandas dataframe with embedding features\n",
    "    scores: list of target variables\n",
    "    \"\"\"\n",
    "    google_USE_emb1, google_USE_emb2 = google_encoder(dataset)\n",
    "    n_google = google_USE_emb1.shape[1]  # length of the embeddings\n",
    "    df = np.concatenate((google_USE_emb1, google_USE_emb2), axis=1)\n",
    "    names = [\"USEEmb1_\" + str(i) for i in range(n_google)] + [\n",
    "        \"USEEmb2_\" + str(i) for i in range(n_google)\n",
    "    ]\n",
    "    df = pd.DataFrame(df, columns=names)\n",
    "    return df, dataset[\"score\"]\n",
    "\n",
    "\n",
    "def write_output(df, path, name):\n",
    "    \"\"\"Write dataframes to correct path\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(\"%s created\" % path)\n",
    "    df.to_csv(path + \"/\" + name, index=False)\n",
    "\n",
    "\n",
    "# Parse arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--sentence_data\", type=str)\n",
    "parser.add_argument(\"--embedded_data\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "embedding_model = hub.Module(module_url)\n",
    "\n",
    "# Read data\n",
    "train = pd.read_csv(args.sentence_data + \"/train.csv\")\n",
    "dev = pd.read_csv(args.sentence_data + \"/dev.csv\")\n",
    "\n",
    "# Get Google USE features\n",
    "training_data, training_scores = feature_engineering(train)\n",
    "validation_data, validation_scores = feature_engineering(dev)\n",
    "\n",
    "# Write out training data to Datastore\n",
    "write_output(training_data, args.embedded_data, \"X_train.csv\")\n",
    "write_output(\n",
    "    pd.DataFrame(training_scores, columns=[\"score\"]), args.embedded_data, \"y_train.csv\"\n",
    ")\n",
    "\n",
    "# Write out validation data to Datastore\n",
    "write_output(validation_data, args.embedded_data, \"X_dev.csv\")\n",
    "write_output(\n",
    "    pd.DataFrame(validation_scores, columns=[\"score\"]), args.embedded_data, \"y_dev.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Create PipelineData object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PipelineData` objects represent a piece of intermediate data in a pipeline. Generally they are produced by one step (as an output) and then consumed by the next step (as an input), introducing an implicit order between steps in a pipeline. We create a PipelineData object that can represent the data produced by our first pipeline step that will be consumed by our second pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data = PipelineData(\"embedded_data\", datastore=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Create PythonScriptStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step defines the `PythonScriptStep`. We give the step a name, tell the step which python script to run (embed.py) and what directory that script is located in (source_directory). \n",
    "\n",
    "We also link the compute target and run configuration that we made previously. Our input is the `DataReference` object (input_data) where our raw sentence data was uploaded and our ouput is the `PipelineData` object (embedded_data) where the embedded data produced by this step will be stored. These are also passed in as arguments so that we have access to the correct data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_step = PythonScriptStep(\n",
    "    name=\"Embed\",\n",
    "    script_name=\"embed.py\",\n",
    "    arguments=[\"--embedded_data\", embedded_data, \"--sentence_data\", input_data],\n",
    "    inputs=[input_data],\n",
    "    outputs=[embedded_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=conda_run_config,\n",
    "    source_directory=project_folder,\n",
    "    allow_reuse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 AutoMLStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoMLStep` creates an AutoML step in a pipeline (see [documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-automl/azureml.train.automl.automlstep?view=azure-ml-py) and [basic example](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-with-automated-machine-learning-step.ipynb)). When using AutoML on remote compute, rather than passing our data directly into the `AutoMLConfig` object as we did in the local example, we must define a get_data.py script with a get_data() function to pass as the data_script argument. This workflow can be used for both local and remote executions (see [details](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-auto-train-remote)). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Define get_data script to load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the get_data.py file and get_data() function that the `AutoMLStep` will execute to collect data. When AutoML is used with a remote compute, the data can not be passed directly as parameters. Rather, a get_data function must be defined to access the data (see [this resource](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-auto-train-remote) for further details). Note that we can directly access the path of the intermediate data (called embedded_data) through `os.environ['AZUREML_DATAREFERENCE_embedded_data']`. This is necessary because the AutoMLStep does not accept additional parameters like the PythonScriptStep does with `arguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./automl-sentence-similarity/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_folder/get_data.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# get location of the embedded_data for future use\n",
    "EMBEDDED_DATA_REF = os.environ[\"AZUREML_DATAREFERENCE_embedded_data\"]\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"Function needed to load data for use on remote AutoML experiments\"\"\"\n",
    "    X_train = pd.read_csv(EMBEDDED_DATA_REF + \"/X_train.csv\")\n",
    "    y_train = pd.read_csv(EMBEDDED_DATA_REF + \"/y_train.csv\")\n",
    "    X_dev = pd.read_csv(EMBEDDED_DATA_REF + \"/X_dev.csv\")\n",
    "    y_dev = pd.read_csv(EMBEDDED_DATA_REF + \"/y_dev.csv\")\n",
    "    return {\"X\": X_train.values, \"y\": y_train.values.flatten(), \"X_valid\": X_dev.values, \"y_valid\": y_dev.values.flatten()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Create AutoMLConfig object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we specify the parameters for the `AutoMLConfig` class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**task**  \n",
    "AutoML supports the following base learners for the regression task: Elastic Net, Light GBM, Gradient Boosting, Decision Tree, K-nearest Neighbors, LARS Lasso, Stochastic Gradient Descent, Random Forest, Extremely Randomized Trees, XGBoost, DNN Regressor, Linear Regression. In addition, AutoML also supports two kinds of ensemble methods: voting (weighted average of the output of multiple base learners) and stacking (training a second \"metalearner\" which uses the base algorithms' predictions to predict the target variable). Specific base learners can be included or excluded in the parameters for the AutoMLConfig class (whitelist_models and blacklist_models) and the voting/stacking ensemble options can be specified as well (enable_voting_ensemble and enable_stack_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**preprocess**  \n",
    "AutoML also has advanced preprocessing methods, eliminating the need for users to perform this manually. Data is automatically scaled and normalized but an additional parameter in the AutoMLConfig class enables the use of more advanced techniques including imputation, generating additional features, transformations, word embeddings, etc. (full list found [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-create-portal-experiments#preprocess)). Note that algorithm-specific preprocessing will be applied even if preprocess=False. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**primary_metric**  \n",
    "The regression metrics available are the following: Spearman Correlation (spearman_correlation), Normalized RMSE (normalized_root_mean_squared_error), Normalized MAE (normalized_mean_absolute_error), and R2 score (r2_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constraints:**  \n",
    "There is a cost_mode parameter to set cost prediction modes (see options [here](https://docs.microsoft.com/en-us/python/api/azureml-train-automl/azureml.train.automl.automlconfig?view=azure-ml-py)). To set constraints on time there are multiple parameters including experiment_exit_score (target score to exit the experiment after achieving), experiment_timeout_minutes (maximum amount of time for all combined iterations), and iterations (total number of different algorithm and parameter combinations to try)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(\n",
    "    debug_log=\"automl_errors.log\",\n",
    "    path=project_folder,\n",
    "    compute_target=compute_target,\n",
    "    run_configuration=conda_run_config,\n",
    "    data_script=project_folder\n",
    "    + \"/get_data.py\",  # local path to script with get_data() function\n",
    "    **automl_settings #where the AutoML main settings are defined\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Create AutoMLStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create `PipelineData` objects for the model data (our outputs) and then create the `AutoMLStep`. The `AutoMLStep` requires a `AutoMLConfig` object and we pass our intermediate data (embedded_data) in as the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PipelineData objects for tracking AutoML metrics\n",
    "\n",
    "metrics_data = PipelineData(\n",
    "    name=\"metrics_data\",\n",
    "    datastore=ds,\n",
    "    pipeline_output_name=\"metrics_output\",\n",
    "    training_output=TrainingOutput(type=\"Metrics\"),\n",
    ")\n",
    "model_data = PipelineData(\n",
    "    name=\"model_data\",\n",
    "    datastore=ds,\n",
    "    pipeline_output_name=\"best_model_output\",\n",
    "    training_output=TrainingOutput(type=\"Model\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_step = AutoMLStep(\n",
    "    name=\"AutoML\",\n",
    "    automl_config=automl_config,  # the AutoMLConfig object created previously\n",
    "    inputs=[\n",
    "        embedded_data\n",
    "    ],  # inputs is the PipelineData that was the output of the previous pipeline step\n",
    "    outputs=[\n",
    "        metrics_data,\n",
    "        model_data,\n",
    "    ],  # PipelineData objects to reference metric and model information\n",
    "    allow_reuse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up our pipeline which requires specifying our `Workspace` and the ordering of the steps that we created (steps parameter). We submit the pipeline and inspect the run details using a RunDetails widget. For remote runs, the execution of iterations is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'auto_prepare_environment' is deprecated and unused. It will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    description=\"pipeline_embed_automl\",  # give a name for the pipeline\n",
    "    workspace=ws,\n",
    "    steps=[embed_step, automl_step],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Embed [c58d89e2][5b5b0541-369d-43bd-84a6-45b8c974112c], (This step will run and generate new outputs)\n",
      "Created step AutoML [30d39d20][0198951d-6c3d-43d4-bcf9-165e76be3f97], (This step will run and generate new outputs)\n",
      "Using data reference stsbenchmark for StepId [14edc38c][e3340790-c54f-4147-8dd0-bcb80a9b7b46], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted pipeline run: ba061cab-5346-4fa8-ba87-044853c6a3ef\n"
     ]
    }
   ],
   "source": [
    "pipeline_run = experiment.submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce2251bd0e6487789ea9955d51f503a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the run details using the provided widget\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://nlpbp.blob.core.windows.net/images/pipelineWidget.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, block until the run has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: ba061cab-5346-4fa8-ba87-044853c6a3ef\n",
      "Link to Portal: https://mlworkspace.azure.ai/portal/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/nlprg/providers/Microsoft.MachineLearningServices/workspaces/MAIDAPTest/experiments/NLP-SS-googleUSE/runs/ba061cab-5346-4fa8-ba87-044853c6a3ef\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: 6b4e06b8-c86f-44af-900f-6c5ffb43c2d6\n",
      "Link to Portal: https://mlworkspace.azure.ai/portal/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/nlprg/providers/Microsoft.MachineLearningServices/workspaces/MAIDAPTest/experiments/NLP-SS-googleUSE/runs/6b4e06b8-c86f-44af-900f-6c5ffb43c2d6\n",
      "StepRun( Embed ) Status: NotStarted\n",
      "StepRun( Embed ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_e5b57106c3577fd6eb865e27c449c8a978f6467bd8418506579052b193e0888a_d.txt\n",
      "========================================================================================================================\n",
      "2019-10-24T00:31:37Z Starting output-watcher...\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_945b0744001aac57e50cac7993f9fefa\n",
      "a1298f4ce990: Pulling fs layer\n",
      "04a3282d9c4b: Pulling fs layer\n",
      "9b0d3db6dc03: Pulling fs layer\n",
      "8269c605f3f1: Pulling fs layer\n",
      "6504d449e70c: Pulling fs layer\n",
      "4e38f320d0d4: Pulling fs layer\n",
      "8269c605f3f1: Waiting\n",
      "6504d449e70c: Waiting\n",
      "b0a763e8ee03: Pulling fs layer\n",
      "11917a028ca4: Pulling fs layer\n",
      "a6c378d11cbf: Pulling fs layer\n",
      "4e38f320d0d4: Waiting\n",
      "6cc007ad9140: Pulling fs layer\n",
      "11917a028ca4: Waiting\n",
      "a6c378d11cbf: Waiting\n",
      "6c1698a608f3: Pulling fs layer\n",
      "fe38735c9276: Pulling fs layer\n",
      "110285ccdc64: Pulling fs layer\n",
      "8d5a74bb29b7: Pulling fs layer\n",
      "6cc007ad9140: Waiting\n",
      "a6845bb806c1: Pulling fs layer\n",
      "797209d3fd6b: Pulling fs layer\n",
      "3dcb39fe5971: Pulling fs layer\n",
      "6c1698a608f3: Waiting\n",
      "fe38735c9276: Waiting\n",
      "a6845bb806c1: Waiting\n",
      "797209d3fd6b: Waiting\n",
      "110285ccdc64: Waiting\n",
      "3dcb39fe5971: Waiting\n",
      "9b0d3db6dc03: Verifying Checksum\n",
      "9b0d3db6dc03: Download complete\n",
      "04a3282d9c4b: Verifying Checksum\n",
      "04a3282d9c4b: Download complete\n",
      "8269c605f3f1: Verifying Checksum\n",
      "8269c605f3f1: Download complete\n",
      "a1298f4ce990: Verifying Checksum\n",
      "a1298f4ce990: Download complete\n",
      "6504d449e70c: Verifying Checksum\n",
      "6504d449e70c: Download complete\n",
      "4e38f320d0d4: Verifying Checksum\n",
      "4e38f320d0d4: Download complete\n",
      "b0a763e8ee03: Verifying Checksum\n",
      "b0a763e8ee03: Download complete\n",
      "6cc007ad9140: Verifying Checksum\n",
      "6cc007ad9140: Download complete\n",
      "11917a028ca4: Verifying Checksum\n",
      "11917a028ca4: Download complete\n",
      "6c1698a608f3: Download complete\n",
      "fe38735c9276: Verifying Checksum\n",
      "fe38735c9276: Download complete\n",
      "110285ccdc64: Verifying Checksum\n",
      "110285ccdc64: Download complete\n",
      "8d5a74bb29b7: Download complete\n",
      "a6c378d11cbf: Verifying Checksum\n",
      "a6c378d11cbf: Download complete\n",
      "a6845bb806c1: Verifying Checksum\n",
      "a6845bb806c1: Download complete\n",
      "3dcb39fe5971: Verifying Checksum\n",
      "3dcb39fe5971: Download complete\n",
      "a1298f4ce990: Pull complete\n",
      "04a3282d9c4b: Pull complete\n",
      "9b0d3db6dc03: Pull complete\n",
      "8269c605f3f1: Pull complete\n",
      "797209d3fd6b: Verifying Checksum\n",
      "797209d3fd6b: Download complete\n",
      "6504d449e70c: Pull complete\n",
      "4e38f320d0d4: Pull complete\n",
      "b0a763e8ee03: Pull complete\n",
      "11917a028ca4: Pull complete\n",
      "a6c378d11cbf: Pull complete\n",
      "6cc007ad9140: Pull complete\n",
      "6c1698a608f3: Pull complete\n",
      "fe38735c9276: Pull complete\n",
      "110285ccdc64: Pull complete\n",
      "8d5a74bb29b7: Pull complete\n",
      "a6845bb806c1: Pull complete\n",
      "797209d3fd6b: Pull complete\n",
      "3dcb39fe5971: Pull complete\n",
      "Digest: sha256:486539ca406b7e02cd37bb55329bf4f69b05785ab09838931d1dd5ec630105db\n",
      "Status: Downloaded newer image for maidaptestc9922809.azurecr.io/azureml/azureml_945b0744001aac57e50cac7993f9fefa:latest\n",
      "141f092c168bd87070335ab5c55d5fce90a7eeff6c15b8efc5394054511c7831\n",
      "2019/10/24 00:32:26 Version: 3.0.00998.0001 Branch: master Commit: 20ee96da\n",
      "2019/10/24 00:32:27 sshd runtime has already been installed in the container\n",
      "ssh-keygen: /azureml-envs/azureml_056b5e8d5d3c9d58e12288ff8676c921/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\n",
      "ssh-keygen: /azureml-envs/azureml_056b5e8d5d3c9d58e12288ff8676c921/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\n",
      "bash: /azureml-envs/azureml_056b5e8d5d3c9d58e12288ff8676c921/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "bash: /azureml-envs/azureml_056b5e8d5d3c9d58e12288ff8676c921/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_e5b57106c3577fd6eb865e27c449c8a978f6467bd8418506579052b193e0888a_d.txt\n",
      "===============================================================================================================\n",
      "bash: /azureml-envs/azureml_056b5e8d5d3c9d58e12288ff8676c921/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "bash: /azureml-envs/azureml_056b5e8d5d3c9d58e12288ff8676c921/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "bash: /azureml-envs/azureml_056b5e8d5d3c9d58e12288ff8676c921/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 154\n",
      "Entering Run History Context Manager.\n",
      "2019-10-24 00:33:32.239656: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2019-10-24 00:33:32.243925: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596985000 Hz\n",
      "2019-10-24 00:33:32.244649: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7665dd0 executing computations on platform Host. Devices:\n",
      "2019-10-24 00:33:32.244706: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "/mnt/batch/tasks/shared/LS_root/jobs/maidaptest/azureml/6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/mounts/workspacefilestore/azureml/6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/embedded_data created\n",
      "/mnt/batch/tasks/shared/LS_root/jobs/maidaptest/azureml/6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/mounts/workspacefilestore/azureml/6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/embedded_data created\n",
      "/mnt/batch/tasks/shared/LS_root/jobs/maidaptest/azureml/6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/mounts/workspacefilestore/azureml/6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/embedded_data created\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_e5b57106c3577fd6eb865e27c449c8a978f6467bd8418506579052b193e0888a_d.txt\n",
      "===============================================================================================================\n",
      "bash: /azureml-envs/azureml_056b5e8d5d3c9d58e12288ff8676c921/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "Starting job release. Current time:2019-10-24T00:36:10.428952\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 639\n",
      "Job release is complete. Current time:2019-10-24T00:36:12.940284\n",
      "\n",
      "StepRun(Embed) Execution Summary\n",
      "=================================\n",
      "StepRun( Embed ) Status: Finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'runId': '6b4e06b8-c86f-44af-900f-6c5ffb43c2d6', 'target': 'eval-gpu', 'status': 'Completed', 'startTimeUtc': '2019-10-24T00:31:37.345759Z', 'endTimeUtc': '2019-10-24T00:36:24.810912Z', 'properties': {'azureml.runsource': 'azureml.StepRun', 'ContentSnapshotId': '13b19e33-a974-4d89-ad75-909746bc0c0d', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.pipelinerunid': 'ba061cab-5346-4fa8-ba87-044853c6a3ef', '_azureml.ComputeTargetType': 'batchai', 'AzureML.DerivedImageName': 'azureml/azureml_945b0744001aac57e50cac7993f9fefa', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'runDefinition': {'script': 'embed.py', 'arguments': ['--embedded_data', '$AZUREML_DATAREFERENCE_embedded_data', '--sentence_data', '$AZUREML_DATAREFERENCE_stsbenchmark'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'eval-gpu', 'dataReferences': {'stsbenchmark': {'dataStoreName': 'workspacefilestore', 'mode': 'Mount', 'pathOnDataStore': 'stsbenchmark_data/', 'pathOnCompute': None, 'overwrite': False}, 'embedded_data': {'dataStoreName': 'workspacefilestore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/embedded_data', 'pathOnCompute': None, 'overwrite': False}}, 'data': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'environment': {'name': 'Experiment NLP-SS-googleUSE Environment', 'version': 'Autosave_2019-10-23T21:24:08Z_05e2d56e', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-sdk[automl]==1.0.53', 'azureml-dataprep==1.1.8', 'azureml-train-automl==1.0.53']}, 'numpy', 'py-xgboost<=0.80', 'pandas', 'tensorflow', 'tensorflow-hub', 'scikit-learn'], 'name': 'azureml_056b5e8d5d3c9d58e12288ff8676c921'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04', 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'shmSize': '1g'}, 'spark': {'repositories': ['[]'], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs']}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '1g', 'arguments': []}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_e5b57106c3577fd6eb865e27c449c8a978f6467bd8418506579052b193e0888a_d.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/azureml-logs/55_azureml-execution-tvmps_e5b57106c3577fd6eb865e27c449c8a978f6467bd8418506579052b193e0888a_d.txt?sv=2018-11-09&sr=b&sig=45LnDWPA%2BmqgLb4ghwhvHXe97Wqe3ZK8KzaiuuwOJWo%3D&st=2019-10-24T00%3A26%3A30Z&se=2019-10-24T08%3A36%3A30Z&sp=r', 'azureml-logs/65_job_prep-tvmps_e5b57106c3577fd6eb865e27c449c8a978f6467bd8418506579052b193e0888a_d.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/azureml-logs/65_job_prep-tvmps_e5b57106c3577fd6eb865e27c449c8a978f6467bd8418506579052b193e0888a_d.txt?sv=2018-11-09&sr=b&sig=vzE%2BKDGr13pzlRW7w%2F7qXqxvzjUCmm1mCRF%2FBU2xcf8%3D&st=2019-10-24T00%3A26%3A30Z&se=2019-10-24T08%3A36%3A30Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/azureml-logs/70_driver_log.txt?sv=2018-11-09&sr=b&sig=Tza0qU3b0zewNQMaeGsN401O4cw4IH31kewn0uEDUzk%3D&st=2019-10-24T00%3A26%3A30Z&se=2019-10-24T08%3A36%3A30Z&sp=r', 'azureml-logs/75_job_post-tvmps_e5b57106c3577fd6eb865e27c449c8a978f6467bd8418506579052b193e0888a_d.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/azureml-logs/75_job_post-tvmps_e5b57106c3577fd6eb865e27c449c8a978f6467bd8418506579052b193e0888a_d.txt?sv=2018-11-09&sr=b&sig=6DuF%2FoAkCDlC1bv%2BDg0GGPLRVJnLcQewQDn4LZ2ax98%3D&st=2019-10-24T00%3A26%3A30Z&se=2019-10-24T08%3A36%3A30Z&sp=r', 'azureml-logs/process_info.json': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/azureml-logs/process_info.json?sv=2018-11-09&sr=b&sig=rx6uph3XKrEIVTjs0I4tUXVi%2FHxJ4E7DkZ%2F7EdUeveE%3D&st=2019-10-24T00%3A26%3A30Z&se=2019-10-24T08%3A36%3A30Z&sp=r', 'azureml-logs/process_status.json': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/azureml-logs/process_status.json?sv=2018-11-09&sr=b&sig=AlsaWwvnQz2z06r0d4MgxDPtY%2BElV96e%2BkT7LAWb4Wc%3D&st=2019-10-24T00%3A26%3A30Z&se=2019-10-24T08%3A36%3A30Z&sp=r', 'logs/azureml/154_azureml.log': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/logs/azureml/154_azureml.log?sv=2018-11-09&sr=b&sig=VLEdXYTt7hUi8hTXLp3WNGwY4pWNxllh1rgn%2Fm%2B%2FQF4%3D&st=2019-10-24T00%3A26%3A30Z&se=2019-10-24T08%3A36%3A30Z&sp=r', 'logs/azureml/azureml.log': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/logs/azureml/azureml.log?sv=2018-11-09&sr=b&sig=9HrtGHQcZa9bIKzJ90zjHsEdFHNyXu50V9x3VmBznQk%3D&st=2019-10-24T00%3A26%3A30Z&se=2019-10-24T08%3A36%3A30Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/logs/azureml/executionlogs.txt?sv=2018-11-09&sr=b&sig=tunQ37BGw4uz9m3R37%2FF5dFq87%2BEzoHNbYmRjnQOdbY%3D&st=2019-10-24T00%3A26%3A30Z&se=2019-10-24T08%3A36%3A30Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/logs/azureml/stderrlogs.txt?sv=2018-11-09&sr=b&sig=AvRdKyA8Cn3%2F1IUbC1LoBdRH5eynTcC%2FuJJgdnFx0kQ%3D&st=2019-10-24T00%3A26%3A30Z&se=2019-10-24T08%3A36%3A30Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.6b4e06b8-c86f-44af-900f-6c5ffb43c2d6/logs/azureml/stdoutlogs.txt?sv=2018-11-09&sr=b&sig=H1oPQ%2BarMgN0uQUyw5Gzyt5jKalaPMRBDD4SzmyDbcQ%3D&st=2019-10-24T00%3A26%3A30Z&se=2019-10-24T08%3A36%3A30Z&sp=r'}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "StepRunId: fc17b5e3-4c72-47e6-aa0d-ff6e9e634a89\n",
      "Link to Portal: https://mlworkspace.azure.ai/portal/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/nlprg/providers/Microsoft.MachineLearningServices/workspaces/MAIDAPTest/experiments/NLP-SS-googleUSE/runs/fc17b5e3-4c72-47e6-aa0d-ff6e9e634a89\n",
      "StepRun( AutoML ) Status: NotStarted\n",
      "StepRun( AutoML ) Status: Running\n",
      "\n",
      "StepRun(AutoML) Execution Summary\n",
      "==================================\n",
      "StepRun( AutoML ) Status: Finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'runId': 'fc17b5e3-4c72-47e6-aa0d-ff6e9e634a89', 'target': 'eval-gpu', 'status': 'Completed', 'startTimeUtc': '2019-10-24T00:47:54.627848Z', 'endTimeUtc': '2019-10-24T00:51:07.480152Z', 'properties': {'azureml.runsource': 'azureml.StepRun', 'ContentSnapshotId': '13b19e33-a974-4d89-ad75-909746bc0c0d', 'StepType': 'AutoMLStep', 'azureml.pipelinerunid': 'ba061cab-5346-4fa8-ba87-044853c6a3ef', 'num_iterations': '2', 'training_type': 'TrainFull', 'acquisition_function': 'EI', 'metrics': 'accuracy', 'primary_metric': 'spearman_correlation', 'train_split': '0', 'MaxTimeSeconds': '900', 'acquisition_parameter': '0', 'num_cross_validation': None, 'target': 'eval-gpu', 'RawAMLSettingsString': \"{'name':'NLP-SS-googleUSE','subscription_id':'15ae9cb6-95c1-483d-a0e3-b1a1a3b06324','resource_group':'nlprg','workspace_name':'MAIDAPTest','path':'./automl-sentence-similarity','iterations':2,'data_script':'./automl-sentence-similarity/get_data.py','primary_metric':'spearman_correlation','task_type':'regression','compute_target':'eval-gpu','spark_context':None,'validation_size':0.0,'n_cross_validations':None,'y_min':None,'y_max':None,'num_classes':None,'preprocess':True,'lag_length':0,'max_cores_per_iteration':1,'max_concurrent_iterations':1,'iteration_timeout_minutes':15,'mem_in_mb':None,'enforce_time_on_windows':True,'experiment_timeout_minutes':None,'experiment_exit_score':None,'blacklist_models':None,'blacklist_algos':['[\\\\'XGBoostRegressor\\\\'',' \\\\'XGBoostRegressor\\\\']'],'whitelist_models':None,'auto_blacklist':True,'exclude_nan_labels':True,'verbosity':20,'debug_log':'automl_errors.log','debug_flag':None,'enable_ensembling':False,'ensemble_iterations':None,'model_explainability':False,'enable_tf':False,'enable_cache':True,'enable_subsampling':False,'subsample_seed':None,'cost_mode':0,'is_timeseries':False,'metric_operation':'maximize','time_column_name':None,'grain_column_names':None,'drop_column_names':None,'group':None,'target_lags':None,'target_rolling_window_size':None,'max_horizon':None,'country_or_region':None,'seasonality':None,'use_stl':None,'season_trend':None,'season':None,'intermediate_datasets':None}\", 'AMLSettingsJsonString': '{\"name\":\"NLP-SS-googleUSE\",\"subscription_id\":\"15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\",\"resource_group\":\"nlprg\",\"workspace_name\":\"MAIDAPTest\",\"path\":\"./automl-sentence-similarity\",\"iterations\":2,\"data_script\":\"./automl-sentence-similarity/get_data.py\",\"primary_metric\":\"spearman_correlation\",\"task_type\":\"regression\",\"compute_target\":\"eval-gpu\",\"spark_context\":null,\"validation_size\":0.0,\"n_cross_validations\":null,\"y_min\":null,\"y_max\":null,\"num_classes\":null,\"preprocess\":true,\"lag_length\":0,\"max_cores_per_iteration\":1,\"max_concurrent_iterations\":1,\"iteration_timeout_minutes\":15,\"mem_in_mb\":null,\"enforce_time_on_windows\":true,\"experiment_timeout_minutes\":null,\"experiment_exit_score\":null,\"blacklist_models\":null,\"blacklist_algos\":[\"[\\'XGBoostRegressor\\'\",\" \\'XGBoostRegressor\\']\"],\"whitelist_models\":null,\"auto_blacklist\":true,\"exclude_nan_labels\":true,\"verbosity\":20,\"debug_log\":\"automl_errors.log\",\"debug_flag\":null,\"enable_ensembling\":false,\"ensemble_iterations\":null,\"model_explainability\":false,\"enable_tf\":false,\"enable_cache\":true,\"enable_subsampling\":false,\"subsample_seed\":null,\"cost_mode\":0,\"is_timeseries\":false,\"metric_operation\":\"maximize\",\"time_column_name\":null,\"grain_column_names\":null,\"drop_column_names\":null,\"group\":null,\"target_lags\":null,\"target_rolling_window_size\":null,\"max_horizon\":null,\"country_or_region\":null,\"seasonality\":null,\"use_stl\":null,\"season_trend\":null,\"season\":null,\"intermediate_datasets\":null}', 'DataPrepJsonString': None, 'EnableSubsampling': 'False', 'runTemplate': 'AutoML', 'snapshotId': '13b19e33-a974-4d89-ad75-909746bc0c0d', 'SetupRunId': 'fc17b5e3-4c72-47e6-aa0d-ff6e9e634a89_setup', 'ProblemInfoJsonString': '{\"dataset_num_categorical\": 0, \"dataset_classes\": 140, \"dataset_features\": 1024, \"dataset_samples\": 5749, \"is_sparse\": false, \"subsampling\": false}'}, 'logFiles': {'logs/azureml/executionlogs.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.fc17b5e3-4c72-47e6-aa0d-ff6e9e634a89/logs/azureml/executionlogs.txt?sv=2018-11-09&sr=b&sig=GncEbsBVCaPV2%2FSRrC3%2BWQHRXSwqgN3HvhlJEtDb5jg%3D&st=2019-10-24T00%3A43%3A20Z&se=2019-10-24T08%3A53%3A20Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.fc17b5e3-4c72-47e6-aa0d-ff6e9e634a89/logs/azureml/stderrlogs.txt?sv=2018-11-09&sr=b&sig=M%2Boym1n8%2BcA9%2F40ixW3CwYNtGRBHrK2hvPslKSywODk%3D&st=2019-10-24T00%3A43%3A20Z&se=2019-10-24T08%3A53%3A20Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.fc17b5e3-4c72-47e6-aa0d-ff6e9e634a89/logs/azureml/stdoutlogs.txt?sv=2018-11-09&sr=b&sig=ezkWVcIzRX484ftTJ6LenU%2B57iAAZoGZPtB%2FU6dVw3o%3D&st=2019-10-24T00%3A43%3A20Z&se=2019-10-24T08%3A53%3A20Z&sp=r'}}\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': 'ba061cab-5346-4fa8-ba87-044853c6a3ef', 'status': 'Completed', 'startTimeUtc': '2019-10-24T00:31:21.878935Z', 'endTimeUtc': '2019-10-24T00:53:18.890669Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': None, 'runType': 'HTTP', 'azureml.parameters': '{}'}, 'logFiles': {'logs/azureml/executionlogs.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.ba061cab-5346-4fa8-ba87-044853c6a3ef/logs/azureml/executionlogs.txt?sv=2018-11-09&sr=b&sig=3Hr%2Fi21vN8HoYDVAce6bZwljh0FrFiM5EUWQk%2BEETNE%3D&st=2019-10-24T00%3A43%3A22Z&se=2019-10-24T08%3A53%3A22Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.ba061cab-5346-4fa8-ba87-044853c6a3ef/logs/azureml/stderrlogs.txt?sv=2018-11-09&sr=b&sig=vfY1VOA9vQexU6KNlT4F5naweNgLak5Gos5nQxu0ZjI%3D&st=2019-10-24T00%3A43%3A22Z&se=2019-10-24T08%3A53%3A22Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://maidaptest3334372853.blob.core.windows.net/azureml/ExperimentRun/dcid.ba061cab-5346-4fa8-ba87-044853c6a3ef/logs/azureml/stdoutlogs.txt?sv=2018-11-09&sr=b&sig=NutjVEMWKLFpzbqejvQ7G03b9XGGg9wXyIKOvxVRjBg%3D&st=2019-10-24T00%3A43%3A22Z&se=2019-10-24T08%3A53%3A22Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_run.wait_for_completion(\n",
    "    show_output=True\n",
    ")  # show console output while run is in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cancel the Run**\n",
    "\n",
    "Interrupting/Restarting the jupyter kernel will not properly cancel the run, which can lead to wasted compute resources. To avoid this, we recommend explicitly canceling a run with the following code:\n",
    "\n",
    "`pipeline_run.cancel()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deploy Sentence Similarity Model\n",
    "\n",
    "Deploying an Azure Machine Learning model as a web service creates a REST API. You can send data to this API and receive the prediction returned by the model.\n",
    "In general, you create a webservice by deploying a model as an image to a Compute Target.\n",
    "\n",
    "Some of the Compute Targets are: \n",
    "1. Azure Container Instance\n",
    "2. Azure Kubernetes Service\n",
    "3. Local web service\n",
    "\n",
    "The general workflow for deploying a model is as follows:\n",
    "1. Register a model\n",
    "2. Prepare to deploy\n",
    "3. Deploy the model to the compute target\n",
    "4. Test the deployed model (webservice)\n",
    "\n",
    "In this notebook we walk you through the process of creating a webservice running on Azure Kubernetes Service ([AKS](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes\n",
    ")) by deploying the model as an image. AKS is good for high-scale production deployments. It provides fast response time and autoscaling of the deployed service. Cluster autoscaling is not supported through the Azure Machine Learning SDK. \n",
    "\n",
    "You can find more information on deploying and serving models [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Register/Retrieve AutoML and Google Universal Sentence Encoder Models for Deployment\n",
    "\n",
    "Registering a model means registering one or more files that make up a model. The Machine Learning models are registered in your current Aure Machine Learning Workspace. The model can either come from Azure Machine Learning or another location, such as your local machine.\n",
    "\n",
    "See other ways to register a model [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where)\n",
    "\n",
    "Below we show how to register a new model and also how to retrieve and register an existing model.\n",
    "\n",
    "### Register a new automl model\n",
    "Register the best AutoML model based on the pipeline results or load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Received unrecognized parameter: time_column_name None\n",
      "WARNING - Received unrecognized parameter: grain_column_names None\n",
      "WARNING - Received unrecognized parameter: drop_column_names None\n",
      "WARNING - Received unrecognized parameter: group None\n",
      "WARNING - Received unrecognized parameter: target_lags None\n",
      "WARNING - Received unrecognized parameter: target_rolling_window_size None\n",
      "WARNING - Received unrecognized parameter: max_horizon None\n",
      "WARNING - Received unrecognized parameter: country_or_region None\n",
      "WARNING - Received unrecognized parameter: seasonality None\n",
      "WARNING - Received unrecognized parameter: use_stl None\n",
      "WARNING - Received unrecognized parameter: season_trend None\n",
      "WARNING - Received unrecognized parameter: season None\n",
      "WARNING - Received unrecognized parameter: intermediate_datasets None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model fc17b5e34c7247ebest\n",
      "fc17b5e34c7247ebest\n"
     ]
    }
   ],
   "source": [
    "automl_step_run = AutoMLStepRun(step_run=pipeline_run.find_step_run(\"AutoML\")[0])\n",
    "\n",
    "# to register the fitted_mode\n",
    "description = \"Pipeline AutoML Model\"\n",
    "tags = {\"area\": \"nlp\", \"type\": \"sentencesimilarity pipelines\"}\n",
    "model = automl_step_run.register_model(description=description, tags=tags)\n",
    "automl_model_name = automl_step_run.model_id\n",
    "print(\n",
    "    automl_step_run.model_id\n",
    ")  # Use this id to deploy the model as a web service in Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve existing model from Azure\n",
    "If you already have a best model then you can skip registering the model by just retrieving the latest version of model by providing its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model with name fc17b5e34c7247ebest\n"
     ]
    }
   ],
   "source": [
    "automl_model_name = \"fc17b5e34c7247ebest\"  # best fit model registered in the workspace\n",
    "model = Model(ws, name=automl_model_name)\n",
    "print(\"Found model with name\", automl_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Google Universal Sentence Encoder Model\n",
    "Register the Google Universal Sentence Encoder model if not already registered in your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model googleUSEmodel\n",
      "Registered googleUSEembeddings model\n"
     ]
    }
   ],
   "source": [
    "# set location for where to download google tensorflow model\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"./googleUSE\"\n",
    "# download model\n",
    "hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "# register model\n",
    "embedding_model = Model.register(\n",
    "    model_path=\"googleUSE\",\n",
    "    model_name=\"googleUSEmodel\",\n",
    "    tags={\"Model\": \"GoogleUSE\"},\n",
    "    description=\"Google Universal Sentence Embedding pretrained model\",\n",
    "    workspace=ws,\n",
    ")\n",
    "print(\"Registered googleUSEembeddings model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve existing Google USE model from Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model with name googleUSEembeddings\n"
     ]
    }
   ],
   "source": [
    "embedding_model = Model(ws, name=\"googleUSEmodel\")\n",
    "print(\"Found model with name googleUSEembeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Create Scoring Script\n",
    "\n",
    "In this section we show an example of an entry script, which is called from the deployed webservice. `score.py` is our entry script. The script must contain:\n",
    "1. init() - This function loads the model in a global object.\n",
    "2. run() - This function is used for model prediction. The inputs and outputs to `run()` typically use JSON for serialization and deserilization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import azureml.train.automl\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core.model import Model\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # reduce logging output\n",
    "\n",
    "\n",
    "def google_encoder(dataset):\n",
    "    \"\"\" Function that embeds sentences using the Google Universal\n",
    "    Sentence Encoder pretrained model\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataset: pandas dataframe with sentences and scores\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    emb1: 512-dimensional representation of sentence1\n",
    "    emb2: 512-dimensional representation of sentence2\n",
    "    \"\"\"\n",
    "    global embedding_model, sess\n",
    "    sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "    sts_input2 = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "    # Apply embedding model and normalize the input\n",
    "    sts_encode1 = tf.nn.l2_normalize(embedding_model(sts_input1), axis=1)\n",
    "    sts_encode2 = tf.nn.l2_normalize(embedding_model(sts_input2), axis=1)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    emb1, emb2 = sess.run(\n",
    "        [sts_encode1, sts_encode2],\n",
    "        feed_dict={sts_input1: dataset[\"sentence1\"], sts_input2: dataset[\"sentence2\"]},\n",
    "    )\n",
    "    return emb1, emb2\n",
    "\n",
    "\n",
    "def feature_engineering(dataset):\n",
    "    \"\"\"Extracts embedding features from the dataset and returns\n",
    "    features and target in a dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataset: pandas dataframe with sentences and scores\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    df: pandas dataframe with embedding features\n",
    "    scores: list of target variables\n",
    "    \"\"\"\n",
    "    google_USE_emb1, google_USE_emb2 = google_encoder(dataset)\n",
    "    return np.concatenate((google_USE_emb1, google_USE_emb2), axis=1)\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model, googleUSE_dir_path\n",
    "    model_path = Model.get_model_path(\n",
    "        model_name=\"<<modelid>>\"\n",
    "    )  # this name is model.id of model that we want to deploy\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # load the path for google USE embedding model\n",
    "    googleUSE_dir_path = Model.get_model_path(model_name=\"googleUSEmodel\")\n",
    "    os.environ[\"TFHUB_CACHE_DIR\"] = googleUSE_dir_path\n",
    "\n",
    "\n",
    "def run(rawdata):\n",
    "    global embedding_model, sess, googleUSE_dir_path, model\n",
    "    try:\n",
    "        # load data and convert to dataframe\n",
    "        data = json.loads(rawdata)[\"data\"]\n",
    "        data_df = pd.DataFrame(data, columns=[\"sentence1\", \"sentence2\"])\n",
    "\n",
    "        # begin a tensorflow session and load tensorhub module\n",
    "        sess = tf.Session()\n",
    "        embedding_model = hub.Module(\n",
    "            googleUSE_dir_path + \"/96e8f1d3d4d90ce86b2db128249eb8143a91db73\"\n",
    "        )\n",
    "\n",
    "        # Embed sentences using Google USE model\n",
    "        embedded_data = feature_engineering(data_df)\n",
    "        # Predict using AutoML saved model\n",
    "        result = model.predict(embedded_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        sess.close()\n",
    "        return json.dumps({\"error\": result})\n",
    "\n",
    "    sess.close()\n",
    "    return json.dumps({\"result\": result.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute the actual model id in the script file.\n",
    "script_file_name = \"score.py\"\n",
    "\n",
    "with open(script_file_name, \"r\") as cefr:\n",
    "    content = cefr.read()\n",
    "\n",
    "with open(script_file_name, \"w\") as cefw:\n",
    "    cefw.write(content.replace(\"<<modelid>>\", automl_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Create a YAML File for the Environment\n",
    "\n",
    "To ensure the fit results are consistent with the training results, the SDK dependency versions need to be the same as the environment that trains the model. The following cells create a file, pipeline_env.yml, which specifies the dependencies from the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline_env.yml'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myenv = CondaDependencies.create(\n",
    "    conda_packages=[\n",
    "        \"numpy\",\n",
    "        \"scikit-learn\",\n",
    "        \"py-xgboost<=0.80\",\n",
    "        \"pandas\",\n",
    "        \"tensorflow\",\n",
    "        \"tensorflow-hub\",\n",
    "    ],\n",
    "    pip_packages=[\"azureml-sdk[automl]==1.0.53.*\"],\n",
    "    python_version=\"3.6.8\",\n",
    ")\n",
    "\n",
    "conda_env_file_name = \"pipeline_env.yml\"\n",
    "myenv.save_to_file(\".\", conda_env_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Image Creation\n",
    "\n",
    "In this step we create a container image which is wrapper containing the entry script, yaml file with package dependencies and the model. The created image is then deployed as a webservice in the next step. This step can take up to 10 minutes and even longer if the model is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image\n",
      "Running...................................................................................\n",
      "Succeeded\n",
      "Image creation operation finished for image pipeline-automl-image:32, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "# trying to add dependencies\n",
    "image_config = ContainerImage.image_configuration(\n",
    "    execution_script=script_file_name,\n",
    "    runtime=\"python\",\n",
    "    conda_file=conda_env_file_name,\n",
    "    description=\"Image with aml pipeline model\",\n",
    "    tags={\"area\": \"nlp\", \"type\": \"sentencesimilarity pipeline\"},\n",
    ")\n",
    "\n",
    "image = ContainerImage.create(\n",
    "    name=\"pipeline-automl-image\",\n",
    "    # this is the model object\n",
    "    models=[model, embedding_model],  # add both embedding and autoML models\n",
    "    image_config=image_config,\n",
    "    workspace=ws,\n",
    ")\n",
    "\n",
    "image.wait_for_creation(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above step fails, then use below command to see logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Provision the AKS Cluster\n",
    "\n",
    "**Time estimate:** Approximately 20 minutes.\n",
    "\n",
    "Creating or attaching an AKS cluster is a one time process for your workspace. You can reuse this cluster for multiple deployments. If you delete the cluster or the resource group that contains it, you must create a new cluster the next time you need to deploy. You can have multiple AKS clusters attached to your workspace.\n",
    "\n",
    "**Note:** Check the Azure Portal to make sure that the AKS Cluster has been provisioned properly before moving forward with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create aks cluser\n",
    "\n",
    "# Use the default configuration (can also provide parameters to customize)\n",
    "prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(\n",
    "    workspace=ws, name=\"nlp-aks-clusterx\", provisioning_configuration=prov_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.6 Deploy the Image as a Web Service on Azure Kubernetes Service\n",
    "\n",
    "In the case of deployment on AKS, in addition to the Docker image, we need to define computational resources. This is typically a cluster of CPUs or a cluster of GPUs. If we already have a Kubernetes-managed cluster in our workspace, we can use it, otherwise, we can create a new one.\n",
    "\n",
    "In this notebook we will use the cluster in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the web service configuration\n",
    "aks_config = AksWebservice.deploy_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to deploy our web service. We will deploy from the Docker image. It contains our AutoML model as well as the Google Universal Sentence Encoder model and the conda environment needed for the scoring script to work properly. The parameters to pass to the Webservice.deploy_from_image() command are similar to those used for deployment on Azure Container Instance ([ACI](https://azure.microsoft.com/en-us/services/container-instances/\n",
    ")). The only major difference is the compute target (aks_target), i.e. the CPU cluster we just spun up.\n",
    "\n",
    "**Note:** This deployment takes a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating service\n",
      "Running........................\n",
      "SucceededAKS service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "# deploy image as web service\n",
    "aks_service_name = \"aks-pipelines-servicex\"\n",
    "\n",
    "aks_service = Webservice.deploy_from_image(\n",
    "    workspace=ws,\n",
    "    name=aks_service_name,\n",
    "    image=image,\n",
    "    deployment_config=aks_config,\n",
    "    deployment_target=aks_target,\n",
    ")\n",
    "aks_service.wait_for_deployment(show_output=True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above step fails then use below command to see logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aks_service.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Test Deployed Webservice\n",
    "\n",
    "Testing the deployed model means running the created webservice. <br>\n",
    "The deployed model can be tested by passing a list of sentence pairs. The output will be a score between 0 and 5, with 0 indicating no meaning overlap between the sentences and 5 meaning equivalence.\n",
    "\n",
    "The run method expects input in json format. The Run() method retrieves API keys behind the scenes to make sure that the call is authenticated. The service has a timeout (default of ~30 seconds) which does not allow passing the large test dataset. To overcome this, you can batch data and send it to the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"This is sentence1\", \"This is sentence1\"],\n",
    "    [\"A hungry cat.\", \"A sleeping cat\"],\n",
    "    [\"Its summer time \", \"Winter is coming\"],\n",
    "]\n",
    "data = {\"data\": sentences}\n",
    "data = json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 11.4971\n",
      "Number of samples predicted: 3\n",
      "[2.74838021117439, 3.092084675226697, 2.8753491248064904]\n"
     ]
    }
   ],
   "source": [
    "# Set up a Timer to see how long the model takes to predict\n",
    "t = Timer()\n",
    "\n",
    "t.start()\n",
    "score = aks_service.run(input_data=data)\n",
    "t.stop()\n",
    "\n",
    "print(\"Time elapsed: {}\".format(t))\n",
    "\n",
    "result = json.loads(score)\n",
    "try:\n",
    "    output = result[\"result\"]\n",
    "    print(\"Number of samples predicted: {}\".format(len(output)))\n",
    "    print(output)\n",
    "except:\n",
    "    print(result[\"error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll calculate the Pearson Correlation on the test set.\n",
    "\n",
    "**What is Pearson Correlation?**\n",
    "\n",
    "Our evaluation metric is Pearson correlation ($\\rho$) which is a measure of the linear correlation between two variables. The formula for calculating Pearson correlation is as follows:  \n",
    "\n",
    "$$\\rho_{X,Y} = \\frac{E[(X-\\mu_X)(Y-\\mu_Y)]}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "This metric takes a value in [-1,1] where -1 represents a perfect negative correlation, 1 represents a perfect positive correlation, and 0 represents no correlation. We utilize the Pearson correlation metric as this is the main metric that [SentEval](http://nlpprogress.com/english/semantic_textual_similarity.html), a widely-used evaluation toolkit for evaluation sentence representations, uses for the STS Benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set sentences\n",
    "data = pd.read_csv(\"data/test.csv\")\n",
    "train_y = data[\"score\"].values.flatten()\n",
    "train_x = data.drop(\"score\", axis=1).values.tolist()\n",
    "data = {\"data\": train_x[:500]}\n",
    "data = json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 21.3725\n",
      "Number of sample predicted : 500\n"
     ]
    }
   ],
   "source": [
    "# Set up a Timer to see how long the model takes to predict\n",
    "t = Timer()\n",
    "\n",
    "t.start()\n",
    "score = aks_service.run(input_data=data)\n",
    "t.stop()\n",
    "\n",
    "print(\"Time elapsed: {}\".format(t))\n",
    "\n",
    "result = json.loads(score)\n",
    "\n",
    "try:\n",
    "    output = result[\"result\"]\n",
    "    print(\"Number of sample predicted : {}\".format(len(output)))\n",
    "except:\n",
    "    print(result[\"error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5866672344677907\n"
     ]
    }
   ],
   "source": [
    "# get Pearson Correlation\n",
    "print(pearsonr(output, train_y[:500])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to use AzureML Pipelines and AutoML to streamline the creation of a machine learning workflow for predicting sentence similarity. After creating the pipeline, the notebook demonstrated the deployment of our sentence similarity model using AKS. The model results reported in this notebook (using Google USE embeddings) are much stronger than the results from using AutoML with its built-in embedding capabilities (as in [AutoML Local Deployment ACI](automl_local_deployment_aci.ipynb)). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_cpu)",
   "language": "python",
   "name": "nlp_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
