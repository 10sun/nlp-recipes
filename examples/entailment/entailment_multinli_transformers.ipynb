{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*  \n",
    "\n",
    "*Licensed under the MIT License.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference on MultiNLI Dataset using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before You Start\n",
    "\n",
    "The running time shown in this notebook is running bert-large-cased on a Standard_NC24rs_v3 Azure Deep Learning Virtual Machine with 4 NVIDIA Tesla V100 GPUs. \n",
    "> **Tip:** If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`** to run the notebook on a small subset of the data and a smaller number of epochs. \n",
    "\n",
    "The table below provides some reference running time on different machine configurations.  \n",
    "\n",
    "|QUICK_RUN|Machine Configurations|Running time|\n",
    "|:---------|:----------------------|:------------|\n",
    "|True|4 **CPU**s, 14GB memory| ~ 15 minutes|\n",
    "|True|1 NVIDIA Tesla K80 GPUs, 12GB GPU memory| ~ 5 minutes|\n",
    "|False|1 NVIDIA Tesla K80 GPUs, 12GB GPU memory| ~ 10.5 hours|\n",
    "|False|4 NVIDIA Tesla V100 GPUs, 64GB GPU memory| ~ 2.5 hours|\n",
    "\n",
    "If you run into CUDA out-of-memory error, try reducing the `BATCH_SIZE` and `MAX_SEQ_LENGTH`, but note that model performance will be compromised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we demostrate fine-tuning pretrained transformer models to perform Natural Language Inference (NLI). We use the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset and the task is to classify sentence pairs into three classes: contradiction, entailment, and neutral.   \n",
    "To classify a sentence pair, we concatenate the tokens in both sentences and separate the sentences by the special [SEP] token. A [CLS] token is prepended to the token list and used as the aggregate sequence representation for the classification task.The NLI task essentially becomes a sequence classification task. For example, the figure below shows how [BERT](https://arxiv.org/abs/1810.04805) classifies sentence pairs. \n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/bert_two_sentence.PNG\">\n",
    "\n",
    "We compare the training time and performance of three models: bert-base-cased, bert-large-cased, and xlnet-large-cased. The model used can be set in the **Configurations** section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1110 19:13:59.935610 140117887072000 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1110 19:13:59.978967 140117887072000 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "nlp_path = os.path.abspath('../../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "    \n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils_nlp.models.transformers.sequence_classification import Processor, SequenceClassifier\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "from utils_nlp.common.timer import Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all the model supported by `SequenceClassifier`, call the `list_supported_models` method.  \n",
    "**Note**: Although `SequenceClassifier` supports distilbert for single sequence classification, distilbert doesn't support sentence pair classification and can not be used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc',\n",
       " 'roberta-base',\n",
       " 'roberta-large',\n",
       " 'roberta-large-mnli',\n",
       " 'xlnet-base-cased',\n",
       " 'xlnet-large-cased',\n",
       " 'distilbert-base-uncased',\n",
       " 'distilbert-base-uncased-distilled-squad']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SequenceClassifier.list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-large-cased\"\n",
    "TO_LOWER = False\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# MODEL_NAME = \"xlnet-large-cased\"\n",
    "# TO_LOWER = False\n",
    "# BATCH_SIZE = 16\n",
    "\n",
    "TRAIN_DATA_USED_FRACTION = 1\n",
    "DEV_DATA_USED_FRACTION = 1\n",
    "NUM_EPOCHS = 2\n",
    "WARMUP_STEPS= 2500\n",
    "\n",
    "if QUICK_RUN:\n",
    "    TRAIN_DATA_USED_FRACTION = 0.001\n",
    "    DEV_DATA_USED_FRACTION = 0.01\n",
    "    NUM_EPOCHS = 1\n",
    "    WARMUP_STEPS= 10\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    BATCH_SIZE = BATCH_SIZE/2\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# model configurations\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "# optimizer configurations\n",
    "LEARNING_RATE= 5e-5\n",
    "\n",
    "# data configurations\n",
    "TEXT_COL_1 = \"sentence1\"\n",
    "TEXT_COL_2 = \"sentence2\"\n",
    "LABEL_COL = \"gold_label\"\n",
    "LABEL_COL_NUM = \"gold_label_num\"\n",
    "\n",
    "CACHE_DIR = TemporaryDirectory().name\n",
    "CACHE_DIR = \"./temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The MultiNLI dataset comes with three subsets: train, dev_matched, dev_mismatched. The dev_matched dataset are from the same genres as the train dataset, while the dev_mismatched dataset are from genres not seen in the training dataset.   \n",
    "The `load_pandas_df` function downloads and extracts the zip files if they don't already exist in `local_cache_path` and returns the data subset specified by `file_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_pandas_df(local_cache_path=CACHE_DIR, file_split=\"train\")\n",
    "dev_df_matched = load_pandas_df(local_cache_path=CACHE_DIR, file_split=\"dev_matched\")\n",
    "dev_df_mismatched = load_pandas_df(local_cache_path=CACHE_DIR, file_split=\"dev_mismatched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_matched = dev_df_matched.loc[dev_df_matched['gold_label'] != '-']\n",
    "dev_df_mismatched = dev_df_mismatched.loc[dev_df_mismatched['gold_label'] != '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 392702\n",
      "Development (matched) dataset size: 9815\n",
      "Development (mismatched) dataset size: 9832\n",
      "\n",
      "   gold_label                                          sentence1  \\\n",
      "0     neutral  Conceptually cream skimming has two basic dime...   \n",
      "1  entailment  you know during the season and i guess at at y...   \n",
      "2  entailment  One of our number will carry out your instruct...   \n",
      "3  entailment  How do you know? All this is their information...   \n",
      "4     neutral  yeah i tell you what though if you go price so...   \n",
      "\n",
      "                                           sentence2  \n",
      "0  Product and geography are what make cream skim...  \n",
      "1  You lose the things to the following level if ...  \n",
      "2  A member of my team will execute your orders w...  \n",
      "3                  This information belongs to them.  \n",
      "4           The tennis shoes have a range of prices.  \n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset size: {}\".format(train_df.shape[0]))\n",
    "print(\"Development (matched) dataset size: {}\".format(dev_df_matched.shape[0]))\n",
    "print(\"Development (mismatched) dataset size: {}\".format(dev_df_mismatched.shape[0]))\n",
    "print()\n",
    "print(train_df[['gold_label', 'sentence1', 'sentence2']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=TRAIN_DATA_USED_FRACTION).reset_index(drop=True)\n",
    "dev_df_matched = dev_df_matched.sample(frac=DEV_DATA_USED_FRACTION).reset_index(drop=True)\n",
    "dev_df_mismatched = dev_df_mismatched.sample(frac=DEV_DATA_USED_FRACTION).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_df[LABEL_COL])\n",
    "train_df[LABEL_COL_NUM] = train_labels \n",
    "num_labels = len(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Preprocess\n",
    "Before training, we tokenize the sentence texts and convert them to lists of tokens. The following steps instantiate a BERT tokenizer given the language, and tokenize the text of the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1110 19:14:11.376676 140117887072000 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at ./temp/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "100%|██████████| 392702/392702 [03:48<00:00, 1715.17it/s]\n",
      "100%|██████████| 9815/9815 [00:05<00:00, 1797.48it/s]\n",
      "100%|██████████| 9832/9832 [00:05<00:00, 1709.69it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = Processor(model_name=MODEL_NAME, cache_dir=CACHE_DIR, to_lower=TO_LOWER)\n",
    "train_dataloader = processor.create_dataloader_from_df(\n",
    "    df=train_df,\n",
    "    text_col=TEXT_COL_1,\n",
    "    label_col=LABEL_COL_NUM,\n",
    "    shuffle=True,\n",
    "    text2_col=TEXT_COL_2,\n",
    "    max_len=MAX_SEQ_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "dev_dataloader_matched = processor.create_dataloader_from_df(\n",
    "    df=dev_df_matched,\n",
    "    text_col=TEXT_COL_1,\n",
    "    shuffle=False,\n",
    "    text2_col=TEXT_COL_2,\n",
    "    max_len=MAX_SEQ_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "dev_dataloader_mismatched = processor.create_dataloader_from_df(\n",
    "    df=dev_df_mismatched,\n",
    "    text_col=TEXT_COL_1,\n",
    "    shuffle=False,\n",
    "    text2_col=TEXT_COL_2,\n",
    "    max_len=MAX_SEQ_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we perform the following preprocessing steps in the cell below:\n",
    "\n",
    "* Convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary\n",
    "* Add the special tokens [CLS] and [SEP] to mark the beginning and end of a sentence\n",
    "* Pad or truncate the token lists to the specified max length\n",
    "* Return mask lists that indicate paddings' positions\n",
    "* Return token type id lists that indicate which sentence the tokens belong to\n",
    "\n",
    "*See the original [implementation](https://github.com/google-research/bert/blob/master/run_classifier.py) for more information on BERT's input format.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SequenceClassifier(\n",
    "    model_name=MODEL_NAME, num_labels=num_labels, cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with Timer() as t:\n",
    "    classifier.fit(\n",
    "            train_dataloader,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            warmup_steps=WARMUP_STEPS,\n",
    "        )\n",
    "\n",
    "print(\"Training time : {:.3f} hrs\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 614/614 [04:53<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time : 0.082 hrs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    predictions_matched = classifier.predict(dev_dataloader_matched)\n",
    "print(\"Prediction time : {:.3f} hrs\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 615/615 [04:53<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time : 0.082 hrs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    predictions_mismatched = classifier.predict(dev_dataloader_mismatched)\n",
    "print(\"Prediction time : {:.3f} hrs\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction      0.872     0.894     0.883      3213\n",
      "   entailment      0.913     0.862     0.887      3479\n",
      "      neutral      0.813     0.842     0.828      3123\n",
      "\n",
      "    micro avg      0.866     0.866     0.866      9815\n",
      "    macro avg      0.866     0.866     0.866      9815\n",
      " weighted avg      0.868     0.866     0.867      9815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_matched = label_encoder.inverse_transform(predictions_matched)\n",
    "print(classification_report(dev_df_matched[LABEL_COL], predictions_matched, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction      0.891     0.888     0.889      3240\n",
      "   entailment      0.899     0.862     0.880      3463\n",
      "      neutral      0.810     0.850     0.830      3129\n",
      "\n",
      "    micro avg      0.867     0.867     0.867      9832\n",
      "    macro avg      0.867     0.867     0.866      9832\n",
      " weighted avg      0.868     0.867     0.867      9832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_mismatched = label_encoder.inverse_transform(predictions_mismatched)\n",
    "print(classification_report(dev_df_mismatched[LABEL_COL], predictions_mismatched, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model name|Training time|Scoring time|Matched F1|Mismatched F1|\n",
    "|:--------:|:-----------:|:----------:|:--------:|:-----------:|\n",
    "|xlnet-large-cased|5.15 hrs|0.11 hrs|0.887|0.890|\n",
    "|bert-large-cased|4.01 hrs|0.08 hrs|0.867|0.867|"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nlp_gpu",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
